#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CSVæ–‡ä»¶å»é‡å·¥å…·

è¯¥è„šæœ¬ä»FuckBilibiliComments.pyä¸­æå–çš„å»é‡ç®—æ³•ï¼Œç”¨äºå¤„ç†ä¸¤ä¸ªCSVæ–‡ä»¶çš„å»é‡æ“ä½œã€‚
åŠŸèƒ½åŒ…æ‹¬ï¼š
1. åˆå¹¶å»é‡ï¼šç”Ÿæˆä¸¤ä¸ªæ–‡ä»¶åˆå¹¶åå»é‡çš„ç»“æœ
2. é‡å¤æ•°æ®ï¼šç”Ÿæˆåœ¨å»é‡è¿‡ç¨‹ä¸­å‘ç°çš„é‡å¤æ•°æ®
3. æ–‡ä»¶Aç‹¬æœ‰ï¼šç”Ÿæˆå­˜åœ¨äºæ–‡ä»¶Aä½†ä¸å­˜åœ¨äºæ–‡ä»¶Bçš„æ•°æ®
4. æ–‡ä»¶Bç‹¬æœ‰ï¼šç”Ÿæˆå­˜åœ¨äºæ–‡ä»¶Bä½†ä¸å­˜åœ¨äºæ–‡ä»¶Açš„æ•°æ®

ä½¿ç”¨æ–¹æ³•ï¼š
python csv_deduplicator.py <æ–‡ä»¶Aè·¯å¾„> <æ–‡ä»¶Bè·¯å¾„> [è¾“å‡ºç›®å½•]

åŸºäºFuckBilibiliComments.pyæå–
Pythonç‰ˆæœ¬è¦æ±‚ï¼šPython 3.7+
"""

# ä¾èµ–æ£€æµ‹å’Œè‡ªåŠ¨å®‰è£…
import sys
import subprocess
import importlib

def check_and_install_dependencies():
    """
    æ£€æµ‹å¹¶è‡ªåŠ¨å®‰è£…å¿…è¦çš„ä¾èµ–åŒ…
    """
    required_packages = {
        'pandas': 'pandas>=1.3.0',
    }
    
    missing_packages = []
    
    print("ğŸ” æ£€æµ‹ä¾èµ–åŒ…...")
    
    for package_name, package_spec in required_packages.items():
        try:
            importlib.import_module(package_name)
            print(f"âœ… {package_name} å·²å®‰è£…")
        except ImportError:
            print(f"âŒ {package_name} æœªå®‰è£…")
            missing_packages.append(package_spec)
    
    if missing_packages:
        print(f"\nğŸ“¦ å‘ç° {len(missing_packages)} ä¸ªç¼ºå¤±çš„ä¾èµ–åŒ…ï¼Œå¼€å§‹è‡ªåŠ¨å®‰è£…...")
        
        for package in missing_packages:
            try:
                print(f"æ­£åœ¨å®‰è£… {package}...")
                subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])
                print(f"âœ… {package} å®‰è£…æˆåŠŸ")
            except subprocess.CalledProcessError as e:
                print(f"âŒ {package} å®‰è£…å¤±è´¥: {e}")
                print("è¯·æ‰‹åŠ¨å®‰è£…ä¾èµ–åŒ…ï¼š")
                print(f"pip install {package}")
                sys.exit(1)
        
        print("\nğŸ‰ æ‰€æœ‰ä¾èµ–åŒ…å®‰è£…å®Œæˆï¼")
    else:
        print("\nâœ… æ‰€æœ‰ä¾èµ–åŒ…å·²æ»¡è¶³è¦æ±‚")

# æ£€æŸ¥Pythonç‰ˆæœ¬
if sys.version_info < (3, 7):
    print("âŒ é”™è¯¯ï¼šæ­¤è„šæœ¬éœ€è¦Python 3.7æˆ–æ›´é«˜ç‰ˆæœ¬")
    print(f"å½“å‰Pythonç‰ˆæœ¬ï¼š{sys.version}")
    print("è¯·å‡çº§Pythonç‰ˆæœ¬åé‡è¯•")
    sys.exit(1)

# è‡ªåŠ¨æ£€æµ‹å’Œå®‰è£…ä¾èµ–
check_and_install_dependencies()

# å¯¼å…¥æ‰€éœ€æ¨¡å—
import pandas as pd
import os
from datetime import datetime
import logging


def setup_logger(output_dir):
    """è®¾ç½®æ—¥å¿—è®°å½•å™¨"""
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    log_file = os.path.join(output_dir, f'csv_deduplicator_{timestamp}.log')
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger(__name__)


def extract_crawl_time_from_comment(comment):
    """
    ä»è¯„è®ºæ•°æ®ä¸­æå–çˆ¬å–æ—¶é—´
    
    Args:
        comment (dict): è¯„è®ºæ•°æ®å­—å…¸
    
    Returns:
        int: æ—¶é—´æˆ³
    """
    crawl_time_str = comment.get('çˆ¬å–æ—¶é—´', '')
    if crawl_time_str:
        try:
            # è§£ææ—¶é—´æ ¼å¼ï¼šYYYYå¹´MMæœˆDDæ—¥_HHæ—¶MMåˆ†SSç§’
            dt = datetime.strptime(crawl_time_str, '%Yå¹´%mæœˆ%dæ—¥_%Hæ—¶%Måˆ†%Sç§’')
            return int(dt.timestamp())
        except:
            pass
    
    # å¤‡é€‰æ–¹æ¡ˆï¼šä½¿ç”¨è¯„è®ºçš„æ—¶é—´æˆ³
    return comment.get('æ—¶é—´æˆ³', 0)


def deduplicate_by_rpid(comments, comment_type, logger):
    """
    åŸºäºrpidå’Œçˆ¬å–æ—¶é—´è¿›è¡Œå»é‡
    
    Args:
        comments (list): è¯„è®ºåˆ—è¡¨
        comment_type (str): è¯„è®ºç±»å‹æ ‡è¯†
        logger: æ—¥å¿—è®°å½•å™¨
    
    Returns:
        tuple: (å»é‡åçš„è¯„è®ºåˆ—è¡¨, é‡å¤è¯„è®ºåˆ—è¡¨)
    """
    rpid_to_comment = {}
    duplicates = []
    
    for comment in comments:
        rpid = comment.get('rpid', '')
        if rpid:
            # å¦‚æœå·²å­˜åœ¨è¯¥rpidï¼Œæ¯”è¾ƒçˆ¬å–æ—¶é—´ï¼Œä¿ç•™çˆ¬å–æ—¶é—´æ›´æ™šçš„
            if rpid in rpid_to_comment:
                existing_crawl_time = extract_crawl_time_from_comment(rpid_to_comment[rpid])
                current_crawl_time = extract_crawl_time_from_comment(comment)
                
                if current_crawl_time > existing_crawl_time:
                    # å°†æ—§è¯„è®ºæ ‡è®°ä¸ºé‡å¤
                    old_comment = rpid_to_comment[rpid].copy()
                    old_comment['é‡å¤æ¥æº'] = comment_type
                    old_comment['åŸå§‹è¯„è®ºæ¥æº'] = comment_type
                    duplicates.append(old_comment)
                    # æ›´æ–°ä¸ºæ–°è¯„è®ºï¼ˆä¿ç•™åŠ¨æ€å±æ€§ï¼šç‚¹èµæ•°ã€å›å¤æ•°ã€æ—¶é—´æˆ³ç­‰ï¼‰
                    rpid_to_comment[rpid] = comment
                else:
                    # å½“å‰è¯„è®ºæ˜¯é‡å¤çš„ï¼ˆçˆ¬å–æ—¶é—´æ›´æ—©æˆ–ç›¸ç­‰ï¼‰
                    duplicate_comment = comment.copy()
                    duplicate_comment['é‡å¤æ¥æº'] = comment_type
                    duplicate_comment['åŸå§‹è¯„è®ºæ¥æº'] = comment_type
                    duplicates.append(duplicate_comment)
            else:
                rpid_to_comment[rpid] = comment
    
    deduped_comments = list(rpid_to_comment.values())
    if logger:
        logger.info(f"{comment_type}å»é‡: {len(comments)} -> {len(deduped_comments)} æ¡è¯„è®ºï¼Œé‡å¤ {len(duplicates)} æ¡")
    
    return deduped_comments, duplicates


def find_unique_data(comments_a, comments_b, label_a, label_b, logger):
    """
    æ‰¾å‡ºä¸¤ä¸ªè¯„è®ºåˆ—è¡¨ä¸­çš„ç‹¬æœ‰æ•°æ®
    
    Args:
        comments_a (list): è¯„è®ºåˆ—è¡¨A
        comments_b (list): è¯„è®ºåˆ—è¡¨B
        label_a (str): åˆ—è¡¨Açš„æ ‡ç­¾
        label_b (str): åˆ—è¡¨Bçš„æ ‡ç­¾
        logger: æ—¥å¿—è®°å½•å™¨
    
    Returns:
        tuple: (Aç‹¬æœ‰çš„è¯„è®º, Bç‹¬æœ‰çš„è¯„è®º)
    """
    # æ„å»ºrpidé›†åˆ
    rpids_a = {comment.get('rpid', '') for comment in comments_a if comment.get('rpid', '')}
    rpids_b = {comment.get('rpid', '') for comment in comments_b if comment.get('rpid', '')}
    
    # æ‰¾å‡ºç‹¬æœ‰çš„rpid
    unique_rpids_a = rpids_a - rpids_b
    unique_rpids_b = rpids_b - rpids_a
    
    # æå–ç‹¬æœ‰çš„è¯„è®º
    unique_comments_a = [comment for comment in comments_a 
                        if comment.get('rpid', '') in unique_rpids_a]
    unique_comments_b = [comment for comment in comments_b 
                        if comment.get('rpid', '') in unique_rpids_b]
    
    if logger:
        logger.info(f"{label_a}ç‹¬æœ‰è¯„è®º: {len(unique_comments_a)} æ¡")
        logger.info(f"{label_b}ç‹¬æœ‰è¯„è®º: {len(unique_comments_b)} æ¡")
    
    return unique_comments_a, unique_comments_b


def validate_csv_file(file_path):
    """
    éªŒè¯CSVæ–‡ä»¶æ˜¯å¦åŒ…å«å¿…è¦çš„å­—æ®µä¸”æ— ç©ºå€¼
    
    Args:
        file_path (str): CSVæ–‡ä»¶è·¯å¾„
    
    Returns:
        tuple: (æ˜¯å¦æœ‰æ•ˆ, é”™è¯¯ä¿¡æ¯)
    """
    try:
        df = pd.read_csv(file_path, encoding='utf-8')
        
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨rpidåˆ—
        if 'rpid' not in df.columns:
            return False, "æ–‡ä»¶ç¼ºå°‘'rpid'åˆ—"
        
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨çˆ¬å–æ—¶é—´åˆ—
        if 'çˆ¬å–æ—¶é—´' not in df.columns:
            return False, "æ–‡ä»¶ç¼ºå°‘'çˆ¬å–æ—¶é—´'åˆ—"
        
        # æ£€æŸ¥rpidåˆ—æ˜¯å¦æœ‰ç©ºå€¼
        rpid_null_count = df['rpid'].isnull().sum()
        rpid_empty_count = (df['rpid'].astype(str).str.strip() == '').sum()
        if rpid_null_count > 0 or rpid_empty_count > 0:
            return False, f"rpidåˆ—å­˜åœ¨ {rpid_null_count + rpid_empty_count} ä¸ªç©ºå€¼æˆ–ç©ºå­—ç¬¦ä¸²"
        
        # æ£€æŸ¥çˆ¬å–æ—¶é—´åˆ—æ˜¯å¦æœ‰ç©ºå€¼
        crawl_time_null_count = df['çˆ¬å–æ—¶é—´'].isnull().sum()
        crawl_time_empty_count = (df['çˆ¬å–æ—¶é—´'].astype(str).str.strip() == '').sum()
        if crawl_time_null_count > 0 or crawl_time_empty_count > 0:
            return False, f"çˆ¬å–æ—¶é—´åˆ—å­˜åœ¨ {crawl_time_null_count + crawl_time_empty_count} ä¸ªç©ºå€¼æˆ–ç©ºå­—ç¬¦ä¸²"
        
        return True, "æ–‡ä»¶éªŒè¯é€šè¿‡"
    
    except Exception as e:
        return False, f"æ–‡ä»¶è¯»å–å¤±è´¥: {str(e)}"


def load_csv_file(file_path, logger):
    """
    åŠ è½½CSVæ–‡ä»¶
    
    Args:
        file_path (str): CSVæ–‡ä»¶è·¯å¾„
        logger: æ—¥å¿—è®°å½•å™¨
    
    Returns:
        list: è¯„è®ºæ•°æ®åˆ—è¡¨
    """
    try:
        df = pd.read_csv(file_path, encoding='utf-8')
        comments = df.to_dict('records')
        logger.info(f"æˆåŠŸåŠ è½½æ–‡ä»¶ {file_path}ï¼Œå…± {len(comments)} æ¡è®°å½•")
        return comments
    except Exception as e:
        logger.error(f"åŠ è½½æ–‡ä»¶ {file_path} å¤±è´¥: {str(e)}")
        return []


def save_csv_file(comments, file_path, logger):
    """
    ä¿å­˜è¯„è®ºæ•°æ®åˆ°CSVæ–‡ä»¶
    
    Args:
        comments (list): è¯„è®ºæ•°æ®åˆ—è¡¨
        file_path (str): è¾“å‡ºæ–‡ä»¶è·¯å¾„
        logger: æ—¥å¿—è®°å½•å™¨
    """
    try:
        if comments:
            df = pd.DataFrame(comments)
            # ä½¿ç”¨UTF-8-BOMç¼–ç ï¼Œç¡®ä¿Excelèƒ½æ­£ç¡®æ˜¾ç¤ºä¸­æ–‡
            df.to_csv(file_path, index=False, encoding='utf-8-sig')
            logger.info(f"æˆåŠŸä¿å­˜ {len(comments)} æ¡è®°å½•åˆ° {file_path}")
        else:
            # åˆ›å»ºç©ºæ–‡ä»¶
            with open(file_path, 'w', encoding='utf-8-sig') as f:
                f.write('')
            logger.info(f"åˆ›å»ºç©ºæ–‡ä»¶ {file_path}")
    except Exception as e:
        logger.error(f"ä¿å­˜æ–‡ä»¶ {file_path} å¤±è´¥: {str(e)}")


def process_csv_deduplication(file_a_path, file_b_path, output_dir, logger):
    """
    å¤„ç†CSVæ–‡ä»¶å»é‡
    
    Args:
        file_a_path (str): æ–‡ä»¶Aè·¯å¾„
        file_b_path (str): æ–‡ä»¶Bè·¯å¾„
        output_dir (str): è¾“å‡ºç›®å½•
        logger: æ—¥å¿—è®°å½•å™¨
    """
    # åŠ è½½CSVæ–‡ä»¶
    logger.info("å¼€å§‹åŠ è½½CSVæ–‡ä»¶...")
    comments_a = load_csv_file(file_a_path, logger)
    comments_b = load_csv_file(file_b_path, logger)
    
    if not comments_a and not comments_b:
        logger.error("ä¸¤ä¸ªæ–‡ä»¶éƒ½æ— æ³•åŠ è½½ï¼Œé€€å‡ºå¤„ç†")
        return
    
    # è·å–æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰ç”¨äºæ ‡è¯†
    file_a_name = os.path.splitext(os.path.basename(file_a_path))[0]
    file_b_name = os.path.splitext(os.path.basename(file_b_path))[0]
    
    # ç¬¬ä¸€æ­¥ï¼šå¯¹æ¯ä¸ªæ–‡ä»¶å†…éƒ¨å»é‡
    logger.info("å¼€å§‹å¯¹æ–‡ä»¶å†…éƒ¨è¿›è¡Œå»é‡...")
    deduped_a, duplicates_a = deduplicate_by_rpid(comments_a, f"æ–‡ä»¶A({file_a_name})", logger)
    deduped_b, duplicates_b = deduplicate_by_rpid(comments_b, f"æ–‡ä»¶B({file_b_name})", logger)
    
    # ç¬¬äºŒæ­¥ï¼šæ‰¾å‡ºç‹¬æœ‰æ•°æ®
    logger.info("å¼€å§‹æŸ¥æ‰¾ç‹¬æœ‰æ•°æ®...")
    unique_a, unique_b = find_unique_data(deduped_a, deduped_b, 
                                         f"æ–‡ä»¶A({file_a_name})", 
                                         f"æ–‡ä»¶B({file_b_name})", logger)
    
    # ç¬¬ä¸‰æ­¥ï¼šåˆå¹¶å¹¶å»é‡
    logger.info("å¼€å§‹åˆå¹¶å»é‡...")
    all_comments = deduped_a + deduped_b
    merged_comments, merge_duplicates = deduplicate_by_rpid(all_comments, "åˆå¹¶å»é‡", logger)
    
    # åˆå¹¶æ‰€æœ‰é‡å¤æ•°æ®
    all_duplicates = duplicates_a + duplicates_b + merge_duplicates
    
    # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # ä¿å­˜ç»“æœæ–‡ä»¶
    logger.info("å¼€å§‹ä¿å­˜ç»“æœæ–‡ä»¶...")
    
    # 1. åˆå¹¶å»é‡æ–‡ä»¶
    merged_file = os.path.join(output_dir, f'åˆå¹¶å»é‡ç»“æœ_{timestamp}.csv')
    save_csv_file(merged_comments, merged_file, logger)
    
    # 2. é‡å¤æ•°æ®æ–‡ä»¶
    duplicates_file = os.path.join(output_dir, f'é‡å¤æ•°æ®_{timestamp}.csv')
    save_csv_file(all_duplicates, duplicates_file, logger)
    
    # 3. æ–‡ä»¶Aç‹¬æœ‰æ•°æ®
    unique_a_file = os.path.join(output_dir, f'ç‹¬æœ‰æ•°æ®_éš¶å±äºæ–‡ä»¶Aï¼š{file_a_name}_{timestamp}.csv')
    save_csv_file(unique_a, unique_a_file, logger)
    
    # 4. æ–‡ä»¶Bç‹¬æœ‰æ•°æ®
    unique_b_file = os.path.join(output_dir, f'ç‹¬æœ‰æ•°æ®_éš¶å±äºæ–‡ä»¶Bï¼š{file_b_name}_{timestamp}.csv')
    save_csv_file(unique_b, unique_b_file, logger)
    
    # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
    generate_statistics_report(comments_a, comments_b, deduped_a, deduped_b, 
                              merged_comments, all_duplicates, unique_a, unique_b,
                              file_a_name, file_b_name, output_dir, timestamp, logger)


def generate_statistics_report(comments_a, comments_b, deduped_a, deduped_b,
                              merged_comments, all_duplicates, unique_a, unique_b,
                              file_a_name, file_b_name, output_dir, timestamp, logger):
    """
    ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
    """
    report_file = os.path.join(output_dir, f'å»é‡ç»Ÿè®¡æŠ¥å‘Š_{timestamp}.txt')
    
    try:
        # ä½¿ç”¨UTF-8-BOMç¼–ç ï¼Œç¡®ä¿åœ¨Windowsè®°äº‹æœ¬ç­‰ç¨‹åºä¸­æ­£ç¡®æ˜¾ç¤ºä¸­æ–‡
        with open(report_file, 'w', encoding='utf-8-sig') as f:
            f.write("CSVæ–‡ä»¶å»é‡ç»Ÿè®¡æŠ¥å‘Š\n")
            f.write("=" * 50 + "\n")
            f.write(f"å¤„ç†æ—¶é—´: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %Hæ—¶%Måˆ†%Sç§’')}\n")
            f.write(f"æ–‡ä»¶A: {file_a_name}\n")
            f.write(f"æ–‡ä»¶B: {file_b_name}\n\n")
            
            f.write("åŸå§‹æ•°æ®ç»Ÿè®¡:\n")
            f.write(f"  æ–‡ä»¶AåŸå§‹è®°å½•æ•°: {len(comments_a)}\n")
            f.write(f"  æ–‡ä»¶BåŸå§‹è®°å½•æ•°: {len(comments_b)}\n")
            f.write(f"  æ€»åŸå§‹è®°å½•æ•°: {len(comments_a) + len(comments_b)}\n\n")
            
            f.write("å»é‡åç»Ÿè®¡:\n")
            f.write(f"  æ–‡ä»¶Aå»é‡å: {len(deduped_a)}\n")
            f.write(f"  æ–‡ä»¶Bå»é‡å: {len(deduped_b)}\n")
            f.write(f"  åˆå¹¶å»é‡å: {len(merged_comments)}\n")
            f.write(f"  æ€»é‡å¤è®°å½•æ•°: {len(all_duplicates)}\n\n")
            
            f.write("ç‹¬æœ‰æ•°æ®ç»Ÿè®¡:\n")
            f.write(f"æ–‡ä»¶Aç‹¬æœ‰: {len(unique_a)}\n")
            f.write(f"æ–‡ä»¶Bç‹¬æœ‰: {len(unique_b)}\n\n")
            
            # è®¡ç®—é‡å¤ç‡
            if len(comments_a) + len(comments_b) > 0:
                duplicate_rate = len(all_duplicates) / (len(comments_a) + len(comments_b)) * 100
                f.write(f"æ€»é‡å¤ç‡: {duplicate_rate:.2f}%\n")
            
            # è®¡ç®—äº¤é›†ç‡
            if len(deduped_a) > 0 and len(deduped_b) > 0:
                intersection = len(deduped_a) + len(deduped_b) - len(merged_comments)
                intersection_rate_a = intersection / len(deduped_a) * 100
                intersection_rate_b = intersection / len(deduped_b) * 100
                f.write(f"æ–‡ä»¶Aä¸æ–‡ä»¶Bçš„äº¤é›†ç‡: {intersection_rate_a:.2f}% (åŸºäºæ–‡ä»¶A)\n")
                f.write(f"æ–‡ä»¶Aä¸æ–‡ä»¶Bçš„äº¤é›†ç‡: {intersection_rate_b:.2f}% (åŸºäºæ–‡ä»¶B)\n")
        
        logger.info(f"ç»Ÿè®¡æŠ¥å‘Šå·²ä¿å­˜åˆ° {report_file}")
    except Exception as e:
        logger.error(f"ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Šå¤±è´¥: {str(e)}")


def main():
    """ä¸»å‡½æ•°"""
    print("CSVæ–‡ä»¶å»é‡å·¥å…·")
    print("=" * 30)
    
    # äº¤äº’å¼è¾“å…¥æ–‡ä»¶Aè·¯å¾„
    while True:
        file_a = input("è¯·è¾“å…¥æ–‡ä»¶Açš„è·¯å¾„: ").strip()
        if not file_a:
            print("æ–‡ä»¶Aè·¯å¾„ä¸èƒ½ä¸ºç©ºï¼Œè¯·é‡æ–°è¾“å…¥")
            continue
        
        # å¤„ç†å¼•å·
        file_a = file_a.strip('"\'')
        
        if not os.path.exists(file_a):
            print(f"é”™è¯¯ï¼šæ–‡ä»¶ {file_a} ä¸å­˜åœ¨ï¼Œè¯·é‡æ–°è¾“å…¥")
            continue
        
        if not file_a.lower().endswith('.csv'):
            print("è­¦å‘Šï¼šæ–‡ä»¶Aä¸æ˜¯CSVæ ¼å¼ï¼Œæ˜¯å¦ç»§ç»­ï¼Ÿ(y/n): ", end="")
            if input().lower() != 'y':
                continue
        
        # éªŒè¯CSVæ–‡ä»¶çš„å­—æ®µå®Œæ•´æ€§
        is_valid, error_msg = validate_csv_file(file_a)
        if not is_valid:
            print(f"é”™è¯¯ï¼šæ–‡ä»¶A {error_msg}ï¼Œè¯·é‡æ–°è¾“å…¥æ–‡ä»¶")
            continue
        
        print("æ–‡ä»¶AéªŒè¯é€šè¿‡")
        break
    
    # äº¤äº’å¼è¾“å…¥æ–‡ä»¶Bè·¯å¾„
    while True:
        file_b = input("è¯·è¾“å…¥æ–‡ä»¶Bçš„è·¯å¾„: ").strip()
        if not file_b:
            print("æ–‡ä»¶Bè·¯å¾„ä¸èƒ½ä¸ºç©ºï¼Œè¯·é‡æ–°è¾“å…¥")
            continue
        
        # å¤„ç†å¼•å·
        file_b = file_b.strip('"\'')
        
        if not os.path.exists(file_b):
            print(f"é”™è¯¯ï¼šæ–‡ä»¶ {file_b} ä¸å­˜åœ¨ï¼Œè¯·é‡æ–°è¾“å…¥")
            continue
        
        if not file_b.lower().endswith('.csv'):
            print("è­¦å‘Šï¼šæ–‡ä»¶Bä¸æ˜¯CSVæ ¼å¼ï¼Œæ˜¯å¦ç»§ç»­ï¼Ÿ(y/n): ", end="")
            if input().lower() != 'y':
                continue
        
        # éªŒè¯CSVæ–‡ä»¶çš„å­—æ®µå®Œæ•´æ€§
        is_valid, error_msg = validate_csv_file(file_b)
        if not is_valid:
            print(f"é”™è¯¯ï¼šæ–‡ä»¶B {error_msg}ï¼Œè¯·é‡æ–°è¾“å…¥æ–‡ä»¶")
            continue
        
        print("æ–‡ä»¶BéªŒè¯é€šè¿‡")
        break
    
    # äº¤äº’å¼è¾“å…¥è¾“å‡ºç›®å½•
    output_input = input("è¯·è¾“å…¥è¾“å‡ºç›®å½•è·¯å¾„ï¼ˆç•™ç©ºåˆ™åœ¨è„šæœ¬åŒç›®å½•ä¸‹åˆ›å»ºæ–‡ä»¶å¤¹ï¼‰: ").strip()
    
    if output_input:
        # å¤„ç†å¼•å·
        output_dir = output_input.strip('"\'')
        output_dir = os.path.abspath(output_dir)
    else:
        # é»˜è®¤åœ¨è„šæœ¬åŒç›®å½•ä¸‹åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹
        script_dir = os.path.dirname(os.path.abspath(__file__))
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_dir = os.path.join(script_dir, f'csv_deduplication_output_{timestamp}')
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    try:
        os.makedirs(output_dir, exist_ok=True)
        print(f"è¾“å‡ºç›®å½•: {output_dir}")
    except Exception as e:
        print(f"åˆ›å»ºè¾“å‡ºç›®å½•å¤±è´¥: {str(e)}")
        sys.exit(1)
    
    # è®¾ç½®æ—¥å¿—
    logger = setup_logger(output_dir)
    
    logger.info("CSVæ–‡ä»¶å»é‡å·¥å…·å¯åŠ¨")
    logger.info(f"æ–‡ä»¶A: {file_a}")
    logger.info(f"æ–‡ä»¶B: {file_b}")
    logger.info(f"è¾“å‡ºç›®å½•: {output_dir}")
    
    print("\nå¼€å§‹å¤„ç†...")
    
    try:
        # æ‰§è¡Œå»é‡å¤„ç†
        process_csv_deduplication(file_a, file_b, output_dir, logger)
        logger.info("CSVæ–‡ä»¶å»é‡å¤„ç†å®Œæˆ")
        print("\nå¤„ç†å®Œæˆï¼è¯·æŸ¥çœ‹è¾“å‡ºç›®å½•ä¸­çš„ç»“æœæ–‡ä»¶ã€‚")
    except Exception as e:
        logger.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        print(f"\nå¤„ç†å¤±è´¥: {str(e)}")
        sys.exit(1)


if __name__ == '__main__':
    main()