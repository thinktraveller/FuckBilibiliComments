#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Bç«™è¯„è®ºçˆ¬å–å·¥å…·

è¯¥è„šæœ¬ç”¨äºçˆ¬å–Bç«™è§†é¢‘è¯„è®ºï¼Œæ”¯æŒå¤šç§æ¨¡å¼ï¼š
- æµ‹è¯•æ¨¡å¼ï¼šå¿«é€Ÿæµ‹è¯•çˆ¬å–åŠŸèƒ½
- è¿­ä»£æ¨¡å¼ï¼šæŒç»­çˆ¬å–ç›´åˆ°æ»¡è¶³æ¡ä»¶
- ç»¼åˆæ¨¡å¼ï¼šå…¨é¢çˆ¬å–å¹¶ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š

Pythonç‰ˆæœ¬è¦æ±‚ï¼šPython 3.7+

ä½œè€…:thinktraveller
æ—¥æœŸ:2025å¹´8æœˆ
ç‰ˆæœ¬:1.0
"""

# ä¾èµ–æ£€æµ‹å’Œè‡ªåŠ¨å®‰è£…
import sys
import subprocess
import importlib

def check_and_install_dependencies():
    """
    æ£€æµ‹å¹¶è‡ªåŠ¨å®‰è£…å¿…è¦çš„ä¾èµ–åŒ…
    """
    required_packages = {
        'requests': 'requests>=2.25.1',
    }
    
    missing_packages = []
    
    print("ğŸ” æ£€æµ‹ä¾èµ–åŒ…...")
    
    for package_name, package_spec in required_packages.items():
        try:
            importlib.import_module(package_name)
            print(f"âœ… {package_name} å·²å®‰è£…")
        except ImportError:
            print(f"âŒ {package_name} æœªå®‰è£…")
            missing_packages.append(package_spec)
    
    if missing_packages:
        print(f"\nğŸ“¦ å‘ç° {len(missing_packages)} ä¸ªç¼ºå¤±çš„ä¾èµ–åŒ…ï¼Œå¼€å§‹è‡ªåŠ¨å®‰è£…...")
        
        for package in missing_packages:
            try:
                print(f"æ­£åœ¨å®‰è£… {package}...")
                subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])
                print(f"âœ… {package} å®‰è£…æˆåŠŸ")
            except subprocess.CalledProcessError as e:
                print(f"âŒ {package} å®‰è£…å¤±è´¥: {e}")
                print("è¯·æ‰‹åŠ¨å®‰è£…ä¾èµ–åŒ…ï¼š")
                print(f"pip install {package}")
                sys.exit(1)
        
        print("\nğŸ‰ æ‰€æœ‰ä¾èµ–åŒ…å®‰è£…å®Œæˆï¼")
    else:
        print("\nâœ… æ‰€æœ‰ä¾èµ–åŒ…å·²æ»¡è¶³è¦æ±‚")

# æ£€æŸ¥Pythonç‰ˆæœ¬
if sys.version_info < (3, 7):
    print("âŒ é”™è¯¯ï¼šæ­¤è„šæœ¬éœ€è¦Python 3.7æˆ–æ›´é«˜ç‰ˆæœ¬")
    print(f"å½“å‰Pythonç‰ˆæœ¬ï¼š{sys.version}")
    print("è¯·å‡çº§Pythonç‰ˆæœ¬åé‡è¯•")
    sys.exit(1)

# è‡ªåŠ¨æ£€æµ‹å’Œå®‰è£…ä¾èµ–
check_and_install_dependencies()

# å¯¼å…¥æ‰€éœ€æ¨¡å—
import requests
import json
import csv
from datetime import datetime
import time
import hashlib
import re
import logging
import os
from collections import Counter
import shutil


# è‡ªå®šä¹‰å¼‚å¸¸ç±»
class CookieBannedException(Exception):
    """Cookieè¢«å°ç¦æ—¶æŠ›å‡ºçš„å¼‚å¸¸"""
    pass


def cleanup_output_files(output_folder, logger=None):
    """
    æ¸…ç†è¾“å‡ºæ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶
    
    Args:
        output_folder (str): è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„
        logger: æ—¥å¿—è®°å½•å™¨
    """
    try:
        if os.path.exists(output_folder):
            # åˆ é™¤æ•´ä¸ªè¾“å‡ºæ–‡ä»¶å¤¹
            shutil.rmtree(output_folder)
            if logger:
                logger.info(f"å·²åˆ é™¤è¾“å‡ºæ–‡ä»¶å¤¹: {output_folder}")
            print(f"ğŸ—‘ï¸  å·²åˆ é™¤æ‰€æœ‰è¾“å‡ºæ–‡ä»¶: {output_folder}")
        else:
            if logger:
                logger.warning(f"è¾“å‡ºæ–‡ä»¶å¤¹ä¸å­˜åœ¨: {output_folder}")
            print(f"âš ï¸  è¾“å‡ºæ–‡ä»¶å¤¹ä¸å­˜åœ¨: {output_folder}")
    except Exception as e:
        if logger:
            logger.error(f"åˆ é™¤è¾“å‡ºæ–‡ä»¶å¤±è´¥: {e}")
        print(f"âŒ åˆ é™¤è¾“å‡ºæ–‡ä»¶å¤±è´¥: {e}")


def handle_cookie_banned_error(output_folder, logger=None):
    """
    å¤„ç†Cookieè¢«å°ç¦çš„é”™è¯¯
    
    Args:
        output_folder (str): è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„
        logger: æ—¥å¿—è®°å½•å™¨
    """
    print("\n" + "="*60)
    print("ğŸš« Cookieè¢«æš‚æ—¶å°ç¦ - ç¨‹åºä¸­æ–­")
    print("="*60)
    print("\nğŸ“‹ é”™è¯¯è¯´æ˜:")
    print("   å½“å‰ä½¿ç”¨çš„Cookieå·²è¢«Bç«™æš‚æ—¶å°ç¦ï¼ˆ412é”™è¯¯ï¼‰")
    print("   è¿™é€šå¸¸æ˜¯ç”±äºè¯·æ±‚é¢‘ç‡è¿‡é«˜æˆ–å…¶ä»–åçˆ¬æœºåˆ¶è§¦å‘")
    
    print("\nğŸ”§ è§£å†³æ–¹æ¡ˆ:")
    print("   1. æ›´æ¢æ–°çš„Cookieï¼ˆæ¨èï¼‰")
    print("      - é‡æ–°ç™»å½•Bç«™è·å–æ–°çš„Cookie")
    print("      - æ›´æ–°config.jsonä¸­çš„Cookieé…ç½®")
    print("   2. ç­‰å¾…ä¸€æ®µæ—¶é—´åé‡è¯•")
    print("      - å»ºè®®ç­‰å¾…30åˆ†é’Ÿåˆ°2å°æ—¶")
    print("      - ä¸‹æ¬¡è¿è¡Œæ—¶é€‚å½“å»¶é•¿ç­‰å¾…æ—¶é—´")
    
    print("\nâš ï¸  å»ºè®®æªæ–½:")
    print("   - å¢åŠ è¯·æ±‚é—´éš”æ—¶é—´ï¼ˆdelay_mså‚æ•°ï¼‰")
    print("   - é¿å…çŸ­æ—¶é—´å†…é¢‘ç¹è¿è¡Œè„šæœ¬")
    print("   - ä½¿ç”¨ä¸åŒçš„è´¦å·Cookieè½®æ¢ä½¿ç”¨")
    
    # æ¸…ç†å·²ç”Ÿæˆçš„æ–‡ä»¶
    print("\nğŸ—‘ï¸  æ­£åœ¨æ¸…ç†å·²ç”Ÿæˆçš„æ–‡ä»¶...")
    cleanup_output_files(output_folder, logger)
    
    print("\n" + "="*60)
    print("ç¨‹åºå·²å®‰å…¨é€€å‡ºï¼Œè¯·æŒ‰ç…§ä¸Šè¿°å»ºè®®å¤„ç†åé‡è¯•")
    print("="*60)


# BVå·è½¬æ¢ç›¸å…³å¸¸é‡å®šä¹‰
XOR_CODE = 23442827791579
MASK_CODE = 2251799813685247
MAX_AID = 2251799813685248
MIN_AID = 1
BASE = 58
BV_LEN = 12

# Base58ç¼–ç è¡¨
ALPHABET = 'FcwAPNKTMug3GV5Lj7EJnHpWsx4tb8haYeviqBz6rkCy12mUSDQX9RdoZf'

# åˆ›å»ºåå‘æŸ¥æ‰¾å­—å…¸
REVERSE_DICT = {char: idx for idx, char in enumerate(ALPHABET)}

def bvid_to_aid(bvid):
    """
    å°†BVå·è½¬æ¢ä¸ºAVå·ï¼ˆoidï¼‰
    
    Args:
        bvid (str): BVå·
        
    Returns:
        int: AVå·ï¼ˆoidï¼‰ï¼Œå¦‚æœè½¬æ¢å¤±è´¥è¿”å›None
    """
    if not isinstance(bvid, str):
        return None
        
    # æ£€æŸ¥BVå·æ ¼å¼
    if len(bvid) != BV_LEN:
        return None
        
    if not (bvid.upper().startswith('BV1')):
        return None
    
    # è½¬æ¢ä¸ºåˆ—è¡¨ä¾¿äºæ“ä½œ
    bvid_list = list(bvid)
    
    # ä½ç½®äº¤æ¢ï¼ˆé€†å‘æ“ä½œï¼‰
    bvid_list[3], bvid_list[9] = bvid_list[9], bvid_list[3]
    bvid_list[4], bvid_list[7] = bvid_list[7], bvid_list[4]
    
    aid = 0
    
    # Base58è§£ç 
    for i in range(3, BV_LEN):
        char = bvid_list[i]
        if char not in REVERSE_DICT:
            return None
        aid = aid * BASE + REVERSE_DICT[char]
    
    # æ£€æŸ¥èŒƒå›´
    if aid >= (MAX_AID << 1) or aid < MAX_AID:
        return None
    
    # è¿›è¡Œæ©ç å’Œå¼‚æˆ–æ“ä½œ
    aid = (aid & MASK_CODE) ^ XOR_CODE
    
    return aid

def aid_to_bvid(aid):
    """
    å°†AVå·ï¼ˆoidï¼‰è½¬æ¢ä¸ºBVå·
    
    Args:
        aid (int): AVå·ï¼ˆoidï¼‰
        
    Returns:
        str: BVå·ï¼Œå¦‚æœè½¬æ¢å¤±è´¥è¿”å›None
    """
    try:
        aid = int(aid)
    except (ValueError, TypeError):
        return None
        
    if aid < MIN_AID or aid >= MAX_AID:
        return None
    
    # åˆå§‹åŒ–BVå·æ•°ç»„
    bvid = ['B', 'V', '1'] + [''] * 9
    
    # è¿›è¡Œå¼‚æˆ–å’Œæ©ç æ“ä½œ
    aid = (MAX_AID | aid) ^ XOR_CODE
    
    # Base58ç¼–ç 
    for i in range(BV_LEN - 1, 2, -1):
        if aid == 0:
            break
        bvid[i] = ALPHABET[aid % BASE]
        aid //= BASE
    
    # ä½ç½®äº¤æ¢
    bvid[3], bvid[9] = bvid[9], bvid[3]
    bvid[4], bvid[7] = bvid[7], bvid[4]
    
    return ''.join(bvid)

def extract_id_from_url(url):
    """
    ä»Bç«™URLä¸­æå–BVå·ï¼Œä¸¥æ ¼éªŒè¯BVå·æ ¼å¼
    
    Args:
        url (str): Bç«™è§†é¢‘URL
        
    Returns:
        tuple: (id_type, id_value) å…¶ä¸­id_typeä¸º'bv'ï¼Œid_valueä¸ºBVå·ï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆæ³•BVå·åˆ™è¿”å›(None, None)
    """
    if not isinstance(url, str):
        return None, None
    
    # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„BVä½ç½®
    bv_positions = []
    search_start = 0
    
    while True:
        # æŸ¥æ‰¾ä¸‹ä¸€ä¸ª"bv"ä½ç½®ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
        bv_pos = url.lower().find('bv', search_start)
        if bv_pos == -1:
            break
        bv_positions.append(bv_pos)
        search_start = bv_pos + 1
    
    # æ£€æŸ¥æ¯ä¸ªBVä½ç½®åé¢æ˜¯å¦æœ‰åˆæ³•çš„10ä½å­—ç¬¦
    for pos in bv_positions:
        # ç¡®ä¿BVåé¢æœ‰è¶³å¤Ÿçš„å­—ç¬¦
        if pos + 12 <= len(url):  # BV + 10ä½å­—ç¬¦
            candidate = url[pos:pos + 12]  # æå–BV + 10ä½å­—ç¬¦
            
            # éªŒè¯æ ¼å¼ï¼šBV + 10ä½æ•°å­—å’Œå­—æ¯
            if candidate[:2].upper() == 'BV':
                remaining_10_chars = candidate[2:]
                
                # æ£€æŸ¥åé¢10ä½æ˜¯å¦åªåŒ…å«æ•°å­—å’Œå¤§å°å†™å­—æ¯
                if len(remaining_10_chars) == 10 and remaining_10_chars.isalnum():
                    # è¿›ä¸€æ­¥æ£€æŸ¥æ˜¯å¦åŒ…å«éæ³•å­—ç¬¦ï¼ˆå¦‚æ–œæ ç­‰ï¼‰
                    if all(c.isalnum() for c in remaining_10_chars):
                        return 'bv', candidate
    
    return None, None

def parse_video_input(user_input):
    """
    è§£æç”¨æˆ·è¾“å…¥çš„Bç«™URLï¼Œæå–å…¶ä¸­çš„BVå·å¹¶è½¬æ¢ä¸ºoid
    
    Args:
        user_input (str): ç”¨æˆ·è¾“å…¥çš„Bç«™URL
        
    Returns:
        int: è§£æå‡ºçš„oidï¼Œå¦‚æœè§£æå¤±è´¥è¿”å›None
    """
    if not user_input or not isinstance(user_input, str):
        return None
    
    user_input = user_input.strip()
    
    # ä»URLæå–BVå·
    id_type, id_value = extract_id_from_url(user_input)
    
    if id_type == 'bv':
        # BVå·è½¬æ¢ä¸ºoid
        return bvid_to_aid(id_value)
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆæ³•çš„BVå·ï¼Œè¿”å›None
    return None

def get_video_info(bvid, timeout=10):
    """
    é€šè¿‡BVå·è·å–è§†é¢‘ä¿¡æ¯
    
    Args:
        bvid (str): è§†é¢‘çš„BVå·
        timeout (int): è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼Œé»˜è®¤10ç§’
        
    Returns:
        dict: è§†é¢‘ä¿¡æ¯å­—å…¸ï¼Œè·å–å¤±è´¥è¿”å›None
    """
    # Bç«™è§†é¢‘ä¿¡æ¯APIæ¥å£
    url = "https://api.bilibili.com/x/web-interface/view"
    
    # è¯·æ±‚å‚æ•°
    params = {"bvid": bvid}
    
    # è®¾ç½®è¯·æ±‚å¤´
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Referer": "https://www.bilibili.com/"
    }
    
    try:
        # å‘é€è¯·æ±‚
        response = requests.get(url, params=params, headers=headers, timeout=timeout)
        response.raise_for_status()
        
        # è§£æJSONå“åº”
        data = response.json()
        
        # æ£€æŸ¥APIè¿”å›çŠ¶æ€
        if data.get("code") == 0:
            video_data = data.get("data", {})
            
            # æå–å…³é”®ä¿¡æ¯
            video_info = {
                "bvid": video_data.get("bvid", ""),
                "aid": video_data.get("aid", ""),
                "title": video_data.get("title", ""),
                "desc": video_data.get("desc", ""),
                "duration": video_data.get("duration", 0),
                "pubdate": video_data.get("pubdate", 0),
                "owner": {
                    "mid": video_data.get("owner", {}).get("mid", ""),
                    "name": video_data.get("owner", {}).get("name", ""),
                    "face": video_data.get("owner", {}).get("face", "")
                },
                "stat": {
                    "view": video_data.get("stat", {}).get("view", 0),
                    "danmaku": video_data.get("stat", {}).get("danmaku", 0),
                    "reply": video_data.get("stat", {}).get("reply", 0),
                    "favorite": video_data.get("stat", {}).get("favorite", 0),
                    "coin": video_data.get("stat", {}).get("coin", 0),
                    "share": video_data.get("stat", {}).get("share", 0),
                    "like": video_data.get("stat", {}).get("like", 0)
                },
                "pic": video_data.get("pic", ""),
                "tname": video_data.get("tname", "")
            }
            
            return video_info
            
        else:
            error_code = data.get('code')
            error_msg = data.get("message", "æœªçŸ¥é”™è¯¯")
            print(f"APIè¿”å›é”™è¯¯: {error_code} - {error_msg}")
            
            # é’ˆå¯¹å¸¸è§é”™è¯¯æä¾›è§£å†³å»ºè®®
            if error_code == -400:
                print("ğŸ’¡ å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:")
                print("   1. æ£€æŸ¥BVå·æ˜¯å¦æ­£ç¡®")
                print("   2. è§†é¢‘å¯èƒ½å·²è¢«åˆ é™¤æˆ–è®¾ä¸ºç§å¯†")
                print("   3. ç½‘ç»œè¿æ¥é—®é¢˜ï¼Œè¯·ç¨åé‡è¯•")
                print("   4. Bç«™APIå¯èƒ½æš‚æ—¶ä¸å¯ç”¨")
            elif error_code == -403:
                print("ğŸ’¡ è®¿é—®è¢«æ‹’ç»ï¼Œå¯èƒ½éœ€è¦ç™»å½•æˆ–æƒé™ä¸è¶³")
            elif error_code == -404:
                print("ğŸ’¡ è§†é¢‘ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥BVå·æ˜¯å¦æ­£ç¡®")
            
            return None
            
    except requests.exceptions.RequestException as e:
        print(f"ç½‘ç»œè¯·æ±‚é”™è¯¯: {e}")
        return None
    except json.JSONDecodeError as e:
        print(f"JSONè§£æé”™è¯¯: {e}")
        return None
    except Exception as e:
        print(f"è·å–è§†é¢‘ä¿¡æ¯æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        return None

def get_video_title_quick(bvid):
    """
    å¿«é€Ÿè·å–è§†é¢‘æ ‡é¢˜
    
    Args:
        bvid (str): è§†é¢‘çš„BVå·
        
    Returns:
        str: è§†é¢‘æ ‡é¢˜ï¼Œè·å–å¤±è´¥è¿”å›None
    """
    video_info = get_video_info(bvid)
    if video_info:
        return video_info.get("title")
    return None

def load_config():
    """
    åŠ è½½é…ç½®æ–‡ä»¶ï¼Œå¦‚æœé…ç½®ä¸ºç©ºåˆ™æç¤ºç”¨æˆ·è¾“å…¥
    
    Returns:
        dict: åŒ…å«cookieå’Œuser_agentçš„é…ç½®å­—å…¸
    """
    config_file = 'config.json'
    
    # å°è¯•è¯»å–é…ç½®æ–‡ä»¶
    try:
        with open(config_file, 'r', encoding='utf-8') as f:
            config = json.load(f)
    except FileNotFoundError:
        print(f"âŒ é…ç½®æ–‡ä»¶ {config_file} ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶")
        config = {
            "cookie": "",
            "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36",
            "description": {
                "cookie": "ä»æµè§ˆå™¨å¼€å‘è€…å·¥å…·ä¸­å¤åˆ¶çš„å®Œæ•´Cookieå­—ç¬¦ä¸²",
                "user_agent": "æµè§ˆå™¨çš„User-Agentå­—ç¬¦ä¸²ï¼Œç”¨äºæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨è¯·æ±‚"
            }
        }
        save_config(config)
    except json.JSONDecodeError:
        print(f"âŒ é…ç½®æ–‡ä»¶ {config_file} æ ¼å¼é”™è¯¯ï¼Œå°†é‡æ–°åˆ›å»º")
        config = {
            "cookie": "",
            "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36",
            "description": {
                "cookie": "ä»æµè§ˆå™¨å¼€å‘è€…å·¥å…·ä¸­å¤åˆ¶çš„å®Œæ•´Cookieå­—ç¬¦ä¸²",
                "user_agent": "æµè§ˆå™¨çš„User-Agentå­—ç¬¦ä¸²ï¼Œç”¨äºæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨è¯·æ±‚"
            }
        }
        save_config(config)
    
    # æ£€æŸ¥cookieæ˜¯å¦ä¸ºç©º
    if not config.get('cookie', '').strip():
        print("\n=== é…ç½®Cookieä¿¡æ¯ ===")
        print("æ£€æµ‹åˆ°Cookieä¸ºç©ºï¼Œéœ€è¦é…ç½®Cookieæ‰èƒ½æ­£å¸¸çˆ¬å–è¯„è®º")
        print("è¯·æŒ‰ä»¥ä¸‹æ­¥éª¤è·å–Cookieï¼š")
        print("1. æ‰“å¼€æµè§ˆå™¨ï¼Œè®¿é—® https://www.bilibili.com")
        print("2. ç™»å½•ä½ çš„Bç«™è´¦å·")
        print("3. æŒ‰F12æ‰“å¼€å¼€å‘è€…å·¥å…·")
        print("4. åˆ‡æ¢åˆ°Network(ç½‘ç»œ)æ ‡ç­¾")
        print("5. åˆ·æ–°é¡µé¢ï¼Œæ‰¾åˆ°ä»»æ„è¯·æ±‚")
        print("6. åœ¨è¯·æ±‚å¤´ä¸­æ‰¾åˆ°Cookieå­—æ®µï¼Œå¤åˆ¶å®Œæ•´çš„Cookieå€¼")
        print()
        
        while True:
            cookie_input = input("è¯·ç²˜è´´å®Œæ•´çš„Cookieå­—ç¬¦ä¸²ï¼ˆç›´æ¥å›è½¦è·³è¿‡ï¼Œä½†å¯èƒ½å½±å“çˆ¬å–æ•ˆæœï¼‰: ").strip()
            if cookie_input:
                config['cookie'] = cookie_input
                save_config(config)
                print("âœ… Cookieé…ç½®å·²ä¿å­˜")
                break
            else:
                confirm = input("ç¡®å®šè¦è·³è¿‡Cookieé…ç½®å—ï¼Ÿè¿™å¯èƒ½å¯¼è‡´çˆ¬å–å¤±è´¥ (y/N): ").strip().lower()
                if confirm == 'y':
                    print("âš ï¸ å·²è·³è¿‡Cookieé…ç½®ï¼Œå¦‚æœçˆ¬å–å¤±è´¥è¯·é‡æ–°é…ç½®")
                    break
    
    # æ£€æŸ¥user_agentæ˜¯å¦éœ€è¦æ›´æ–°
    if not config.get('user_agent', '').strip():
        config['user_agent'] = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36"
        save_config(config)
    
    return config

def save_config(config):
    """
    ä¿å­˜é…ç½®æ–‡ä»¶
    
    Args:
        config (dict): é…ç½®å­—å…¸
    """
    config_file = 'config.json'
    try:
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=4)
    except Exception as e:
        print(f"âŒ ä¿å­˜é…ç½®æ–‡ä»¶å¤±è´¥: {e}")

def get_request_headers(config):
    """
    æ ¹æ®é…ç½®ç”Ÿæˆè¯·æ±‚å¤´
    
    Args:
        config (dict): é…ç½®å­—å…¸
    
    Returns:
        dict: è¯·æ±‚å¤´å­—å…¸
    """
    return {
        'Cookie': config.get('cookie', ''),
        'User-Agent': config.get('user_agent', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36'),
        'Referer': 'https://www.bilibili.com',
        'Accept': 'application/json, text/plain, */*',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Sec-Fetch-Dest': 'empty',
        'Sec-Fetch-Mode': 'cors',
        'Sec-Fetch-Site': 'same-site',
        'sec-ch-ua': '"Google Chrome";v="131", "Chromium";v="131", "Not_A Brand";v="24"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-platform': '"Windows"'
    }

def generate_safe_filename(video_title, oid, suffix="", file_type="original"):
    """
    ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶åï¼ŒåŸºäºè§†é¢‘æ ‡é¢˜å’Œæ—¶é—´æˆ³
    
    Args:
        video_title (str): è§†é¢‘æ ‡é¢˜
        oid (str): è§†é¢‘oid
        suffix (str): æ–‡ä»¶ååç¼€
        file_type (str): æ–‡ä»¶ç±»å‹ - "original", "final", "stats", "log"
    
    Returns:
        str: å®‰å…¨çš„æ–‡ä»¶å
    """
    # ä½¿ç”¨YYYYMMDDæ ¼å¼çš„æ—¥æœŸå’ŒHHMMSSæ ¼å¼çš„æ—¶é—´
    date_str = datetime.now().strftime('%Y%m%d')
    time_str = datetime.now().strftime('%H%M%S')
    
    if video_title:
        # æ¸…ç†è§†é¢‘æ ‡é¢˜ä¸­çš„éæ³•å­—ç¬¦
        safe_title = re.sub(r'[<>:"/\\|?*]', '_', video_title)
        # é™åˆ¶æ ‡é¢˜é•¿åº¦é¿å…è·¯å¾„è¿‡é•¿
        if len(safe_title) > 30:
            safe_title = safe_title[:30] + '...'
        
        # è·å–BVå·
        try:
            bv_id = aid_to_bvid(int(oid))
        except:
            bv_id = f"AV{oid}"
        
        # æ ¹æ®æ–‡ä»¶ç±»å‹ç”Ÿæˆä¸åŒçš„å‘½åæ ¼å¼
        if file_type == "original":
            # åŸå§‹æ•°æ®æ–‡ä»¶ï¼šè¯„è®ºçˆ¬å–åŸå§‹æ•°æ®_{æè¿°}_{è§†é¢‘æ ‡é¢˜}_{BVå·}_{æ—¶åˆ†ç§’}_{æ—¥æœŸ}
            if suffix:
                base_name = f"è¯„è®ºçˆ¬å–åŸå§‹æ•°æ®_{suffix}_{safe_title}_{bv_id}_{time_str}_{date_str}"
            else:
                base_name = f"è¯„è®ºçˆ¬å–åŸå§‹æ•°æ®_{safe_title}_{bv_id}_{time_str}_{date_str}"
        elif file_type == "final":
            # æœ€ç»ˆæ–‡ä»¶ï¼šè¯„è®ºçˆ¬å–_{æ’åºæ–¹å¼}_{è§†é¢‘æ ‡é¢˜}_{BVå·}_{æ—¶åˆ†ç§’}_{æ—¥æœŸ}
            if suffix:
                base_name = f"è¯„è®ºçˆ¬å–_{suffix}_{safe_title}_{bv_id}_{time_str}_{date_str}"
            else:
                base_name = f"è¯„è®ºçˆ¬å–_{safe_title}_{bv_id}_{time_str}_{date_str}"
        elif file_type == "stats":
            # ç»Ÿè®¡æ–‡ä»¶ï¼šè¯„è®ºçˆ¬å–ç»Ÿè®¡ç»“æœ_{ç»Ÿè®¡ç±»å‹}_{è§†é¢‘æ ‡é¢˜}_{BVå·}_{æ—¶åˆ†ç§’}_{æ—¥æœŸ}
            if suffix:
                base_name = f"è¯„è®ºçˆ¬å–_{suffix}_{safe_title}_{bv_id}_{time_str}_{date_str}"
            else:
                base_name = f"è¯„è®ºçˆ¬å–_{safe_title}_{bv_id}_{time_str}_{date_str}"
        elif file_type == "log":
            # æ—¥å¿—æ–‡ä»¶ï¼šè¯„è®ºçˆ¬å–æ—¥å¿—_{è§†é¢‘æ ‡é¢˜}_{BVå·}_{æ—¶åˆ†ç§’}_{æ—¥æœŸ}_{é¡µé¢ä¿¡æ¯}
            if suffix:
                base_name = f"è¯„è®ºçˆ¬å–æ—¥å¿—_{safe_title}_{bv_id}_{time_str}_{date_str}_{suffix}"
            else:
                base_name = f"è¯„è®ºçˆ¬å–æ—¥å¿—_{safe_title}_{bv_id}_{time_str}_{date_str}"
        elif file_type == "other":
            # å…¶ä»–ç±»å‹æ–‡ä»¶ï¼ˆå¦‚æ–‡æ¡£ã€è¯´æ˜ç­‰ï¼‰
            if suffix:
                base_name = f"è¯„è®ºçˆ¬å–_{suffix}_{safe_title}_{bv_id}_{time_str}_{date_str}"
            else:
                base_name = f"è¯„è®ºçˆ¬å–_å…¶ä»–æ–‡ä»¶_{safe_title}_{bv_id}_{time_str}_{date_str}"
        else:
            # é»˜è®¤æ ¼å¼
            base_name = f"{safe_title}_{bv_id}_{time_str}_{date_str}"
    else:
        # å½“video_titleä¸ºç©ºæ—¶ï¼Œä»ç„¶æ ¹æ®file_typeç”Ÿæˆæ­£ç¡®çš„æ–‡ä»¶åæ ¼å¼
        try:
            bv_id = aid_to_bvid(int(oid))
        except:
            bv_id = f"AV{oid}"
        
        # æ ¹æ®æ–‡ä»¶ç±»å‹ç”Ÿæˆä¸åŒçš„å‘½åæ ¼å¼
        if file_type == "original":
            # åŸå§‹æ•°æ®æ–‡ä»¶
            if suffix:
                base_name = f"è¯„è®ºçˆ¬å–åŸå§‹æ•°æ®_{suffix}_{bv_id}_{time_str}_{date_str}"
            else:
                base_name = f"è¯„è®ºçˆ¬å–åŸå§‹æ•°æ®_{bv_id}_{time_str}_{date_str}"
        elif file_type == "final":
            # æœ€ç»ˆæ–‡ä»¶
            if suffix:
                base_name = f"è¯„è®ºçˆ¬å–_{suffix}_{bv_id}_{time_str}_{date_str}"
            else:
                base_name = f"è¯„è®ºçˆ¬å–_{bv_id}_{time_str}_{date_str}"
        elif file_type == "stats":
            # ç»Ÿè®¡æ–‡ä»¶
            if suffix:
                base_name = f"è¯„è®ºçˆ¬å–ç»Ÿè®¡ç»“æœ_{suffix}_{bv_id}_{time_str}_{date_str}"
            else:
                base_name = f"è¯„è®ºçˆ¬å–ç»Ÿè®¡ç»“æœ_{bv_id}_{time_str}_{date_str}"
        elif file_type == "log":
            # æ—¥å¿—æ–‡ä»¶
            if suffix:
                base_name = f"è¯„è®ºçˆ¬å–æ—¥å¿—_{bv_id}_{time_str}_{date_str}_{suffix}"
            else:
                base_name = f"è¯„è®ºçˆ¬å–æ—¥å¿—_{bv_id}_{time_str}_{date_str}"
        elif file_type == "other":
            # å…¶ä»–ç±»å‹æ–‡ä»¶ï¼ˆå¦‚æ–‡æ¡£ã€è¯´æ˜ç­‰ï¼‰
            if suffix:
                base_name = f"è¯„è®ºçˆ¬å–_{suffix}_{bv_id}_{time_str}_{date_str}"
            else:
                base_name = f"è¯„è®ºçˆ¬å–_å…¶ä»–æ–‡ä»¶_{bv_id}_{time_str}_{date_str}"
        else:
            # æœªçŸ¥æ–‡ä»¶ç±»å‹ï¼Œä½¿ç”¨é€šç”¨æ ¼å¼
            if suffix:
                base_name = f"è¯„è®ºçˆ¬å–_{file_type}_{suffix}_{bv_id}_{time_str}_{date_str}"
            else:
                base_name = f"è¯„è®ºçˆ¬å–_{file_type}_{bv_id}_{time_str}_{date_str}"
    
    return base_name

def create_output_folder(oid, video_title=None, mode_type=None):
    """
    åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹
    
    Args:
        oid (str): è§†é¢‘oidï¼Œç”¨äºç”Ÿæˆæ–‡ä»¶å¤¹å
        video_title (str, optional): è§†é¢‘æ ‡é¢˜ï¼Œç”¨äºç”Ÿæˆæ–‡ä»¶å¤¹å
        mode_type (str, optional): è¿è¡Œæ¨¡å¼ç±»å‹ï¼Œç”¨äºç”Ÿæˆç‰¹å®šçš„æ–‡ä»¶å¤¹å
            - "test_time": æµ‹è¯•æ¨¡å¼æ—¶é—´æ’åº
            - "test_popularity": æµ‹è¯•æ¨¡å¼çƒ­åº¦æ’åº
            - "iteration_time": è¿­ä»£æ¨¡å¼é™å®šæ—¶é—´
            - "iteration_rate": è¿­ä»£æ¨¡å¼é™å®šé‡å¤ç‡
            - "comprehensive": ç»¼åˆæ¨¡å¼
            - None: é»˜è®¤æ¨¡å¼
    
    Returns:
        str: åˆ›å»ºçš„æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆæ—¥å¿—æ–‡ä»¶ç›´æ¥ä¿å­˜åœ¨æ­¤æ–‡ä»¶å¤¹ä¸‹ï¼‰
    """
    date_str = datetime.now().strftime('%Y%m%d')
    time_str = datetime.now().strftime('%H%M%S')  # æ·»åŠ æ—¶åˆ†ç§’æ ¼å¼
    
    if video_title:
        # æ¸…ç†è§†é¢‘æ ‡é¢˜ä¸­çš„éæ³•å­—ç¬¦
        safe_title = re.sub(r'[<>:"/\\|?*]', '_', video_title)
        # é™åˆ¶æ ‡é¢˜é•¿åº¦é¿å…è·¯å¾„è¿‡é•¿
        if len(safe_title) > 30:
            safe_title = safe_title[:30] + '...'
        
        # è·å–BVå·
        try:
            bv_id = aid_to_bvid(int(oid))
        except:
            bv_id = f"AV{oid}"
        
        # æ ¹æ®æ¨¡å¼ç±»å‹ç”Ÿæˆä¸åŒçš„æ–‡ä»¶å¤¹å
        if mode_type == "test_time":
            folder_name = f"è¯„è®ºçˆ¬å–_æµ‹è¯•æ¨¡å¼æ—¶é—´æ’åº_{safe_title}_{bv_id}_{time_str}_{date_str}"
        elif mode_type == "test_popularity":
            folder_name = f"è¯„è®ºçˆ¬å–_æµ‹è¯•æ¨¡å¼çƒ­åº¦æ’åº_{safe_title}_{bv_id}_{time_str}_{date_str}"
        elif mode_type == "iteration_time":
            folder_name = f"è¯„è®ºçˆ¬å–_è¿­ä»£æ¨¡å¼é™å®šæ—¶é—´_{safe_title}_{bv_id}_{time_str}_{date_str}"
        elif mode_type == "iteration_rate":
            folder_name = f"è¯„è®ºçˆ¬å–_è¿­ä»£æ¨¡å¼é™å®šé‡å¤ç‡_{safe_title}_{bv_id}_{time_str}_{date_str}"
        elif mode_type == "comprehensive":
            folder_name = f"è¯„è®ºçˆ¬å–_ç»¼åˆæ¨¡å¼_{safe_title}_{bv_id}_{time_str}_{date_str}"
        else:
            # é»˜è®¤æ ¼å¼ï¼šè¯„è®ºçˆ¬å–_è§†é¢‘æ ‡é¢˜_BVå·_æ—¶åˆ†ç§’_æ—¥æœŸ
            folder_name = f"è¯„è®ºçˆ¬å–_{safe_title}_{bv_id}_{time_str}_{date_str}"
    else:
        # å¦‚æœæ²¡æœ‰è§†é¢‘æ ‡é¢˜ï¼Œä½¿ç”¨ç®€åŒ–æ ¼å¼
        try:
            bv_id = aid_to_bvid(int(oid))
        except:
            bv_id = f"AV{oid}"
        folder_name = f"bilibili_crawler_{oid}_{date_str}"
    
    # åˆ›å»ºä¸»æ–‡ä»¶å¤¹
    if not os.path.exists(folder_name):
        os.makedirs(folder_name)
    
    return folder_name

def setup_logging(oid, output_folder):
    """
    è®¾ç½®æ—¥å¿—é…ç½®
    
    Args:
        oid (str): è§†é¢‘oidï¼Œç”¨äºç”Ÿæˆæ—¥å¿—æ–‡ä»¶å
        output_folder (str): è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„
    
    Returns:
        tuple: (é…ç½®å¥½çš„æ—¥å¿—è®°å½•å™¨, ä¸»æ—¥å¿—æ–‡ä»¶è·¯å¾„)
    """
    # åˆ›å»ºlogså­æ–‡ä»¶å¤¹
    logs_folder = os.path.join(output_folder, 'logs')
    if not os.path.exists(logs_folder):
        os.makedirs(logs_folder)
    
    # ç”Ÿæˆä¸»æ—¥å¿—æ–‡ä»¶åï¼Œä¿å­˜åˆ°logsæ–‡ä»¶å¤¹
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    main_log_filename = os.path.join(logs_folder, f'{timestamp}_bilibili_crawler_{oid}_main.log')
    
    # ç¦ç”¨requestså’Œurllib3çš„DEBUGæ—¥å¿—ï¼Œé¿å…å¹²æ‰°æˆ‘ä»¬çš„è‡ªå®šä¹‰æ—¥å¿—
    logging.getLogger('urllib3.connectionpool').setLevel(logging.WARNING)
    logging.getLogger('requests.packages.urllib3.connectionpool').setLevel(logging.WARNING)
    
    # é…ç½®æ—¥å¿—æ ¼å¼
    logging.basicConfig(
        level=logging.DEBUG,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(main_log_filename, encoding='utf-8'),
            # ä¸æ·»åŠ æ§åˆ¶å°å¤„ç†å™¨ï¼Œé¿å…åœ¨ç»ˆç«¯æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—
        ]
    )
    
    logger = logging.getLogger(__name__)
    logger.info(f"å¼€å§‹çˆ¬å–è§†é¢‘ oid={oid} çš„è¯„è®º")
    logger.info(f"ä¸»æ—¥å¿—æ–‡ä»¶: {main_log_filename}")
    logger.info(f"æ¯é¡µè¯·æ±‚æ—¥å¿—å°†ä¿å­˜åœ¨: {logs_folder}")
    
    return logger, main_log_filename

def create_page_logger(output_folder, oid, page_num):
    """
    ä¸ºæ¯é¡µè¯·æ±‚åˆ›å»ºå•ç‹¬çš„æ—¥å¿—è®°å½•å™¨
    
    Args:
        output_folder (str): è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„
        oid (str): è§†é¢‘oid
        page_num (int): é¡µç 
    
    Returns:
        tuple: (é¡µé¢æ—¥å¿—è®°å½•å™¨, é¡µé¢æ—¥å¿—æ–‡ä»¶è·¯å¾„)
    """
    # ç¡®ä¿logså­æ–‡ä»¶å¤¹å­˜åœ¨
    logs_folder = os.path.join(output_folder, 'logs')
    if not os.path.exists(logs_folder):
        os.makedirs(logs_folder)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S_%f')[:-3]  # åŒ…å«æ¯«ç§’
    page_log_filename = os.path.join(logs_folder, f'{timestamp}_page_{page_num:04d}_{oid}.log')
    
    # åˆ›å»ºé¡µé¢ä¸“ç”¨çš„æ—¥å¿—è®°å½•å™¨
    page_logger = logging.getLogger(f'page_{page_num}_{oid}')
    page_logger.setLevel(logging.DEBUG)
    
    # æ¸…é™¤ä¹‹å‰çš„å¤„ç†å™¨
    page_logger.handlers.clear()
    
    # æ·»åŠ æ–‡ä»¶å¤„ç†å™¨
    page_handler = logging.FileHandler(page_log_filename, encoding='utf-8')
    page_handler.setLevel(logging.DEBUG)
    page_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    page_handler.setFormatter(page_formatter)
    page_logger.addHandler(page_handler)
    
    # é˜²æ­¢æ—¥å¿—ä¼ æ’­åˆ°çˆ¶è®°å½•å™¨
    page_logger.propagate = False
    
    page_logger.info(f"å¼€å§‹è®°å½•ç¬¬ {page_num} é¡µçš„è¯·æ±‚å’Œå“åº”")
    page_logger.info(f"è§†é¢‘oid: {oid}")
    
    return page_logger, page_log_filename

def validate_oid(oid):
    """
    éªŒè¯oidæ ¼å¼æ˜¯å¦æ­£ç¡®
    
    Args:
        oid (str): è¾“å…¥çš„oid
    
    Returns:
        bool: æ ¼å¼æ˜¯å¦æ­£ç¡®
    """
    # oidåº”è¯¥æ˜¯çº¯æ•°å­—
    return oid.isdigit() and len(oid) > 0

def get_user_input():
    """
    è·å–ç”¨æˆ·è¾“å…¥çš„Bç«™è§†é¢‘ç½‘å€ï¼Œæå–BVå·å¹¶è½¬æ¢ä¸ºoid
    
    Returns:
        tuple: (oid, mode, ps, delay_ms, max_pages, test_sort_mode, iteration_config) æˆ– (None, None, None, None, None, None, None) å¦‚æœè¾“å…¥æ— æ•ˆ
    """
    print("=== Bç«™è¯„è®ºçˆ¬è™« ===")
    print("æ”¯æŒç»¼åˆæ¨¡å¼ã€æµ‹è¯•æ¨¡å¼å’Œè¿­ä»£æ¨¡å¼")
    print()
    print("è¯·è¾“å…¥Bç«™è§†é¢‘ç½‘å€ï¼Œç¨‹åºå°†è‡ªåŠ¨æå–å…¶ä¸­çš„BVå·")
    print("ç¤ºä¾‹ï¼šhttps://www.bilibili.com/video/BV1kJ411N7AB")
    print()
    
    # è·å–è§†é¢‘ç½‘å€å¹¶è½¬æ¢ä¸ºoid
    video_input = input("è¯·è¾“å…¥Bç«™è§†é¢‘ç½‘å€: ").strip()
    
    if not video_input:
        print("âŒ è¾“å…¥ä¸èƒ½ä¸ºç©º")
        print("ç¨‹åºé€€å‡º")
        return None, None, None, None, None, None, None
    
    # è§£æç”¨æˆ·è¾“å…¥çš„ç½‘å€
    oid = parse_video_input(video_input)
    
    if oid is None:
        print(f"âŒ è¾“å…¥ç½‘å€é”™è¯¯: {video_input}")
        print("æœªèƒ½åœ¨ç½‘å€ä¸­æ‰¾åˆ°åˆæ³•çš„BVå·")
        print("BVå·æ ¼å¼è¦æ±‚ï¼šBV + 10ä½æ•°å­—å’Œå­—æ¯ç»„åˆï¼ˆä¸åŒ…å«æ–œæ ç­‰ç‰¹æ®Šç¬¦å·ï¼‰")
        print("ç¨‹åºé€€å‡º")
        return None, None, None, None, None, None, None
    
    # éªŒè¯oidæ˜¯å¦æœ‰æ•ˆ
    if not isinstance(oid, int) or oid <= 0:
        print(f"âŒ BVå·è½¬æ¢åçš„oidæ— æ•ˆ: {oid}")
        print("ç¨‹åºé€€å‡º")
        return None, None, None, None, None, None, None
    
    # æ˜¾ç¤ºè½¬æ¢ç»“æœ
    id_type, bv_id = extract_id_from_url(video_input)
    if id_type == 'bv':
        print(f"âœ… ç½‘å€è§£ææˆåŠŸ: {bv_id} â†’ oid: {oid}")
    
    # è·å–è§†é¢‘ä¿¡æ¯å¹¶è®©ç”¨æˆ·ç¡®è®¤
    print("\nğŸ” æ­£åœ¨è·å–è§†é¢‘ä¿¡æ¯...")
    video_info = None
    try:
        # è·å–å®Œæ•´çš„è§†é¢‘ä¿¡æ¯
        video_info = get_video_info(bv_id)
        if video_info and video_info.get('title'):
            video_title = video_info['title']
            print(f"ğŸ“º è§†é¢‘æ ‡é¢˜: {video_title}")
            
            # è®©ç”¨æˆ·ç¡®è®¤è§†é¢‘ä¿¡æ¯
            while True:
                confirm = input("\nè¯·ç¡®è®¤è¿™æ˜¯æ‚¨è¦çˆ¬å–çš„è§†é¢‘å—ï¼Ÿ(y/nï¼Œç›´æ¥å›è½¦é»˜è®¤ç¡®è®¤): ").strip().lower()
                if not confirm or confirm in ['y', 'yes', 'æ˜¯']:
                    print("âœ… è§†é¢‘ä¿¡æ¯ç¡®è®¤å®Œæˆ")
                    break
                elif confirm in ['n', 'no', 'å¦']:
                    print("âŒ ç”¨æˆ·å–æ¶ˆæ“ä½œ")
                    print("ç¨‹åºé€€å‡º")
                    return None, None, None, None, None, None, None, video_info
                else:
                    print("âŒ è¯·è¾“å…¥ y æˆ– n")
        else:
            # å¦‚æœè·å–å®Œæ•´ä¿¡æ¯å¤±è´¥ï¼Œå°è¯•åªè·å–æ ‡é¢˜
            video_title = get_video_title_quick(bv_id)
            if video_title:
                print(f"ğŸ“º è§†é¢‘æ ‡é¢˜: {video_title}")
                print("âš ï¸  æ— æ³•è·å–å®Œæ•´è§†é¢‘ä¿¡æ¯ï¼Œä½†å°†ç»§ç»­æ‰§è¡Œçˆ¬å–")
            else:
                print("âŒ æ— æ³•è·å–è§†é¢‘æ ‡é¢˜ï¼Œç¨‹åºç»ˆæ­¢")
                print("ç¨‹åºé€€å‡º")
                return None, None, None, None, None, None, None, video_info
            
    except Exception as e:
        print(f"âŒ è·å–è§†é¢‘ä¿¡æ¯å¤±è´¥: {e}")
        # å°è¯•è·å–æ ‡é¢˜ä½œä¸ºå¤‡é€‰
        try:
            video_title = get_video_title_quick(bv_id)
            if not video_title:
                print("âŒ æ— æ³•è·å–è§†é¢‘æ ‡é¢˜ï¼Œç¨‹åºç»ˆæ­¢")
                print("ç¨‹åºé€€å‡º")
                return None, None, None, None, None, None, None, None
        except:
            print("âŒ æ— æ³•è·å–è§†é¢‘æ ‡é¢˜ï¼Œç¨‹åºç»ˆæ­¢")
            print("ç¨‹åºé€€å‡º")
            return None, None, None, None, None, None, None, None
    
    # é€‰æ‹©çˆ¬å–æ¨¡å¼
    print("\nè¯·é€‰æ‹©çˆ¬å–æ¨¡å¼ï¼š")
    print("ç³»ç»ŸåŸºäºçƒ­åº¦çˆ¬å–å’Œæ—¶é—´çˆ¬å–ä¸¤ç§åŸºç¡€åŠŸèƒ½ï¼Œæä¾›ä»¥ä¸‹ä¸‰ç§è¿è¡Œæ–¹å¼ï¼š")
    print("1. ç»¼åˆæ¨¡å¼ - å…ˆè¿›è¡Œçƒ­åº¦çˆ¬å–ï¼Œå†è¿›è¡Œæ—¶é—´çˆ¬å–ï¼Œè‡ªåŠ¨å»é‡ä¼˜åŒ–ï¼Œå¯çˆ¬å–ä¸Šé™åœ¨15000æ¡å·¦å³")
    print("2. æµ‹è¯•æ¨¡å¼ - å•ç‹¬æµ‹è¯•çƒ­åº¦æˆ–æ—¶é—´çˆ¬å–åŠŸèƒ½ï¼Œå¯è‡ªå®šä¹‰å‚æ•°")
    print("3. è¿­ä»£æ¨¡å¼ - ä¼˜å…ˆç”¨äºæ–°å‘å‡ºçš„ã€ä¸”è¯„è®ºæ•°å¾ˆå¯èƒ½è¶…è¿‡15000æ¡çš„è§†é¢‘ï¼ŒæŒç»­è¿½è¸ªæ–°è¯„è®ºçš„å‡ºç°å¹¶è¿­ä»£è¯„è®ºæ•°æ®ï¼Œä»¥çƒ­åº¦çˆ¬å–-æ—¶é—´çˆ¬å–è¿›è¡Œå¾ªç¯ï¼Œå¯è‡ªå®šä¹‰è¿­ä»£æ—¶é—´æˆ–æœ€é«˜é‡å¤ç‡ï¼ˆå½“æ–°ä¸€è½®è¯„è®ºä¸ä¸Šä¸€è½®è¯„è®ºçš„é‡å¤ç‡é«˜äºè®¾å®šé‡å¤ç‡æ—¶åœæ­¢è¿­ä»£ï¼‰")
    
    iteration_config = None  # åˆå§‹åŒ–è¿­ä»£é…ç½®
    
    while True:
        mode_choice = input("è¯·é€‰æ‹©æ¨¡å¼ (1/2/3ï¼Œç›´æ¥å›è½¦é»˜è®¤é€‰æ‹©1): ").strip()
        
        if not mode_choice or mode_choice == '1':
            # ç»¼åˆæ¨¡å¼
            mode = 'comprehensive'
            mode_name = "ç»¼åˆæ¨¡å¼"
            max_pages = None
            test_sort_mode = None
            print(f"\nâœ… ä½¿ç”¨{mode_name}ï¼šæ™ºèƒ½ç»„åˆçƒ­åº¦å’Œæ—¶é—´çˆ¬å–ï¼ŒåŸºäºrpidå’Œæ—¶é—´æˆ³å»é‡")
            break
        elif mode_choice == '2':
            # æµ‹è¯•æ¨¡å¼
            mode = 'test'
            mode_name = "æµ‹è¯•æ¨¡å¼"
            
            # é€‰æ‹©åŸºç¡€çˆ¬å–åŠŸèƒ½
            print("\nè¯·é€‰æ‹©è¦æµ‹è¯•çš„åŸºç¡€çˆ¬å–åŠŸèƒ½ï¼š")
            print("1. çƒ­åº¦çˆ¬å– - æŒ‰ç‚¹èµæ•°æ’åºè·å–çƒ­é—¨è¯„è®º")
            print("2. æ—¶é—´çˆ¬å– - æŒ‰å‘å¸ƒæ—¶é—´æ’åºè·å–æœ€æ–°è¯„è®º")
            
            while True:
                sort_choice = input("è¯·é€‰æ‹©æ’åºæ–¹å¼ (1/2): ").strip()
                if sort_choice == '1':
                    test_sort_mode = 1  # çƒ­åº¦æ’åº
                    sort_name = "çƒ­åº¦çˆ¬å–"
                    break
                elif sort_choice == '2':
                    test_sort_mode = 0  # æ—¶é—´æ’åº
                    sort_name = "æ—¶é—´çˆ¬å–"
                    break
                else:
                    print("âŒ è¯·è¾“å…¥1æˆ–2")
            
            # è®¾ç½®çˆ¬å–é¡µæ•°
            while True:
                pages_input = input("è¯·è¾“å…¥è¦çˆ¬å–çš„é¡µæ•°ï¼ˆ1-50ï¼Œç›´æ¥å›è½¦é»˜è®¤5é¡µï¼‰: ").strip()
                
                if not pages_input:
                    max_pages = 5
                    break
                
                try:
                    max_pages = int(pages_input)
                    if max_pages < 1:
                        print("âŒ é¡µæ•°ä¸èƒ½å°äº1ï¼Œè¯·é‡æ–°è¾“å…¥")
                        continue
                    elif max_pages > 50:
                        print("âŒ é¡µæ•°ä¸èƒ½è¶…è¿‡50ï¼Œè¯·é‡æ–°è¾“å…¥")
                        continue
                    break
                except ValueError:
                    print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
            
            print(f"\nâœ… ä½¿ç”¨{mode_name}ï¼šå•ç‹¬æµ‹è¯•{sort_name}åŠŸèƒ½ï¼Œçˆ¬å–{max_pages}é¡µ")
            break
        elif mode_choice == '3':
            # è¿­ä»£æ¨¡å¼
            mode = 'iteration'
            mode_name = "è¿­ä»£æ¨¡å¼"
            max_pages = None
            test_sort_mode = None
            
            # é€‰æ‹©è¿­ä»£ç­–ç•¥
            print("\nè¯·é€‰æ‹©è¿­ä»£ç­–ç•¥ï¼š")
            print("1. æ—¶é—´è¿­ä»£ - åœ¨æŒ‡å®šæ—¶é—´å†…äº¤æ›¿æ‰§è¡Œçƒ­åº¦å’Œæ—¶é—´çˆ¬å–")
            print("2. é‡å¤ç‡è¿­ä»£ - åŸºäºé‡å¤ç‡é˜ˆå€¼è‡ªåŠ¨åœæ­¢äº¤æ›¿çˆ¬å–")
            
            while True:
                iteration_type_choice = input("è¯·é€‰æ‹©è¿­ä»£ç±»å‹ (1/2): ").strip()
                if iteration_type_choice == '1':
                    # æ—¶é—´è¿­ä»£
                    iteration_type = 'time'
                    
                    while True:
                        time_input = input("è¯·è¾“å…¥è¿­ä»£æ—¶é—´ï¼ˆå°æ—¶ï¼Œå»ºè®®1-24å°æ—¶ï¼‰: ").strip()
                        try:
                            iteration_hours = float(time_input)
                            if iteration_hours <= 0:
                                print("âŒ è¿­ä»£æ—¶é—´å¿…é¡»å¤§äº0ï¼Œè¯·é‡æ–°è¾“å…¥")
                                continue
                            elif iteration_hours > 72:
                                print("âŒ è¿­ä»£æ—¶é—´è¿‡é•¿ï¼ˆè¶…è¿‡72å°æ—¶ï¼‰ï¼Œè¯·é‡æ–°è¾“å…¥")
                                continue
                            break
                        except ValueError:
                            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
                    
                    iteration_config = {
                        'type': 'time',
                        'hours': iteration_hours
                    }
                    print(f"\nâœ… ä½¿ç”¨{mode_name}ï¼šæ—¶é—´è¿­ä»£ï¼ŒæŒç»­{iteration_hours}å°æ—¶")
                    break
                    
                elif iteration_type_choice == '2':
                    # é‡å¤ç‡è¿­ä»£
                    iteration_type = 'duplicate_rate'
                    
                    # è·å–çƒ­åº¦çˆ¬å–é‡å¤ç‡é˜ˆå€¼
                    while True:
                        hot_rate_input = input("è¯·è¾“å…¥çƒ­åº¦çˆ¬å–é‡å¤ç‡é˜ˆå€¼ï¼ˆ0-100%ï¼Œå»ºè®®85-95ï¼‰: ").strip()
                        try:
                            hot_duplicate_rate = float(hot_rate_input)
                            if hot_duplicate_rate <= 0 or hot_duplicate_rate >= 100:
                                print("âŒ é‡å¤ç‡å¿…é¡»å¤§äº0ä¸”å°äº100ï¼Œè¯·é‡æ–°è¾“å…¥")
                                continue
                            break
                        except ValueError:
                            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
                    
                    # è·å–æ—¶é—´çˆ¬å–é‡å¤ç‡é˜ˆå€¼
                    while True:
                        time_rate_input = input("è¯·è¾“å…¥æ—¶é—´çˆ¬å–é‡å¤ç‡é˜ˆå€¼ï¼ˆ0-100%ï¼Œå»ºè®®60-80ï¼‰: ").strip()
                        try:
                            time_duplicate_rate = float(time_rate_input)
                            if time_duplicate_rate <= 0 or time_duplicate_rate >= 100:
                                print("âŒ é‡å¤ç‡å¿…é¡»å¤§äº0ä¸”å°äº100ï¼Œè¯·é‡æ–°è¾“å…¥")
                                continue
                            break
                        except ValueError:
                            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
                    
                    iteration_config = {
                        'type': 'duplicate_rate',
                        'hot_rate_threshold': hot_duplicate_rate,
                        'time_rate_threshold': time_duplicate_rate
                    }
                    print(f"\nâœ… ä½¿ç”¨{mode_name}ï¼šé‡å¤ç‡è¿­ä»£ï¼Œçƒ­åº¦é˜ˆå€¼{hot_duplicate_rate}%ï¼Œæ—¶é—´é˜ˆå€¼{time_duplicate_rate}%")
                    break
                else:
                    print("âŒ è¯·è¾“å…¥1æˆ–2")
            break
        else:
            print("âŒ è¯·è¾“å…¥1ã€2æˆ–3")
    
    # ä½¿ç”¨å›ºå®šçš„æ¯é¡µè¯„è®ºæ•°é‡
    ps = 20  # å›ºå®šä½¿ç”¨20ä½œä¸ºæ¯é¡µè¯„è®ºæ•°é‡
    print(f"\nâœ… æ¯é¡µè¯„è®ºæ•°é‡å·²è®¾ç½®ä¸º: {ps}")
    
    # è·å–å»¶æ—¶è®¾ç½®
    print("\nè¯·è®¾ç½®è¯·æ±‚å»¶æ—¶ï¼ˆé˜²æ­¢è¢«é™åˆ¶ï¼‰ï¼š")
    while True:
        delay_input = input("è¯·è¾“å…¥æ¯æ¬¡è¯·æ±‚åçš„ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼Œå»ºè®®1-5ç§’ï¼Œç›´æ¥å›è½¦ä½¿ç”¨é»˜è®¤3ç§’ï¼‰: ").strip()
        
        if not delay_input:  # ä½¿ç”¨é»˜è®¤å€¼
            delay_seconds = 3
            break
        
        try:
            delay_seconds = float(delay_input)
            if delay_seconds < 0:
                print("âŒ å»¶æ—¶ä¸èƒ½ä¸ºè´Ÿæ•°ï¼Œè¯·é‡æ–°è¾“å…¥")
                continue
            elif delay_seconds > 30:
                print("âŒ å»¶æ—¶è¿‡é•¿ï¼ˆè¶…è¿‡30ç§’ï¼‰ï¼Œè¯·é‡æ–°è¾“å…¥")
                continue
            break
        except ValueError:
            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
    
    # è½¬æ¢ä¸ºæ¯«ç§’ä¾›å†…éƒ¨ä½¿ç”¨
    delay_ms = int(delay_seconds * 1000)
    
    print(f"\nâœ… é…ç½®å®Œæˆï¼oid: {oid}, æ’åº: {mode_name}, æ¯é¡µæ•°é‡: {ps}, å»¶æ—¶: {delay_seconds}ç§’")
    print(f"å¼€å§‹çˆ¬å–è¯„è®º...\n")
    
    # è·å–è§†é¢‘æ ‡é¢˜ç”¨äºæ–‡ä»¶å‘½å
    video_title = None
    try:
        video_title = get_video_title_quick(bv_id)
    except:
        pass  # å¦‚æœè·å–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å‘½å
    
    return oid, mode, ps, delay_ms, max_pages, test_sort_mode, iteration_config, video_title, video_info

# ç»Ÿä¸€çš„è¯·æ±‚å¤´é…ç½®
DEFAULT_HEADERS = {
    'Cookie': 'enable_web_push=DISABLE; buvid4=EF198646-5A11-454B-4282-A3DD7F2A1D3A91404-023112011-; buvid_fp_plain=undefined; PVID=1; buvid3=5E675ECD-B52A-B31A-EDB5-0117F2E3CA8B16806infoc; b_nut=1744450616; _uuid=CCBC10E18-9101E-5449-10DC6-E3E1217872FE19335infoc; enable_feed_channel=ENABLE; theme_style=light; theme-tip-show=SHOWED; fingerprint=d448c4404bf82931a120eb33d24a21f4; buvid_fp=d448c4404bf82931a120eb33d24a21f4; rpdid=|(k|k)~~~umm0J\'u~R~lkmYl); hit-dyn-v2=1; header_theme_version=OPEN; DedeUserID=3546372995287458; DedeUserID__ckMd5=b77446cbfee6faaf; theme-avatar-tip-show=SHOWED; theme-switch-show=SHOWED; bili_ticket=eyJhbGciOiJIUzI1NiIsImtpZCI6InMwMyIsInR5cCI6IkpXVCJ9.eyJleHAiOjE3NTU5NTkwNzYsImlhdCI6MTc1NTY5OTgxNiwicGx0IjotMX0.SsfNdmWtKhLYyEpQ2UI4-K-1l54Pa3E95nUqJNfORbE; bili_ticket_expires=1755959016; SESSDATA=3ba56d51%2C1771259398%2C5237d%2A82CjD7v3JJ-PIa1hdtFiye-sojBBhyazngSXqcZWjjSAvNhsxtvD7aKuvX9cwoCs7dpc4SVlRqRk53VjZvNElDTUVRanJIbDVfZzZhMnAzdVNjNEVpUnI4TENKdXhNajJDS0d4M0RaaDA0cmtKbVF1bVlXaXgtWGpuak9GeFNDNHVYMEdNajNWV0hBIIEC; bili_jct=f66114d94cef981ad6e98825c3287c87; sid=7jkya1il; CURRENT_QUALITY=0; CURRENT_FNVAL=4048; home_feed_column=5; browser_resolution=1883-864; b_lsid=810A10E189_198D1DE7272; bp_t_offset_3546372995287458=1103932343124492288',
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36',
    'Referer': 'https://www.bilibili.com',
    'Accept': 'application/json, text/plain, */*',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
    'Accept-Encoding': 'gzip, deflate, br',
    'Connection': 'keep-alive',
    'Sec-Fetch-Dest': 'empty',
    'Sec-Fetch-Mode': 'cors',
    'Sec-Fetch-Site': 'same-site',
    'sec-ch-ua': '"Google Chrome";v="131", "Chromium";v="131", "Not_A Brand";v="24"',
    'sec-ch-ua-mobile': '?0',
    'sec-ch-ua-platform': '"Windows"'
}

def get_response(url, data=None, method='GET'):
    """å‘é€HTTPè¯·æ±‚
    
    Args:
        url (str): è¯·æ±‚URL
        data (dict, optional): è¯·æ±‚æ•°æ®
        method (str): è¯·æ±‚æ–¹æ³•ï¼Œé»˜è®¤ä¸ºGET
    
    Returns:
        requests.Response: å“åº”å¯¹è±¡
    """
    try:
        if method.upper() == 'POST':
            response = requests.post(url, data=data, headers=DEFAULT_HEADERS, timeout=10)
        else:
            response = requests.get(url, params=data, headers=DEFAULT_HEADERS, timeout=10)
        
        response.raise_for_status()  # æ£€æŸ¥HTTPé”™è¯¯
        return response
    except requests.exceptions.RequestException as e:
        print(f"è¯·æ±‚å¤±è´¥: {e}")
        return None

def generate_w_rid(params):
    """
    ç”Ÿæˆw_ridç­¾å
    
    Args:
        params (dict): åŒ…å«æ‰€æœ‰å‚æ•°çš„å­—å…¸ï¼Œé™¤äº†w_ridå¤–çš„æ‰€æœ‰å‚æ•°
    
    Returns:
        str: ç”Ÿæˆçš„32ä½w_rid
    """
    # å®šä¹‰å›ºå®šå€¼aï¼ˆå°è¯•æ›´æ–°çš„å€¼ï¼‰
    a = "ea1db124af3c7062474693fa704f4ff8"
    
    # ä»paramsä¸­ç§»é™¤w_ridï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    l = {k: v for k, v in params.items() if k != 'w_rid'}
    
    # å®šä¹‰å‚æ•°é¡ºåºï¼Œç¡®ä¿web_locationåœ¨æ­£ç¡®ä½ç½®
    # å¯¹äºç¬¬ä¸€é¡µï¼šoid, type, sort, ps, seek_rpid(å¦‚æœæœ‰), plat, web_location, wts
    # å¯¹äºå…¶ä»–é¡µï¼šoid, type, sort, ps, pn, plat, web_location, wts
    param_order = ['oid', 'type', 'sort', 'ps']
    
    # æ·»åŠ åˆ†é¡µç›¸å…³å‚æ•°
    if 'pn' in l:
        param_order.append('pn')
    if 'seek_rpid' in l:
        param_order.append('seek_rpid')
    
    # æ·»åŠ å›ºå®šå‚æ•°
    param_order.extend(['plat', 'web_location', 'wts'])
    
    # æŒ‰ç…§æŒ‡å®šé¡ºåºæ„å»ºå‚æ•°å­—ç¬¦ä¸²
    param_list = []
    for key in param_order:
        if key in l:
            param_list.append(f"{key}={l[key]}")
    
    # æ·»åŠ ä»»ä½•å‰©ä½™çš„å‚æ•°ï¼ˆæŒ‰å­—æ¯é¡ºåºï¼‰
    remaining_keys = set(l.keys()) - set(param_order)
    for key in sorted(remaining_keys):
        param_list.append(f"{key}={l[key]}")
    
    v = "&".join(param_list)
    
    # ç»„åˆå­—ç¬¦ä¸²ï¼šv + a
    string = v + a
    
    # æ‰“å°è°ƒè¯•ä¿¡æ¯
    print(f"ç­¾åå­—ç¬¦ä¸²: {string}")
    
    # è¿›è¡ŒMD5åŠ å¯†
    w_rid = hashlib.md5(string.encode('utf-8')).hexdigest()
    
    print(f"ç”Ÿæˆçš„w_rid: {w_rid}")
    
    return w_rid

def get_bilibili_comments(oid, mode=1, ps=20, next_offset='', is_first_page=True, page_num=1, logger=None, output_folder=None):
    """
    è·å–Bç«™è§†é¢‘è¯„è®º
    
    Args:
        oid: è§†é¢‘çš„oidï¼ˆç¨¿ä»¶avidï¼‰
        mode (int): æ’åºæ¨¡å¼ï¼Œæ ¹æ®Bç«™APIæ–‡æ¡£ï¼š0=æŒ‰æ—¶é—´æ’åºï¼Œ1=æŒ‰ç‚¹èµæ•°æ’åºï¼ˆçƒ­åº¦ï¼‰ï¼Œ2=æŒ‰å›å¤æ•°æ’åº
        next_offset (str): åˆ†é¡µåç§»é‡ï¼Œç”¨äºè·å–ä¸‹ä¸€é¡µè¯„è®º
        is_first_page (bool): æ˜¯å¦ä¸ºç¬¬ä¸€é¡µ
        page_num (int): é¡µç 
        logger: ä¸»æ—¥å¿—è®°å½•å™¨
        output_folder: è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„ï¼Œç”¨äºåˆ›å»ºé¡µé¢æ—¥å¿—
    
    Returns:
        dict: è¯„è®ºæ•°æ®
    """
    # åˆ›å»ºé¡µé¢ä¸“ç”¨æ—¥å¿—è®°å½•å™¨
    page_logger = None
    page_log_file = None
    if output_folder:
        print(f"ğŸ” è°ƒè¯•ï¼šæ­£åœ¨ä¸ºç¬¬ {page_num} é¡µåˆ›å»ºé¡µé¢æ—¥å¿—è®°å½•å™¨...")
        page_logger, page_log_file = create_page_logger(output_folder, oid, page_num)
        print(f"ğŸ” è°ƒè¯•ï¼šé¡µé¢æ—¥å¿—æ–‡ä»¶å·²åˆ›å»º: {os.path.basename(page_log_file)}")
    else:
        print(f"ğŸ” è°ƒè¯•ï¼šoutput_folder ä¸ºç©ºï¼Œè·³è¿‡é¡µé¢æ—¥å¿—è®°å½•å™¨åˆ›å»º")
    
    # å°è¯•ä½¿ç”¨ä¸åŒçš„è¯„è®ºAPIæ¥å£
    # url = "https://api.bilibili.com/x/v2/reply/wbi/main"  # WBIæ¥å£
    url = "https://api.bilibili.com/x/v2/reply"  # åŸºç¡€æ¥å£
    
    if logger:
        logger.info(f"å¼€å§‹è¯·æ±‚ç¬¬ {page_num} é¡µè¯„è®ºï¼ŒAPI: {url}")
    if page_logger:
        page_logger.info(f"å¼€å§‹è¯·æ±‚ç¬¬ {page_num} é¡µè¯„è®º")
        page_logger.info(f"APIæ¥å£: {url}")
    
    # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è¯·æ±‚å¤´é…ç½®
    config = load_config()
    request_headers = get_request_headers(config)
    
    # å¦‚æœæ²¡æœ‰æä¾›oidï¼Œä½¿ç”¨é»˜è®¤å€¼
    if oid is None:
        oid = "115066334743950"
    
    # æ„å»ºåˆ†é¡µå­—ç¬¦ä¸²
    if next_offset:
        pagination_str = f'{{"offset":"{next_offset}"}}'
    else:
        pagination_str = '{"offset":""}'
    
    # ç”Ÿæˆå½“å‰æ—¶é—´æˆ³
    wts = str(int(time.time()))
    
    # æ„å»ºåŸºç¡€å‚æ•°ï¼ˆç”¨äºw_ridç­¾åï¼‰
    sign_params = {
        'oid': str(oid),
        'type': '1',
        'sort': str(mode),
        'ps': str(ps),  # ç”¨æˆ·è®¾å®šçš„æ¯é¡µè¯„è®ºæ•°é‡
        'plat': '1',  # å¹³å°å‚æ•°
        'web_location': '1315875',  # æ–°å¢çš„web_locationå‚æ•°
        'wts': wts  # æ—¶é—´æˆ³
    }
    
    # æ·»åŠ åˆ†é¡µå‚æ•°
    if not is_first_page:
        sign_params['pn'] = str(page_num)
    else:
        # ç¬¬ä¸€é¡µå¯èƒ½éœ€è¦seek_rpidå‚æ•°
        if next_offset:
            sign_params['seek_rpid'] = next_offset
    
    # ç”Ÿæˆw_ridç­¾å
    w_rid = generate_w_rid(sign_params)
    
    # æ„å»ºæœ€ç»ˆè¯·æ±‚å‚æ•°ï¼ˆæŒ‰ç…§æŒ‡å®šé¡ºåºï¼‰
    params = {
        'oid': str(oid),
        'type': '1',
        'sort': str(mode),
        'ps': str(ps)
    }
    
    # æ·»åŠ åˆ†é¡µå‚æ•°
    if not is_first_page:
        params['pn'] = str(page_num)
    else:
        if next_offset:
            params['seek_rpid'] = next_offset
    
    # æŒ‰ç…§æŒ‡å®šä½ç½®æ·»åŠ å‚æ•°
    params['plat'] = '1'
    params['web_location'] = '1315875'
    params['w_rid'] = w_rid
    params['wts'] = wts
    
    # è®°å½•è¯¦ç»†çš„è¯·æ±‚ä¿¡æ¯
    if logger:
        logger.info(f"=== ç¬¬ {page_num} é¡µè¯·æ±‚å¼€å§‹ ===")
        logger.info(f"è¯·æ±‚URL: {url}")
        logger.info(f"è¯·æ±‚å‚æ•°: {json.dumps(params, ensure_ascii=False, indent=2)}")
        logger.info(f"è¯·æ±‚å¤´: {json.dumps(request_headers, ensure_ascii=False, indent=2)}")
    
    if page_logger:
        page_logger.info(f"=== ç¬¬ {page_num} é¡µè¯·æ±‚è¯¦æƒ… ===")
        page_logger.info(f"è¯·æ±‚æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        page_logger.info(f"è¯·æ±‚URL: {url}")
        page_logger.info(f"è¯·æ±‚å‚æ•°: {json.dumps(params, ensure_ascii=False, indent=2)}")
        page_logger.info(f"è¯·æ±‚å¤´: {json.dumps(request_headers, ensure_ascii=False, indent=2)}")
    
    # ä½¿ç”¨è‡ªå®šä¹‰è¯·æ±‚å¤´å‘é€è¯·æ±‚
    try:
        request_start_time = time.time()
        if page_logger:
            page_logger.info(f"å¼€å§‹å‘é€GETè¯·æ±‚...")
        
        response = requests.get(url, params=params, headers=request_headers, timeout=10)
        request_end_time = time.time()
        request_duration = round((request_end_time - request_start_time) * 1000, 2)  # è½¬æ¢ä¸ºæ¯«ç§’
        
        response.raise_for_status()
        
        if response.status_code == 200:
            result = response.json()
            
            # è®°å½•è¯¦ç»†çš„å“åº”ä¿¡æ¯
            if logger:
                logger.info(f"=== ç¬¬ {page_num} é¡µå“åº”æˆåŠŸ ===")
                logger.info(f"å“åº”çŠ¶æ€ç : {response.status_code}")
                logger.info(f"è¯·æ±‚è€—æ—¶: {request_duration}ms")
                logger.info(f"å“åº”å¤´: {json.dumps(dict(response.headers), ensure_ascii=False, indent=2)}")
                logger.info(f"å“åº”æ•°æ®å¤§å°: {len(response.text)} å­—ç¬¦")
                if 'data' in result and 'replies' in result['data']:
                    replies_count = len(result['data']['replies']) if result['data']['replies'] else 0
                    logger.info(f"æœ¬é¡µè¯„è®ºæ•°é‡: {replies_count} æ¡")
                # è®°å½•å®Œæ•´å“åº”æ•°æ®åˆ°ä¸»æ—¥å¿—
                logger.info(f"å®Œæ•´å“åº”æ•°æ®: {json.dumps(result, ensure_ascii=False, indent=2)}")
                logger.info(f"åŸå§‹å“åº”æ–‡æœ¬: {response.text}")
            
            if page_logger:
                page_logger.info(f"=== ç¬¬ {page_num} é¡µå“åº”è¯¦æƒ… ===")
                page_logger.info(f"å“åº”æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
                page_logger.info(f"å“åº”çŠ¶æ€ç : {response.status_code}")
                page_logger.info(f"è¯·æ±‚è€—æ—¶: {request_duration}ms")
                page_logger.info(f"å“åº”å¤´: {json.dumps(dict(response.headers), ensure_ascii=False, indent=2)}")
                page_logger.info(f"å“åº”æ•°æ®å¤§å°: {len(response.text)} å­—ç¬¦")
                if 'data' in result and 'replies' in result['data']:
                    replies_count = len(result['data']['replies']) if result['data']['replies'] else 0
                    page_logger.info(f"æœ¬é¡µè¯„è®ºæ•°é‡: {replies_count} æ¡")
                page_logger.info(f"å®Œæ•´å“åº”æ•°æ®: {json.dumps(result, ensure_ascii=False, indent=2)}")
                page_logger.info(f"=== ç¬¬ {page_num} é¡µæ—¥å¿—è®°å½•å®Œæˆ ===")
            
            return result
        else:
            # æ£€æŸ¥æ˜¯å¦ä¸º412é”™è¯¯ï¼ˆCookieè¢«å°ç¦ï¼‰
            if response.status_code == 412:
                error_msg = f"âŒ æ£€æµ‹åˆ°412é”™è¯¯ - Cookieå¯èƒ½è¢«æš‚æ—¶å°ç¦"
                if logger:
                    logger.error(f"=== ç¬¬ {page_num} é¡µé‡åˆ°412é”™è¯¯ - è§¦å‘ä¸­æ–­æœºåˆ¶ ===")
                    logger.error(f"é”™è¯¯çŠ¶æ€ç : {response.status_code}")
                    logger.error(f"è¯·æ±‚è€—æ—¶: {request_duration}ms")
                    logger.error(f"å“åº”å¤´: {json.dumps(dict(response.headers), ensure_ascii=False, indent=2)}")
                    logger.error(f"å®Œæ•´å“åº”å†…å®¹: {response.text}")
                    logger.error("å³å°†è§¦å‘ç¨‹åºä¸­æ–­å’Œæ–‡ä»¶æ¸…ç†")
                if page_logger:
                    page_logger.error(f"=== ç¬¬ {page_num} é¡µé‡åˆ°412é”™è¯¯è¯¦æƒ… ===")
                    page_logger.error(f"å¤±è´¥æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
                    page_logger.error(f"é”™è¯¯çŠ¶æ€ç : {response.status_code}")
                    page_logger.error(f"è¯·æ±‚è€—æ—¶: {request_duration}ms")
                    page_logger.error(f"å“åº”å¤´: {json.dumps(dict(response.headers), ensure_ascii=False, indent=2)}")
                    page_logger.error(f"å®Œæ•´å“åº”å†…å®¹: {response.text}")
                    page_logger.error(f"=== ç¬¬ {page_num} é¡µ412é”™è¯¯æ—¥å¿—è®°å½•å®Œæˆ ===")
                print(f"\n{error_msg}")
                
                # æŠ›å‡ºç‰¹æ®Šå¼‚å¸¸ä»¥è§¦å‘ä¸­æ–­æœºåˆ¶
                raise CookieBannedException("Cookieè¢«æš‚æ—¶å°ç¦ï¼Œè§¦å‘ç¨‹åºä¸­æ–­")
            else:
                error_msg = f"è·å–è¯„è®ºå¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}"
                if logger:
                    logger.error(f"=== ç¬¬ {page_num} é¡µè¯·æ±‚å¤±è´¥ ===")
                    logger.error(f"é”™è¯¯çŠ¶æ€ç : {response.status_code}")
                    logger.error(f"è¯·æ±‚è€—æ—¶: {request_duration}ms")
                    logger.error(f"å“åº”å¤´: {json.dumps(dict(response.headers), ensure_ascii=False, indent=2)}")
                    logger.error(f"å®Œæ•´å“åº”å†…å®¹: {response.text}")
                if page_logger:
                    page_logger.error(f"=== ç¬¬ {page_num} é¡µè¯·æ±‚å¤±è´¥è¯¦æƒ… ===")
                    page_logger.error(f"å¤±è´¥æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
                    page_logger.error(f"é”™è¯¯çŠ¶æ€ç : {response.status_code}")
                    page_logger.error(f"è¯·æ±‚è€—æ—¶: {request_duration}ms")
                    page_logger.error(f"å“åº”å¤´: {json.dumps(dict(response.headers), ensure_ascii=False, indent=2)}")
                    page_logger.error(f"å®Œæ•´å“åº”å†…å®¹: {response.text}")
                    page_logger.error(f"=== ç¬¬ {page_num} é¡µé”™è¯¯æ—¥å¿—è®°å½•å®Œæˆ ===")
                print(f"âŒ {error_msg}")
                return None
    except requests.exceptions.RequestException as e:
        request_end_time = time.time()
        request_duration = round((request_end_time - request_start_time) * 1000, 2)
        error_msg = f"è¯·æ±‚å¼‚å¸¸: {e}"
        if logger:
            logger.error(f"=== ç¬¬ {page_num} é¡µè¯·æ±‚å¼‚å¸¸ ===")
            logger.error(f"å¼‚å¸¸ç±»å‹: {type(e).__name__}")
            logger.error(f"å¼‚å¸¸ä¿¡æ¯: {str(e)}")
            logger.error(f"è¯·æ±‚è€—æ—¶: {request_duration}ms")
            # å¦‚æœå¼‚å¸¸åŒ…å«å“åº”ä¿¡æ¯ï¼Œä¹Ÿè®°å½•ä¸‹æ¥
            if hasattr(e, 'response') and e.response is not None:
                logger.error(f"å¼‚å¸¸å“åº”çŠ¶æ€ç : {e.response.status_code}")
                logger.error(f"å¼‚å¸¸å“åº”å¤´: {json.dumps(dict(e.response.headers), ensure_ascii=False, indent=2)}")
                logger.error(f"å¼‚å¸¸å“åº”å†…å®¹: {e.response.text}")
        if page_logger:
            page_logger.error(f"=== ç¬¬ {page_num} é¡µè¯·æ±‚å¼‚å¸¸è¯¦æƒ… ===")
            page_logger.error(f"å¼‚å¸¸æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
            page_logger.error(f"å¼‚å¸¸ç±»å‹: {type(e).__name__}")
            page_logger.error(f"å¼‚å¸¸ä¿¡æ¯: {str(e)}")
            page_logger.error(f"è¯·æ±‚è€—æ—¶: {request_duration}ms")
            # å¦‚æœå¼‚å¸¸åŒ…å«å“åº”ä¿¡æ¯ï¼Œä¹Ÿè®°å½•ä¸‹æ¥
            if hasattr(e, 'response') and e.response is not None:
                page_logger.error(f"å¼‚å¸¸å“åº”çŠ¶æ€ç : {e.response.status_code}")
                page_logger.error(f"å¼‚å¸¸å“åº”å¤´: {json.dumps(dict(e.response.headers), ensure_ascii=False, indent=2)}")
                page_logger.error(f"å¼‚å¸¸å“åº”å†…å®¹: {e.response.text}")
            page_logger.error(f"=== ç¬¬ {page_num} é¡µå¼‚å¸¸æ—¥å¿—è®°å½•å®Œæˆ ===")
        print(f"âŒ {error_msg}")
        return None

def process_comments_page(replies, start_index=1, logger=None, oid=None):
    """
    å¤„ç†å•é¡µè¯„è®ºæ•°æ®ï¼ŒåŒ…æ‹¬ä¸»æ¥¼è¯„è®ºå’Œæ¥¼ä¸­æ¥¼å›å¤
    
    Args:
        replies: è¯„è®ºåˆ—è¡¨
        start_index: èµ·å§‹åºå·
        logger: æ—¥å¿—è®°å½•å™¨
        oid: è§†é¢‘oidï¼Œç”¨äºè·å–æ›´å¤šæ¥¼ä¸­æ¥¼è¯„è®º
    
    Returns:
        list: å¤„ç†åçš„è¯„è®ºæ•°æ®åˆ—è¡¨
    """
    csv_data = []
    
    for i, reply in enumerate(replies, start_index):
        # å¤„ç†ä¸»æ¥¼è¯„è®º
        main_comment = process_single_comment(reply, main_floor_num=i, sub_floor_num=0, logger=logger)
        csv_data.append(main_comment)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ¥¼ä¸­æ¥¼å›å¤éœ€è¦å¤„ç†
        reply_control = reply.get('reply_control', {})
        sub_reply_entry_text = reply_control.get('sub_reply_entry_text', '')
        sub_replies = reply.get('replies', [])
        sub_reply_count = reply.get('rcount', 0)  # æ€»å›å¤æ•°
        
        # ä¼˜å…ˆæ£€æŸ¥sub_reply_entry_textå­—æ®µï¼ˆæŒ‰ç…§å®ç°æŒ‡å—ï¼‰
        if sub_reply_entry_text and 'æ¡å›å¤' in sub_reply_entry_text:
            # ä»"å…±37æ¡å›å¤"ä¸­æå–æ•°å­—
            import re
            match = re.search(r'å…±(\d+)æ¡å›å¤', sub_reply_entry_text)
            if match:
                total_replies = int(match.group(1))
                if logger:
                    logger.info(f"ä¸»æ¥¼ {i} æ£€æµ‹åˆ°sub_reply_entry_text: {sub_reply_entry_text}ï¼Œæ€»å…±æœ‰ {total_replies} æ¡å›å¤")
                
                # èˆå¼ƒåŸæœ¬é‡‡é›†çš„æ¥¼ä¸­æ¥¼è¯„è®ºï¼Œé‡æ–°è·å–å®Œæ•´çš„æ¥¼ä¸­æ¥¼è¯„è®º
                if logger:
                    logger.info(f"èˆå¼ƒåŸæœ¬çš„ {len(sub_replies)} æ¡æ¥¼ä¸­æ¥¼è¯„è®ºï¼Œé‡æ–°è·å–å®Œæ•´çš„ {total_replies} æ¡æ¥¼ä¸­æ¥¼è¯„è®º")
                
                # è·å–æ‰€æœ‰æ¥¼ä¸­æ¥¼è¯„è®º
                all_sub_replies = get_all_sub_replies(reply.get('rpid'), oid, total_replies, logger)
                
                if all_sub_replies:
                    for j, sub_reply in enumerate(all_sub_replies, 1):
                        sub_comment = process_single_comment(sub_reply, main_floor_num=None, sub_floor_num=j, logger=logger, is_sub_reply=True)
                        csv_data.append(sub_comment)
                        
                        if logger:
                            logger.info(f"å¤„ç†å®Œæ•´æ¥¼ä¸­æ¥¼å›å¤ {i}.{j}: ç”¨æˆ·={sub_comment.get('ç”¨æˆ·åç§°', sub_comment.get('ç”¨æˆ·å', ''))}, ç‚¹èµ={sub_comment['ç‚¹èµæ•°']}")
        
        elif sub_replies:
            # æ²¡æœ‰sub_reply_entry_textä½†æœ‰æ¥¼ä¸­æ¥¼å›å¤ï¼ŒæŒ‰åŸé€»è¾‘å¤„ç†
            for j, sub_reply in enumerate(sub_replies, 1):
                sub_comment = process_single_comment(sub_reply, main_floor_num=None, sub_floor_num=j, logger=logger, is_sub_reply=True)
                csv_data.append(sub_comment)
                
                if logger:
                    logger.info(f"å¤„ç†æ¥¼ä¸­æ¥¼å›å¤ {i}.{j}: ç”¨æˆ·={sub_comment.get('ç”¨æˆ·åç§°', sub_comment.get('ç”¨æˆ·å', ''))}, ç‚¹èµ={sub_comment['ç‚¹èµæ•°']}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ›´å¤šæ¥¼ä¸­æ¥¼è¯„è®ºéœ€è¦è·å–
            if sub_reply_count > len(sub_replies):
                if logger:
                    logger.info(f"ä¸»æ¥¼ {i} æœ‰ {sub_reply_count} æ¡å›å¤ï¼Œå½“å‰åªè·å–äº† {len(sub_replies)} æ¡ï¼Œå°è¯•è·å–æ›´å¤šæ¥¼ä¸­æ¥¼è¯„è®º")
                
                # è·å–æ›´å¤šæ¥¼ä¸­æ¥¼è¯„è®º
                additional_sub_replies = get_all_sub_replies(reply.get('rpid'), oid, sub_reply_count, logger, skip_count=len(sub_replies))
                
                if additional_sub_replies:
                    for k, additional_sub_reply in enumerate(additional_sub_replies, len(sub_replies) + 1):
                        sub_comment = process_single_comment(additional_sub_reply, main_floor_num=None, sub_floor_num=k, logger=logger, is_sub_reply=True)
                        csv_data.append(sub_comment)
                        
                        if logger:
                            logger.info(f"å¤„ç†é¢å¤–æ¥¼ä¸­æ¥¼å›å¤ {i}.{k}: ç”¨æˆ·={sub_comment.get('ç”¨æˆ·åç§°', sub_comment.get('ç”¨æˆ·å', ''))}, ç‚¹èµ={sub_comment['ç‚¹èµæ•°']}")
        
        elif sub_reply_count > 0:
            # ä¸»æ¥¼æœ‰å›å¤ä½†å½“å‰é¡µé¢æ²¡æœ‰æ˜¾ç¤ºï¼Œå°è¯•è·å–
            if logger:
                logger.info(f"ä¸»æ¥¼ {i} æœ‰ {sub_reply_count} æ¡å›å¤ä½†æœªåœ¨å½“å‰é¡µé¢æ˜¾ç¤ºï¼Œå°è¯•è·å–æ¥¼ä¸­æ¥¼è¯„è®º")
            
            all_sub_replies = get_all_sub_replies(reply.get('rpid'), oid, sub_reply_count, logger)
            
            if all_sub_replies:
                for j, sub_reply in enumerate(all_sub_replies, 1):
                    sub_comment = process_single_comment(sub_reply, main_floor_num=None, sub_floor_num=j, logger=logger, is_sub_reply=True)
                    csv_data.append(sub_comment)
                    
                    if logger:
                        logger.info(f"å¤„ç†è·å–çš„æ¥¼ä¸­æ¥¼å›å¤ {i}.{j}: ç”¨æˆ·={sub_comment.get('ç”¨æˆ·åç§°', sub_comment.get('ç”¨æˆ·å', ''))}, ç‚¹èµ={sub_comment['ç‚¹èµæ•°']}")
    
    return csv_data


def get_all_sub_replies(root_rpid, oid, total_replies, logger=None, skip_count=0):
    """
    è·å–æŒ‡å®šä¸»æ¥¼è¯„è®ºçš„æ‰€æœ‰æ¥¼ä¸­æ¥¼å›å¤ï¼ˆæ”¯æŒå¤šé¡µè¿­ä»£ï¼‰
    
    Args:
        root_rpid: ä¸»æ¥¼è¯„è®ºçš„rpid
        oid: è§†é¢‘oid
        total_replies: æ€»å›å¤æ•°é‡
        logger: æ—¥å¿—è®°å½•å™¨
        skip_count: è·³è¿‡çš„å›å¤æ•°é‡ï¼ˆå·²è·å–çš„å›å¤æ•°ï¼‰
    
    Returns:
        list: æ‰€æœ‰æ¥¼ä¸­æ¥¼å›å¤åˆ—è¡¨
    """
    if not root_rpid or not oid or total_replies <= 0:
        return []
    
    all_replies = []
    page_size = 20  # Bç«™æ¯é¡µæœ€å¤š20æ¡å›å¤
    
    # è®¡ç®—éœ€è¦è·å–çš„é¡µæ•°
    start_page = (skip_count // page_size) + 1
    total_pages = ((total_replies - skip_count - 1) // page_size) + 1
    
    if logger:
        logger.info(f"å¼€å§‹è·å–æ¥¼ä¸­æ¥¼å›å¤: root_rpid={root_rpid}, æ€»å›å¤æ•°={total_replies}, è·³è¿‡={skip_count}, éœ€è¦è·å–é¡µæ•°={total_pages}")
    
    # Bç«™æ¥¼ä¸­æ¥¼å›å¤API
    url = "https://api.bilibili.com/x/v2/reply/reply"
    
    # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è¯·æ±‚å¤´é…ç½®
    config = load_config()
    request_headers = get_request_headers(config)
    
    for page in range(start_page, start_page + total_pages):
        # æ„å»ºè¯·æ±‚å‚æ•°
        params = {
            'oid': str(oid),
            'type': '1',
            'root': str(root_rpid),
            'ps': str(page_size),
            'pn': str(page)
        }
        
        try:
            if logger:
                logger.info(f"è¯·æ±‚æ¥¼ä¸­æ¥¼å›å¤ç¬¬ {page} é¡µ: root_rpid={root_rpid}")
            
            response = requests.get(url, params=params, headers=request_headers, timeout=10)
            response.raise_for_status()
            
            data = response.json()
            
            if data.get('code') == 0:
                replies_data = data.get('data', {})
                page_replies = replies_data.get('replies', [])
                
                if logger:
                    logger.info(f"ç¬¬ {page} é¡µæˆåŠŸè·å– {len(page_replies)} æ¡æ¥¼ä¸­æ¥¼å›å¤")
                
                # å¤„ç†ç¬¬ä¸€é¡µçš„è·³è¿‡é€»è¾‘
                if page == start_page and skip_count > 0:
                    skip_in_page = skip_count % page_size
                    page_replies = page_replies[skip_in_page:]
                    if logger:
                        logger.info(f"ç¬¬ {page} é¡µè·³è¿‡å‰ {skip_in_page} æ¡å›å¤ï¼Œå®é™…è·å– {len(page_replies)} æ¡")
                
                all_replies.extend(page_replies)
                
                # å¦‚æœè¿™ä¸€é¡µçš„å›å¤æ•°å°‘äºé¡µé¢å¤§å°ï¼Œè¯´æ˜å·²ç»æ˜¯æœ€åä¸€é¡µ
                if len(page_replies) < page_size:
                    if logger:
                        logger.info(f"ç¬¬ {page} é¡µå›å¤æ•° {len(page_replies)} < {page_size}ï¼Œå·²åˆ°è¾¾æœ€åä¸€é¡µ")
                    break
                    
            else:
                error_msg = f"è·å–æ¥¼ä¸­æ¥¼å›å¤ç¬¬ {page} é¡µå¤±è´¥: {data.get('message', 'æœªçŸ¥é”™è¯¯')}"
                if logger:
                    logger.warning(error_msg)
                break
                
        except requests.exceptions.RequestException as e:
            error_msg = f"è¯·æ±‚æ¥¼ä¸­æ¥¼å›å¤ç¬¬ {page} é¡µå¼‚å¸¸: {e}"
            if logger:
                logger.error(error_msg)
            break
        except Exception as e:
            error_msg = f"å¤„ç†æ¥¼ä¸­æ¥¼å›å¤ç¬¬ {page} é¡µæ•°æ®å¼‚å¸¸: {e}"
            if logger:
                logger.error(error_msg)
            break
        
        # æ·»åŠ è¯·æ±‚é—´éš”ï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
        import time
        time.sleep(0.5)
    
    if logger:
        logger.info(f"æ¥¼ä¸­æ¥¼å›å¤è·å–å®Œæˆ: æ€»å…±è·å– {len(all_replies)} æ¡å›å¤")
    
    return all_replies


def get_additional_sub_replies(root_rpid, oid, skip_count=0, logger=None):
    """
    è·å–æŒ‡å®šä¸»æ¥¼è¯„è®ºçš„æ›´å¤šæ¥¼ä¸­æ¥¼å›å¤ï¼ˆå…¼å®¹æ€§å‡½æ•°ï¼Œå·²åºŸå¼ƒï¼‰
    
    Args:
        root_rpid: ä¸»æ¥¼è¯„è®ºçš„rpid
        oid: è§†é¢‘oid
        skip_count: è·³è¿‡çš„å›å¤æ•°é‡ï¼ˆå·²è·å–çš„å›å¤æ•°ï¼‰
        logger: æ—¥å¿—è®°å½•å™¨
    
    Returns:
        list: é¢å¤–çš„æ¥¼ä¸­æ¥¼å›å¤åˆ—è¡¨
    """
    if logger:
        logger.warning("get_additional_sub_replieså‡½æ•°å·²åºŸå¼ƒï¼Œè¯·ä½¿ç”¨get_all_sub_replieså‡½æ•°")
    
    # è°ƒç”¨æ–°å‡½æ•°ï¼Œä¼°ç®—æ€»å›å¤æ•°ä¸ºskip_count + 20
    return get_all_sub_replies(root_rpid, oid, skip_count + 20, logger, skip_count)


def process_single_comment(reply, main_floor_num=None, sub_floor_num=None, logger=None, is_sub_reply=False):
    """
    å¤„ç†å•æ¡è¯„è®ºæ•°æ®
    
    Args:
        reply: å•æ¡è¯„è®ºæ•°æ®
        main_floor_num: ä¸»æ¥¼åºå·ï¼ˆä¸»æ¥¼è¯„è®ºæ—¶ä¸ºæ•°å­—ï¼Œæ¥¼ä¸­æ¥¼å›å¤æ—¶ä¸ºNoneï¼‰
        sub_floor_num: æ¥¼ä¸­æ¥¼åºå·ï¼ˆä¸»æ¥¼è¯„è®ºæ—¶ä¸º0ï¼Œæ¥¼ä¸­æ¥¼å›å¤æ—¶ä¸ºæ•°å­—ï¼‰
        logger: æ—¥å¿—è®°å½•å™¨
        is_sub_reply: æ˜¯å¦ä¸ºæ¥¼ä¸­æ¥¼å›å¤
    
    Returns:
        dict: å¤„ç†åçš„è¯„è®ºæ•°æ®
    """
    # å¤„ç†IPåœ°åŒºä¿¡æ¯
    location = reply.get('reply_control', {}).get('location', 'æœªçŸ¥åœ°åŒº')
    if location.startswith('IPå±åœ°ï¼š'):
        location = location.replace('IPå±åœ°ï¼š', '')
    
    # å¤„ç†æ—¶é—´æˆ³è½¬æ¢
    ctime = reply.get('ctime', 0)
    if ctime:
        formatted_time = datetime.fromtimestamp(ctime).strftime('%Y-%m-%d %H:%M:%S')
    else:
        formatted_time = 'æœªçŸ¥æ—¶é—´'
    
    # å¤„ç†å›å¤å¯¹è±¡ä¿¡æ¯ï¼ˆæ¥¼ä¸­æ¥¼ç‰¹æœ‰ï¼‰
    reply_to = ''
    if is_sub_reply:
        parent_reply_member = reply.get('parent_reply_member', {})
        if parent_reply_member:
            reply_to = f"@{parent_reply_member.get('name', 'æœªçŸ¥ç”¨æˆ·')}"
    
    # è·å–å½“å‰çˆ¬å–æ—¶é—´
    current_crawl_time = datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥_%Hæ—¶%Måˆ†%Sç§’')
    
    # åˆ›å»ºè¯„è®ºå­—å…¸
    comment_dict = {
        'ä¸»æ¥¼åºå·': main_floor_num if main_floor_num is not None else '',
        'æ¥¼ä¸­æ¥¼åºå·': sub_floor_num if sub_floor_num != 0 else (0 if not is_sub_reply else sub_floor_num),
        'ç”¨æˆ·åç§°': reply.get('member', {}).get('uname', 'æœªçŸ¥ç”¨æˆ·'),
        'è¯„è®ºå†…å®¹': reply.get('content', {}).get('message', 'æ— å†…å®¹'),
        'å›å¤å¯¹è±¡': reply_to,
        'ç‚¹èµæ•°': reply.get('like', 0),
        'å›å¤æ•°': reply.get('rcount', 0),
        'å‘å¸ƒæ—¶é—´': formatted_time,
        'æ—¶é—´æˆ³': ctime,  # æ·»åŠ æ—¶é—´æˆ³å­—æ®µï¼Œä¿ç•™10ä½æ—¶é—´æˆ³æ ¼å¼
        'ç”¨æˆ·ç­‰çº§': reply.get('member', {}).get('level_info', {}).get('current_level', 0),
        'IPåœ°åŒº': location,
        'æ€§åˆ«': reply.get('member', {}).get('sex', 'æœªçŸ¥æ€§åˆ«'),
        'è¯„è®ºç±»å‹': 'æ¥¼ä¸­æ¥¼å›å¤' if is_sub_reply else 'ä¸»æ¥¼è¯„è®º',
        'rpid': reply.get('rpid_str', reply.get('rpid', '')),  # æ·»åŠ rpidå­—æ®µï¼Œä¼˜å…ˆä½¿ç”¨rpid_strï¼Œfallbackåˆ°rpid
        'parent': reply.get('parent_str', reply.get('parent', 0)),  # æ·»åŠ parentå­—æ®µï¼Œä¼˜å…ˆä½¿ç”¨parent_strï¼Œfallbackåˆ°parent
        'çˆ¬å–æ—¶é—´': current_crawl_time  # æ·»åŠ çˆ¬å–æ—¶é—´å­—æ®µ
    }
    
    # è®°å½•åˆ°æ—¥å¿—æ–‡ä»¶
    if logger:
        comment_type = 'æ¥¼ä¸­æ¥¼å›å¤' if is_sub_reply else 'ä¸»æ¥¼è¯„è®º'
        index_display = f"{main_floor_num}.{sub_floor_num}" if is_sub_reply else str(main_floor_num)
        logger.info(f"å¤„ç†{comment_type} {index_display}: ç”¨æˆ·={comment_dict['ç”¨æˆ·åç§°']}, ç‚¹èµ={comment_dict['ç‚¹èµæ•°']}, å†…å®¹é•¿åº¦={len(comment_dict['è¯„è®ºå†…å®¹'])}å­—ç¬¦")
        logger.debug(f"è¯„è®ºè¯¦æƒ… {index_display}: {comment_dict}")
    
    return comment_dict

def process_reply_relationships(comments, logger=None):
    """
    å¤„ç†è¯„è®ºå›å¤å…³ç³»ï¼Œä¸ºparentä¸ä¸º0çš„æ¥¼ä¸­æ¥¼è¯„è®ºåˆ›å»ºæ–°çš„å›å¤å¯¹è±¡æ 
    
    Args:
        comments (list): è¯„è®ºæ•°æ®åˆ—è¡¨
        logger: æ—¥å¿—è®°å½•å™¨
    
    Returns:
        list: å¤„ç†åçš„è¯„è®ºæ•°æ®åˆ—è¡¨
    """
    if not comments:
        return comments
    
    # åˆ›å»ºrpidåˆ°è¯„è®ºçš„æ˜ å°„
    rpid_to_comment = {}
    for comment in comments:
        rpid = comment.get('rpid', '')
        if rpid:
            rpid_to_comment[str(rpid)] = comment
    
    processed_comments = []
    
    for comment in comments:
        processed_comment = {
            'ä¸»æ¥¼åºå·': comment['ä¸»æ¥¼åºå·'],
            'æ¥¼ä¸­æ¥¼åºå·': comment['æ¥¼ä¸­æ¥¼åºå·'],
            'ç”¨æˆ·åç§°': comment.get('ç”¨æˆ·åç§°', comment.get('ç”¨æˆ·å', '')),
            'è¯„è®ºå†…å®¹': comment['è¯„è®ºå†…å®¹'],
            'ç‚¹èµæ•°': comment['ç‚¹èµæ•°'],
            'å›å¤æ•°': comment.get('å›å¤æ•°', 0),
            'å‘å¸ƒæ—¶é—´': comment.get('å‘å¸ƒæ—¶é—´', ''),
            'ç”¨æˆ·ç­‰çº§': comment.get('ç”¨æˆ·ç­‰çº§', ''),
            'IPåœ°åŒº': comment.get('IPåœ°åŒº', ''),
            'æ€§åˆ«': comment.get('æ€§åˆ«', ''),
            'è¯„è®ºç±»å‹': comment.get('è¯„è®ºç±»å‹', '')
        }
        
        parent = comment.get('parent', 0)
        
        # å¦‚æœparentä¸ä¸º0ï¼ˆæ¥¼ä¸­æ¥¼è¯„è®ºï¼‰ï¼Œå¤„ç†å›å¤å…³ç³»
        if parent != 0 and comment['æ¥¼ä¸­æ¥¼åºå·'] != 0:
            parent_comment = rpid_to_comment.get(str(parent))
            if parent_comment:
                target_username = parent_comment.get('ç”¨æˆ·åç§°', parent_comment.get('ç”¨æˆ·å', ''))
                current_username = comment.get('ç”¨æˆ·åç§°', comment.get('ç”¨æˆ·å', ''))
                
                # åˆ›å»ºå›å¤å¯¹è±¡æ ï¼Œä½¿ç”¨è¢«å›å¤è¯„è®ºçš„æ¥¼ä¸­æ¥¼åºå·
                target_floor_index = parent_comment['æ¥¼ä¸­æ¥¼åºå·']
                processed_comment['å›å¤è¯„è®ºå¯¹è±¡'] = f"@{target_username},{target_floor_index}"
                
                if logger:
                    logger.debug(f"å¤„ç†å›å¤å…³ç³»: {current_username} å›å¤ {target_username}ï¼Œæ¥¼ä¸­æ¥¼åºå· {target_floor_index}")
            else:
                # æ‰¾ä¸åˆ°å¯¹åº”çš„parentè¯„è®ºï¼Œä½¿ç”¨åŸå§‹å›å¤å¯¹è±¡
                processed_comment['å›å¤è¯„è®ºå¯¹è±¡'] = comment.get('å›å¤å¯¹è±¡', '')
                if logger:
                    logger.warning(f"æœªæ‰¾åˆ°parent={parent}å¯¹åº”çš„è¯„è®ºï¼Œä½¿ç”¨åŸå§‹å›å¤å¯¹è±¡")
        else:
            # ä¸»æ¥¼è¯„è®ºæˆ–parentä¸º0çš„è¯„è®ºï¼Œä¸è®¾ç½®å›å¤å¯¹è±¡
            processed_comment['å›å¤è¯„è®ºå¯¹è±¡'] = ''
        
        processed_comments.append(processed_comment)
    
    if logger:
        logger.info(f"å›å¤å…³ç³»å¤„ç†å®Œæˆï¼Œå…±å¤„ç† {len(processed_comments)} æ¡è¯„è®º")
    
    return processed_comments







def process_and_organize_data(all_comments, output_folder, oid, logger=None, video_title=None, sort_by_popularity=True, video_info=None, mode=None):
    """
    æ•´ç†å’Œç»Ÿè®¡è¯„è®ºæ•°æ®
    
    Args:
        all_comments (list): æ‰€æœ‰è¯„è®ºæ•°æ®
        output_folder (str): è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„
        oid (str): è§†é¢‘oid
        logger: æ—¥å¿—è®°å½•å™¨
        sort_by_popularity (bool): æ˜¯å¦æŒ‰çƒ­åº¦æ’åºæ•´ç†æ•°æ®
        video_info (dict): è§†é¢‘ä¿¡æ¯
        mode (str): è¿è¡Œæ¨¡å¼ï¼Œç”¨äºåŒºåˆ†æ˜¯å¦ç”Ÿæˆæ—¶é—´æ’åºæœ€ç»ˆæ–‡ä»¶
    
    Returns:
        tuple: (None, æ•´ç†æ•°æ®æ–‡ä»¶è·¯å¾„æˆ–None, ç»Ÿè®¡æ–‡ä»¶è·¯å¾„)
    """
    if not all_comments:
        return None, None, None
    
    # ç”Ÿæˆæ•´ç†æ•°æ®æ–‡ä»¶ï¼ˆåŒæ—¶è¾“å‡ºæœ‰æ¥¼ä¸­æ¥¼å’Œæ— æ¥¼ä¸­æ¥¼ç‰ˆæœ¬ï¼‰
    processed_filename = None
    main_floor_filename = None
    
    if sort_by_popularity:
        # æŒ‰çƒ­åº¦æ’åºæ•´ç†æ•°æ®
        sorted_comments = sort_comments_by_popularity(all_comments, logger)
        
        # å¤„ç†å›å¤å…³ç³»
        processed_comments = process_reply_relationships(sorted_comments, logger)
        main_floor_comments = []  # åªåŒ…å«ä¸»æ¥¼è¯„è®ºçš„åˆ—è¡¨
        
        # ç­›é€‰ä¸»æ¥¼è¯„è®º
        for comment in processed_comments:
            if comment['æ¥¼ä¸­æ¥¼åºå·'] == 0:
                main_floor_comments.append(comment)
        
        # ä¿å­˜å®Œæ•´æ•´ç†æ•°æ®ï¼ˆåŒ…å«æ¥¼ä¸­æ¥¼ï¼‰
        filename = generate_safe_filename(video_title, oid, "æœ€ç»ˆæ•°æ®_æŒ‰çƒ­åº¦æ’åº", "final")
        processed_filename = os.path.join(output_folder, f'{filename}.csv')
        with open(processed_filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
            if processed_comments:
                fieldnames = ['ä¸»æ¥¼åºå·', 'æ¥¼ä¸­æ¥¼åºå·', 'ç”¨æˆ·åç§°', 'è¯„è®ºå†…å®¹', 'å›å¤è¯„è®ºå¯¹è±¡', 'ç‚¹èµæ•°', 'å›å¤æ•°', 'å‘å¸ƒæ—¶é—´', 'ç”¨æˆ·ç­‰çº§', 'IPåœ°åŒº', 'æ€§åˆ«', 'è¯„è®ºç±»å‹']
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(processed_comments)
        
        # ä¿å­˜åªåŒ…å«ä¸»æ¥¼è¯„è®ºçš„æ–‡ä»¶ï¼ˆæ— æ¥¼ä¸­æ¥¼ï¼‰
        main_floor_name = generate_safe_filename(video_title, oid, "æœ€ç»ˆæ•°æ®_æŒ‰çƒ­åº¦æ’åºä¸”æ— æ¥¼ä¸­æ¥¼", "final")
        main_floor_filename = os.path.join(output_folder, f'{main_floor_name}.csv')
        with open(main_floor_filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
            if main_floor_comments:
                # ä¸»æ¥¼è¯„è®ºæ–‡ä»¶åŒ…å«å‘å¸ƒæ—¶é—´å’Œå›å¤æ•°å­—æ®µ
                fieldnames = ['ä¸»æ¥¼åºå·', 'ç”¨æˆ·åç§°', 'è¯„è®ºå†…å®¹', 'ç‚¹èµæ•°', 'å›å¤æ•°', 'å‘å¸ƒæ—¶é—´']
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()
                
                # è¿‡æ»¤æ‰ä¸éœ€è¦çš„å­—æ®µ
                filtered_comments = []
                for comment in main_floor_comments:
                    filtered_comment = {key: comment[key] for key in fieldnames if key in comment}
                    filtered_comments.append(filtered_comment)
                
                writer.writerows(filtered_comments)
        
        if logger:
            logger.info(f"æŒ‰çƒ­åº¦æ’åºæ•´ç†å®Œæˆï¼Œå…± {len(processed_comments)} æ¡è¯„è®º")
            logger.info(f"å®Œæ•´è¯„è®ºæ–‡ä»¶å·²ä¿å­˜: {processed_filename}")
            logger.info(f"ä¸»æ¥¼è¯„è®ºæ–‡ä»¶å·²ä¿å­˜: {main_floor_filename}ï¼Œå…± {len(main_floor_comments)} æ¡ä¸»æ¥¼è¯„è®º")
    else:
        # æŒ‰æ—¶é—´æ’åºæ•´ç†æ•°æ® - åªæœ‰åœ¨æµ‹è¯•æ¨¡å¼æ—¶é—´æ’åºä¸‹æ‰ç”Ÿæˆæœ€ç»ˆæ–‡ä»¶
        if mode == "test_time":
            sorted_comments = sort_comments_by_time(all_comments, logger)
            
            # å¤„ç†å›å¤å…³ç³»
            processed_comments = process_reply_relationships(sorted_comments, logger)
            main_floor_comments = []  # åªåŒ…å«ä¸»æ¥¼è¯„è®ºçš„åˆ—è¡¨
            
            # ç­›é€‰ä¸»æ¥¼è¯„è®º
            for comment in processed_comments:
                if comment['æ¥¼ä¸­æ¥¼åºå·'] == 0:
                    main_floor_comments.append(comment)
            
            # ä¿å­˜å®Œæ•´æ•´ç†æ•°æ®ï¼ˆåŒ…å«æ¥¼ä¸­æ¥¼ï¼‰
            filename = generate_safe_filename(video_title, oid, "æœ€ç»ˆæ•°æ®_æŒ‰æ—¶é—´æ’åº", "final")
            processed_filename = os.path.join(output_folder, f'{filename}.csv')
            with open(processed_filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
                if processed_comments:
                    fieldnames = ['ä¸»æ¥¼åºå·', 'æ¥¼ä¸­æ¥¼åºå·', 'ç”¨æˆ·åç§°', 'è¯„è®ºå†…å®¹', 'å›å¤è¯„è®ºå¯¹è±¡', 'ç‚¹èµæ•°', 'å›å¤æ•°', 'å‘å¸ƒæ—¶é—´', 'ç”¨æˆ·ç­‰çº§', 'IPåœ°åŒº', 'æ€§åˆ«', 'è¯„è®ºç±»å‹']
                    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                    writer.writeheader()
                    writer.writerows(processed_comments)
            
            # ä¿å­˜åªåŒ…å«ä¸»æ¥¼è¯„è®ºçš„æ–‡ä»¶ï¼ˆæ— æ¥¼ä¸­æ¥¼ï¼‰
            main_floor_name = generate_safe_filename(video_title, oid, "æœ€ç»ˆæ•°æ®_æŒ‰æ—¶é—´æ’åºä¸”æ— æ¥¼ä¸­æ¥¼", "final")
            main_floor_filename = os.path.join(output_folder, f'{main_floor_name}.csv')
            with open(main_floor_filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
                if main_floor_comments:
                    # ä¸»æ¥¼è¯„è®ºæ–‡ä»¶åŒ…å«å‘å¸ƒæ—¶é—´å’Œå›å¤æ•°å­—æ®µ
                    fieldnames = ['ä¸»æ¥¼åºå·', 'ç”¨æˆ·åç§°', 'è¯„è®ºå†…å®¹', 'ç‚¹èµæ•°', 'å›å¤æ•°', 'å‘å¸ƒæ—¶é—´']
                    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                    writer.writeheader()
                    
                    # è¿‡æ»¤æ‰ä¸éœ€è¦çš„å­—æ®µ
                    filtered_comments = []
                    for comment in main_floor_comments:
                        filtered_comment = {key: comment[key] for key in fieldnames if key in comment}
                        filtered_comments.append(filtered_comment)
                    
                    writer.writerows(filtered_comments)
            
            if logger:
                logger.info(f"æŒ‰æ—¶é—´æ’åºæ•´ç†å®Œæˆï¼Œå…± {len(processed_comments)} æ¡è¯„è®º")
                logger.info(f"å®Œæ•´è¯„è®ºæ–‡ä»¶å·²ä¿å­˜: {processed_filename}")
                logger.info(f"ä¸»æ¥¼è¯„è®ºæ–‡ä»¶å·²ä¿å­˜: {main_floor_filename}ï¼Œå…± {len(main_floor_comments)} æ¡ä¸»æ¥¼è¯„è®º")
        else:
            # éæµ‹è¯•æ¨¡å¼æ—¶é—´æ’åºï¼Œä¸ç”Ÿæˆæœ€ç»ˆæ–‡ä»¶ï¼Œåªç”¨äºç»Ÿè®¡
            processed_filename = None
            main_floor_filename = None
            if logger:
                logger.info(f"éæµ‹è¯•æ¨¡å¼æ—¶é—´æ’åºï¼Œè·³è¿‡æœ€ç»ˆæ–‡ä»¶ç”Ÿæˆï¼Œä»…ç”¨äºç»Ÿè®¡")
    
    # ç”Ÿæˆç»Ÿè®¡æ–‡ä»¶ï¼ˆæ·»åŠ "ç»Ÿè®¡ç»“æœ"å‰ç¼€ï¼‰
    stats_name = generate_safe_filename(video_title, oid, "ç»Ÿè®¡ç»“æœ", "stats")
    stats_filename = os.path.join(output_folder, f'{stats_name}.txt')
    # ç”ŸæˆBVå·
    bv_id = None
    if oid:
        try:
            bv_id = aid_to_bvid(int(oid))
        except:
            pass
    generate_statistics(all_comments, stats_filename, logger, oid, video_title, bv_id, video_info)
    
    # æ—¶é—´ç»Ÿè®¡å°†åœ¨ä¸»å‡½æ•°ä¸­ç»Ÿä¸€å¤„ç†ï¼Œé¿å…é‡å¤è°ƒç”¨
    
    return None, processed_filename, stats_filename

def sort_comments_by_popularity(all_comments, logger=None):
    """
    æŒ‰çƒ­åº¦æ’åºæ•´ç†è¯„è®ºæ•°æ®
    
    æ’åºè§„åˆ™ï¼š
    1. ä¸»æ¥¼è¯„è®ºæŒ‰ç‚¹èµæ•°é™åºæ’åºï¼Œç‚¹èµæ•°ç›¸åŒæ—¶æŒ‰å‘å¸ƒæ—¶é—´å‡åºæ’åº
    2. æ¯ä¸ªä¸»æ¥¼çš„æ¥¼ä¸­æ¥¼è¯„è®ºæŒ‰å‘å¸ƒæ—¶é—´å‡åºæ’åº
    3. é‡æ–°åˆ†é…åºå·ï¼šä¸»æ¥¼åºå·ä»1å¼€å§‹ï¼Œæ¥¼ä¸­æ¥¼åºå·åœ¨æ¯ä¸ªä¸»æ¥¼å†…ä»1å¼€å§‹
    
    Args:
        all_comments (list): æ‰€æœ‰è¯„è®ºæ•°æ®
        logger: æ—¥å¿—è®°å½•å™¨
    
    Returns:
        list: æŒ‰çƒ­åº¦æ’åºåçš„è¯„è®ºæ•°æ®
    """
    if not all_comments:
        return []
    
    if logger:
        logger.info("å¼€å§‹æŒ‰çƒ­åº¦æ’åºæ•´ç†è¯„è®ºæ•°æ®")
    
    # åˆ†ç¦»ä¸»æ¥¼è¯„è®ºå’Œæ¥¼ä¸­æ¥¼è¯„è®º
    main_comments = []  # ä¸»æ¥¼è¯„è®º
    sub_comments_dict = {}  # æ¥¼ä¸­æ¥¼è¯„è®ºï¼Œä»¥åŸä¸»æ¥¼åºå·ä¸ºkey
    
    for comment in all_comments:
        if comment['è¯„è®ºç±»å‹'] == 'ä¸»æ¥¼è¯„è®º':
            main_comments.append(comment)
            # åˆå§‹åŒ–è¯¥ä¸»æ¥¼çš„æ¥¼ä¸­æ¥¼åˆ—è¡¨
            original_main_index = comment['ä¸»æ¥¼åºå·']
            sub_comments_dict[original_main_index] = []
        elif comment['è¯„è®ºç±»å‹'] == 'æ¥¼ä¸­æ¥¼å›å¤':
            # æ‰¾åˆ°å¯¹åº”çš„ä¸»æ¥¼è¯„è®º
            # éœ€è¦é€šè¿‡éå†æ‰¾åˆ°å¯¹åº”çš„ä¸»æ¥¼
            for main_comment in main_comments:
                # æ£€æŸ¥æ˜¯å¦å±äºåŒä¸€ä¸ªä¸»æ¥¼ï¼ˆé€šè¿‡ç”¨æˆ·åã€æ—¶é—´ç­‰ä¿¡æ¯åˆ¤æ–­ï¼‰
                # è¿™é‡Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ›´å¥½çš„æ–¹æ³•æ¥å…³è”æ¥¼ä¸­æ¥¼å’Œä¸»æ¥¼
                pass
    
    # ç”±äºå½“å‰æ•°æ®ç»“æ„çš„é™åˆ¶ï¼Œæˆ‘ä»¬éœ€è¦é‡æ–°è®¾è®¡å…³è”é€»è¾‘
    # ä¸´æ—¶è§£å†³æ–¹æ¡ˆï¼šæŒ‰ç…§è¯„è®ºåœ¨åŸå§‹åˆ—è¡¨ä¸­çš„é¡ºåºæ¥å…³è”
    main_comments = []
    current_main_comment = None
    current_sub_comments = []
    sorted_result = []
    
    for comment in all_comments:
        if comment['è¯„è®ºç±»å‹'] == 'ä¸»æ¥¼è¯„è®º':
            # å¦‚æœä¹‹å‰æœ‰ä¸»æ¥¼è¯„è®ºï¼Œå…ˆå¤„ç†ä¹‹å‰çš„ä¸»æ¥¼å’Œå…¶æ¥¼ä¸­æ¥¼
            if current_main_comment is not None:
                main_comments.append({
                    'main': current_main_comment,
                    'subs': current_sub_comments.copy()
                })
            
            # å¼€å§‹æ–°çš„ä¸»æ¥¼
            current_main_comment = comment
            current_sub_comments = []
        elif comment['è¯„è®ºç±»å‹'] == 'æ¥¼ä¸­æ¥¼å›å¤':
            # æ·»åŠ åˆ°å½“å‰ä¸»æ¥¼çš„æ¥¼ä¸­æ¥¼åˆ—è¡¨
            if current_main_comment is not None:
                current_sub_comments.append(comment)
    
    # å¤„ç†æœ€åä¸€ä¸ªä¸»æ¥¼
    if current_main_comment is not None:
        main_comments.append({
            'main': current_main_comment,
            'subs': current_sub_comments.copy()
        })
    
    if logger:
        logger.info(f"åˆ†ç¦»å®Œæˆï¼š{len(main_comments)} ä¸ªä¸»æ¥¼è¯„è®º")
    
    # å¯¹ä¸»æ¥¼è¯„è®ºæŒ‰çƒ­åº¦æ’åº
    def sort_key_main(comment_group):
        main_comment = comment_group['main']
        likes = main_comment['ç‚¹èµæ•°'] if isinstance(main_comment['ç‚¹èµæ•°'], int) else 0
        # å‘å¸ƒæ—¶é—´è½¬æ¢ä¸ºæ—¶é—´æˆ³ç”¨äºæ’åºï¼ˆæ—¶é—´è¶Šæ—©ï¼Œæ—¶é—´æˆ³è¶Šå°ï¼‰
        time_str = main_comment['å‘å¸ƒæ—¶é—´']
        try:
            if time_str and time_str != 'æœªçŸ¥æ—¶é—´':
                time_obj = datetime.strptime(time_str, '%Y-%m-%d %H:%M:%S')
                timestamp = time_obj.timestamp()
            else:
                timestamp = float('inf')  # æœªçŸ¥æ—¶é—´æ’åœ¨æœ€å
        except:
            timestamp = float('inf')
        
        return (-likes, timestamp)  # ç‚¹èµæ•°é™åºï¼Œæ—¶é—´æˆ³å‡åº
    
    main_comments.sort(key=sort_key_main)
    
    if logger:
        logger.info("ä¸»æ¥¼è¯„è®ºçƒ­åº¦æ’åºå®Œæˆ")
    
    # é‡æ–°ç»„ç»‡æ•°æ®å¹¶åˆ†é…åºå·
    sorted_result = []
    
    for main_index, comment_group in enumerate(main_comments, 1):
        main_comment = comment_group['main'].copy()
        sub_comments = comment_group['subs']
        
        # é‡æ–°åˆ†é…ä¸»æ¥¼åºå·
        main_comment['ä¸»æ¥¼åºå·'] = main_index
        main_comment['æ¥¼ä¸­æ¥¼åºå·'] = 0
        sorted_result.append(main_comment)
        
        # å¯¹æ¥¼ä¸­æ¥¼è¯„è®ºæŒ‰æ—¶é—´æ’åº
        def sort_key_sub(comment):
            time_str = comment['å‘å¸ƒæ—¶é—´']
            try:
                if time_str and time_str != 'æœªçŸ¥æ—¶é—´':
                    time_obj = datetime.strptime(time_str, '%Y-%m-%d %H:%M:%S')
                    return time_obj.timestamp()
                else:
                    return float('inf')
            except:
                return float('inf')
        
        sub_comments.sort(key=sort_key_sub)
        
        # é‡æ–°åˆ†é…æ¥¼ä¸­æ¥¼åºå·
        for sub_index, sub_comment in enumerate(sub_comments, 1):
            sub_comment_copy = sub_comment.copy()
            sub_comment_copy['ä¸»æ¥¼åºå·'] = ''
            sub_comment_copy['æ¥¼ä¸­æ¥¼åºå·'] = sub_index
            sorted_result.append(sub_comment_copy)
    
    if logger:
        logger.info(f"çƒ­åº¦æ’åºå®Œæˆï¼Œå…±æ•´ç† {len(sorted_result)} æ¡è¯„è®º")
    
    return sorted_result

def sort_comments_by_time(all_comments, logger=None):
    """
    æŒ‰æ—¶é—´æ’åºæ•´ç†è¯„è®ºæ•°æ®
    
    æ’åºè§„åˆ™ï¼š
    1. ä¸»æ¥¼è¯„è®ºæŒ‰å‘å¸ƒæ—¶é—´å‡åºæ’åº
    2. æ¯ä¸ªä¸»æ¥¼çš„æ¥¼ä¸­æ¥¼è¯„è®ºæŒ‰å‘å¸ƒæ—¶é—´å‡åºæ’åº
    3. é‡æ–°åˆ†é…åºå·ï¼šä¸»æ¥¼åºå·ä»1å¼€å§‹ï¼Œæ¥¼ä¸­æ¥¼åºå·åœ¨æ¯ä¸ªä¸»æ¥¼å†…ä»1å¼€å§‹
    
    Args:
        all_comments (list): æ‰€æœ‰è¯„è®ºæ•°æ®
        logger: æ—¥å¿—è®°å½•å™¨
    
    Returns:
        list: æŒ‰æ—¶é—´æ’åºåçš„è¯„è®ºæ•°æ®
    """
    if not all_comments:
        return []
    
    if logger:
        logger.info("å¼€å§‹æŒ‰æ—¶é—´æ’åºæ•´ç†è¯„è®ºæ•°æ®")
    
    # ä¸´æ—¶è§£å†³æ–¹æ¡ˆï¼šæŒ‰ç…§è¯„è®ºåœ¨åŸå§‹åˆ—è¡¨ä¸­çš„é¡ºåºæ¥å…³è”
    main_comments = []
    current_main_comment = None
    current_sub_comments = []
    
    for comment in all_comments:
        if comment['è¯„è®ºç±»å‹'] == 'ä¸»æ¥¼è¯„è®º':
            # å¦‚æœä¹‹å‰æœ‰ä¸»æ¥¼è¯„è®ºï¼Œå…ˆå¤„ç†ä¹‹å‰çš„ä¸»æ¥¼å’Œå…¶æ¥¼ä¸­æ¥¼
            if current_main_comment is not None:
                main_comments.append({
                    'main': current_main_comment,
                    'subs': current_sub_comments.copy()
                })
            
            # å¼€å§‹æ–°çš„ä¸»æ¥¼
            current_main_comment = comment
            current_sub_comments = []
        elif comment['è¯„è®ºç±»å‹'] == 'æ¥¼ä¸­æ¥¼å›å¤':
            # æ·»åŠ åˆ°å½“å‰ä¸»æ¥¼çš„æ¥¼ä¸­æ¥¼åˆ—è¡¨
            if current_main_comment is not None:
                current_sub_comments.append(comment)
    
    # å¤„ç†æœ€åä¸€ä¸ªä¸»æ¥¼
    if current_main_comment is not None:
        main_comments.append({
            'main': current_main_comment,
            'subs': current_sub_comments.copy()
        })
    
    if logger:
        logger.info(f"åˆ†ç¦»å®Œæˆï¼š{len(main_comments)} ä¸ªä¸»æ¥¼è¯„è®º")
    
    # å¯¹ä¸»æ¥¼è¯„è®ºæŒ‰æ—¶é—´æ’åº
    def sort_key_main(comment_group):
        main_comment = comment_group['main']
        time_str = main_comment['å‘å¸ƒæ—¶é—´']
        try:
            if time_str and time_str != 'æœªçŸ¥æ—¶é—´':
                time_obj = datetime.strptime(time_str, '%Y-%m-%d %H:%M:%S')
                timestamp = time_obj.timestamp()
            else:
                timestamp = float('inf')  # æœªçŸ¥æ—¶é—´æ’åœ¨æœ€å
        except:
            timestamp = float('inf')
        
        return timestamp  # æ—¶é—´æˆ³å‡åº
    
    main_comments.sort(key=sort_key_main)
    
    if logger:
        logger.info("ä¸»æ¥¼è¯„è®ºæ—¶é—´æ’åºå®Œæˆ")
    
    # é‡æ–°ç»„ç»‡æ•°æ®å¹¶åˆ†é…åºå·
    sorted_result = []
    
    for main_index, comment_group in enumerate(main_comments, 1):
        main_comment = comment_group['main'].copy()
        sub_comments = comment_group['subs']
        
        # é‡æ–°åˆ†é…ä¸»æ¥¼åºå·
        main_comment['ä¸»æ¥¼åºå·'] = main_index
        main_comment['æ¥¼ä¸­æ¥¼åºå·'] = 0
        sorted_result.append(main_comment)
        
        # å¯¹æ¥¼ä¸­æ¥¼è¯„è®ºæŒ‰æ—¶é—´æ’åº
        def sort_key_sub(comment):
            time_str = comment['å‘å¸ƒæ—¶é—´']
            try:
                if time_str and time_str != 'æœªçŸ¥æ—¶é—´':
                    time_obj = datetime.strptime(time_str, '%Y-%m-%d %H:%M:%S')
                    return time_obj.timestamp()
                else:
                    return float('inf')
            except:
                return float('inf')
        
        sub_comments.sort(key=sort_key_sub)
        
        # é‡æ–°åˆ†é…æ¥¼ä¸­æ¥¼åºå·
        for sub_index, sub_comment in enumerate(sub_comments, 1):
            sub_comment_copy = sub_comment.copy()
            sub_comment_copy['ä¸»æ¥¼åºå·'] = ''
            sub_comment_copy['æ¥¼ä¸­æ¥¼åºå·'] = sub_index
            sorted_result.append(sub_comment_copy)
    
    if logger:
        logger.info(f"æ—¶é—´æ’åºå®Œæˆï¼Œå…±æ•´ç† {len(sorted_result)} æ¡è¯„è®º")
    
    return sorted_result

# å¤‡ç”¨å‡½æ•°å·²åˆ é™¤ï¼Œä¸å†éœ€è¦

def generate_restructured_time_statistics(all_comments, output_folder, bvid, logger=None, video_title=None, video_info=None):
    """
    åŸºäºæ–°è®¾è®¡æ€è·¯é‡æ–°è®¾è®¡çš„æ—¶é—´ç»Ÿè®¡åŠŸèƒ½
    æ ¹æ®æ—¶é—´è·¨åº¦ç”Ÿæˆå¤šä¸ªç²’åº¦çš„ç»Ÿè®¡æ–‡ä»¶ï¼Œä¸¥æ ¼æŒ‰ç…§è®¾è®¡æ€è·¯æ‰§è¡Œ
    
    Args:
        all_comments (list): æ‰€æœ‰è¯„è®ºæ•°æ®
        output_folder (str): è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„
        bvid (str): è§†é¢‘BVå·
        logger: æ—¥å¿—è®°å½•å™¨
        video_title (str): è§†é¢‘æ ‡é¢˜
        video_info (dict): è§†é¢‘ä¿¡æ¯ï¼Œå¦‚æœæä¾›åˆ™ä¸å†é‡å¤è·å–
    
    Returns:
        list: ç”Ÿæˆçš„ç»Ÿè®¡æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    if not all_comments:
        return []
    
    # è·å–è§†é¢‘ä¿¡æ¯ï¼ˆå¦‚æœæœªæä¾›ï¼‰
    if not video_info:
        video_info = get_video_info(bvid)
        if not video_info:
            print("âŒ æ— æ³•è·å–è§†é¢‘ä¿¡æ¯ï¼Œæ— æ³•è¿›è¡ŒæŒ‰æ—¶é—´ç»Ÿè®¡")
            print("âš ï¸  æŒ‰æ—¶é—´ç»Ÿè®¡éœ€è¦è§†é¢‘å‘å¸ƒæ—¶é—´ä¿¡æ¯ï¼Œè·³è¿‡ç»Ÿè®¡")
            
            if logger:
                logger.warning("æ— æ³•è·å–è§†é¢‘å‘å¸ƒæ—¶é—´ï¼Œè·³è¿‡æŒ‰æ—¶é—´ç»Ÿè®¡")
                
            return []
    
    # è·å–è§†é¢‘å‘å¸ƒæ—¶é—´
    publish_timestamp = video_info.get('pubdate', 0)
    if publish_timestamp == 0:
        print("âŒ æ— æ³•è·å–è§†é¢‘å‘å¸ƒæ—¶é—´ï¼Œä½¿ç”¨ç°æœ‰æ—¶é—´ç»Ÿè®¡æ–¹å¼")
        return []
    
    publish_datetime = datetime.fromtimestamp(publish_timestamp)
    
    # è·å–æ‰€æœ‰æœ‰æ•ˆè¯„è®ºæ—¶é—´æˆ³
    valid_comments = [comment for comment in all_comments if comment.get('æ—¶é—´æˆ³', 0) > 0]
    if not valid_comments:
        return []
    
    # è¿‡æ»¤æ‰å‘å¸ƒæ—¶é—´ä¹‹å‰çš„è¯„è®º
    new_comments = [comment for comment in valid_comments if comment['æ—¶é—´æˆ³'] >= publish_timestamp]
    if not new_comments:
        print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°è§†é¢‘å‘å¸ƒåçš„è¯„è®º")
        return []
    
    timestamps = [comment['æ—¶é—´æˆ³'] for comment in new_comments]
    min_timestamp = min(timestamps)
    max_timestamp = max(timestamps)
    
    # è®¡ç®—æœ€æ–°è¯„è®ºä¸è§†é¢‘å‘å¸ƒæ—¶é—´çš„å·®å€¼
    time_diff = max_timestamp - publish_timestamp
    
    # è·å–æœ€æ–°å’Œæœ€æ—§è¯„è®ºçš„æ—¥æœŸæ—¶é—´
    max_datetime = datetime.fromtimestamp(max_timestamp)
    
    # æ ¹æ®è®¾è®¡æ€è·¯çš„å†³ç­–æ ‘åˆ¤æ–­éœ€è¦ç”Ÿæˆçš„ç»Ÿè®¡ç²’åº¦ï¼ˆå¯èƒ½ç”Ÿæˆå¤šä¸ªï¼‰
    granularities_to_generate = []
    
    # åˆ¤æ–­æ˜¯å¦è·¨å¹´
    if publish_datetime.year != max_datetime.year:
        granularities_to_generate = ['year', 'month', 'day']
    # åˆ¤æ–­æ˜¯å¦åŒå¹´ä¸åŒæœˆ
    elif publish_datetime.month != max_datetime.month:
        granularities_to_generate = ['month', 'day']
    # åˆ¤æ–­æ˜¯å¦åŒå¹´åŒæœˆä¸åŒæ—¥
    elif publish_datetime.date() != max_datetime.date():
        if time_diff > 604800:  # å¤§äº7å¤©
            granularities_to_generate = ['day']
        else:  # å°äºç­‰äº7å¤©
            granularities_to_generate = ['day', 'hour']
    # åŒå¹´åŒæœˆåŒæ—¥
    else:
        granularities_to_generate = ['hour', 'minute']
    
    if logger:
        logger.info(f"åŸºäºæ—¶é—´å·®å€¼{time_diff}ç§’ï¼Œéœ€è¦ç”Ÿæˆç»Ÿè®¡ç²’åº¦: {granularities_to_generate}")
    
    generated_files = []
    
    # ä¸ºæ¯ä¸ªéœ€è¦çš„ç²’åº¦ç”Ÿæˆç»Ÿè®¡æ–‡ä»¶å’Œå›¾è¡¨
    for granularity in granularities_to_generate:
        granularity_name = {'minute': 'åˆ†é’Ÿ', 'hour': 'å°æ—¶', 'day': 'æ—¥', 'month': 'æœˆ', 'year': 'å¹´'}[granularity]
        
        # ç”Ÿæˆç»Ÿè®¡æ•°æ®
        stats, time_points, counts = generate_time_stats_by_granularity(
            new_comments, granularity, publish_datetime, max_timestamp
        )
        
        # ä¿å­˜ç»Ÿè®¡ç»“æœåˆ°txtæ–‡ä»¶
        stats_file = save_restructured_time_statistics(
            stats, granularity_name, output_folder, bvid, publish_timestamp, max_timestamp, 
            logger, video_title, video_info, new_comments
        )
        generated_files.append(stats_file)
        
        # ç”ŸæˆæŠ˜çº¿å›¾
        chart_file = generate_time_trend_chart(
            time_points, counts, granularity_name, output_folder, bvid, video_title, publish_datetime, max_timestamp
        )
        if chart_file:
            generated_files.append(chart_file)
    
    return generated_files

from datetime import timedelta

def generate_time_stats_by_granularity(comments, granularity, publish_datetime, max_timestamp):
    """
    æ ¹æ®æŒ‡å®šç²’åº¦ç”Ÿæˆæ—¶é—´ç»Ÿè®¡æ•°æ®
    æŒ‰åˆ†é’Ÿå’Œå°æ—¶ç»Ÿè®¡æ—¶ä»¥è§†é¢‘å‘å¸ƒæ—¶é—´ä¸ºåŸºå‡†è¿›è¡Œæ—¶é—´æ®µåˆ’åˆ†
    
    Args:
        comments (list): è¯„è®ºæ•°æ®
        granularity (str): ç»Ÿè®¡ç²’åº¦ ('minute', 'hour', 'day', 'month', 'year')
        publish_datetime (datetime): è§†é¢‘å‘å¸ƒæ—¶é—´
        max_timestamp (int): æœ€æ–°è¯„è®ºæ—¶é—´æˆ³
    
    Returns:
        tuple: (stats, time_points, counts)
    """
    stats = {}
    time_points = []
    counts = []
    
    if granularity == 'minute':
        # æŒ‰åˆ†é’Ÿç»Ÿè®¡ - ä»¥è§†é¢‘å‘å¸ƒæ—¶é—´ä¸ºåŸºå‡†
        publish_timestamp = int(publish_datetime.timestamp())
        
        # è®¡ç®—éœ€è¦ç»Ÿè®¡çš„åˆ†é’Ÿæ•°
        total_minutes = int((max_timestamp - publish_timestamp) / 60) + 1
        
        for i in range(total_minutes):
            start_time = publish_timestamp + i * 60
            end_time = start_time + 60
            
            # ç»Ÿè®¡åœ¨è¿™ä¸ªæ—¶é—´æ®µå†…çš„è¯„è®ºæ•°é‡
            count = sum(1 for comment in comments 
                       if start_time <= comment['æ—¶é—´æˆ³'] < end_time)
            
            # ç”Ÿæˆæ˜¾ç¤ºç”¨çš„æ—¶é—´æ®µæè¿°
            if i == 0:
                key = f"è§†é¢‘å‘å¸ƒå0-1åˆ†é’Ÿå†…æ–°å¢è¯„è®ºæ•°é‡"
            else:
                key = f"è§†é¢‘å‘å¸ƒå{i}-{i+1}åˆ†é’Ÿå†…æ–°å¢è¯„è®ºæ•°é‡"
            
            stats[key] = count
            time_points.append(datetime.fromtimestamp(start_time))
            counts.append(count)
            
    elif granularity == 'hour':
        # æŒ‰å°æ—¶ç»Ÿè®¡ - ä»¥è§†é¢‘å‘å¸ƒæ—¶é—´ä¸ºåŸºå‡†
        publish_timestamp = int(publish_datetime.timestamp())
        
        # è®¡ç®—éœ€è¦ç»Ÿè®¡çš„å°æ—¶æ•°
        total_hours = int((max_timestamp - publish_timestamp) / 3600) + 1
        
        for i in range(total_hours):
            start_time = publish_timestamp + i * 3600
            end_time = start_time + 3600
            
            # ç»Ÿè®¡åœ¨è¿™ä¸ªæ—¶é—´æ®µå†…çš„è¯„è®ºæ•°é‡
            count = sum(1 for comment in comments 
                       if start_time <= comment['æ—¶é—´æˆ³'] < end_time)
            
            # ç”Ÿæˆæ˜¾ç¤ºç”¨çš„æ—¶é—´æ®µæè¿°
            if i == 0:
                key = f"è§†é¢‘å‘å¸ƒå0-1å°æ—¶å†…æ–°å¢çš„è¯„è®ºæ•°é‡"
            else:
                key = f"è§†é¢‘å‘å¸ƒå{i}-{i+1}å°æ—¶å†…æ–°å¢çš„è¯„è®ºæ•°é‡"
            
            stats[key] = count
            time_points.append(datetime.fromtimestamp(start_time))
            counts.append(count)
            
    elif granularity == 'day':
        # æŒ‰æ—¥ç»Ÿè®¡ - æ ¹æ®æ­£å¸¸æ—¶é—´ç»Ÿè®¡
        day_stats = {}
        for comment in comments:
            comment_dt = datetime.fromtimestamp(comment['æ—¶é—´æˆ³'])
            day_key = comment_dt.strftime('%Y/%m/%d')
            day_stats[day_key] = day_stats.get(day_key, 0) + 1
        
        # æŒ‰æ—¥æœŸæ’åº
        for day_key in sorted(day_stats.keys()):
            key = f"{day_key}å†…æ–°å¢çš„è¯„è®ºæ•°"
            stats[key] = day_stats[day_key]
            # è§£ææ—¥æœŸç”¨äºå›¾è¡¨
            year, month, day = map(int, day_key.split('/'))
            time_points.append(datetime(year, month, day))
            counts.append(day_stats[day_key])
            
    elif granularity == 'month':
        # æŒ‰æœˆç»Ÿè®¡ - æ ¹æ®æ­£å¸¸æ—¶é—´ç»Ÿè®¡
        month_stats = {}
        for comment in comments:
            comment_dt = datetime.fromtimestamp(comment['æ—¶é—´æˆ³'])
            month_key = comment_dt.strftime('%Y/%m')
            month_stats[month_key] = month_stats.get(month_key, 0) + 1
        
        # æŒ‰æœˆä»½æ’åº
        for month_key in sorted(month_stats.keys()):
            key = f"{month_key}å†…æ–°å¢çš„è¯„è®ºæ•°"
            stats[key] = month_stats[month_key]
            # è§£ææœˆä»½ç”¨äºå›¾è¡¨
            year, month = map(int, month_key.split('/'))
            time_points.append(datetime(year, month, 1))
            counts.append(month_stats[month_key])
            
    elif granularity == 'year':
        # æŒ‰å¹´ç»Ÿè®¡ - æ ¹æ®æ­£å¸¸æ—¶é—´ç»Ÿè®¡
        year_stats = {}
        for comment in comments:
            comment_dt = datetime.fromtimestamp(comment['æ—¶é—´æˆ³'])
            year_key = comment_dt.strftime('%Y')
            year_stats[year_key] = year_stats.get(year_key, 0) + 1
        
        # æŒ‰å¹´ä»½æ’åº
        for year_key in sorted(year_stats.keys()):
            key = f"{year_key}å†…æ–°å¢çš„è¯„è®ºæ•°"
            stats[key] = year_stats[year_key]
            # è§£æå¹´ä»½ç”¨äºå›¾è¡¨
            year = int(year_key)
            time_points.append(datetime(year, 1, 1))
            counts.append(year_stats[year_key])
    
    return stats, time_points, counts

def save_restructured_time_statistics(stats, granularity_name, output_folder, bvid, 
                                    publish_timestamp, max_timestamp, logger=None, 
                                    video_title=None, video_info=None, comments=None):
    """
    ä¿å­˜é‡æ–°è®¾è®¡çš„æ—¶é—´ç»Ÿè®¡ç»“æœï¼Œä¸¥æ ¼æŒ‰ç…§è®¾è®¡æ€è·¯æ ¼å¼
    """
    # åˆ›å»º"æŒ‰æ—¶é—´ç»Ÿè®¡"æ–‡ä»¶å¤¹
    time_stats_folder = os.path.join(output_folder, "æŒ‰æ—¶é—´ç»Ÿè®¡")
    if not os.path.exists(time_stats_folder):
        os.makedirs(time_stats_folder)
    
    # ç”Ÿæˆæ–‡ä»¶å - ä¸¥æ ¼æŒ‰ç…§è®¾è®¡æ€è·¯è¦æ±‚çš„æ ¼å¼ï¼š"è¯„è®ºçˆ¬å–ç»Ÿè®¡ç»“æœ_æŒ‰åˆ†é’Ÿ/å°æ—¶/æ—¥/æœˆ/å¹´ç»Ÿè®¡_{è§†é¢‘åç§°}_{è§†é¢‘BVå·}"
    safe_title = "".join(c for c in (video_title or "") if c.isalnum() or c in (' ', '-', '_')).strip()
    safe_title = safe_title[:30]  # é™åˆ¶é•¿åº¦ï¼Œé¿å…æ–‡ä»¶åè¿‡é•¿
    
    filename = f"è¯„è®ºçˆ¬å–ç»Ÿè®¡ç»“æœ_æŒ‰{granularity_name}ç»Ÿè®¡_{safe_title}_{bvid}.txt"
    filepath = os.path.join(time_stats_folder, filename)
    
    # è®¡ç®—ç»Ÿè®¡æ±‡æ€»ä¿¡æ¯
    if comments:
        min_timestamp = min(comment['æ—¶é—´æˆ³'] for comment in comments if comment.get('æ—¶é—´æˆ³', 0) > 0)
    else:
        min_timestamp = publish_timestamp
    
    total_comments = sum(stats.values())
    total_periods = len(stats)
    
    # æ‰¾å‡ºæœ€é«˜å³°å’Œæœ€ä½è°·
    if stats:
        max_count = max(stats.values())
        min_count = min(stats.values())
        max_period = [k for k, v in stats.items() if v == max_count][0]
        min_period = [k for k, v in stats.items() if v == min_count][0]
        
        # æå–æ—¶é—´æ®µæè¿°
        max_period_desc = max_period.replace('å†…æ–°å¢çš„è¯„è®ºæ•°', '')
        min_period_desc = min_period.replace('å†…æ–°å¢çš„è¯„è®ºæ•°', '')
    else:
        max_count = min_count = 0
        max_period_desc = min_period_desc = "æ— æ•°æ®"
    
    # è®¡ç®—å¹³å‡å€¼
    avg_comments = total_comments / total_periods if total_periods > 0 else 0
    
    # å†™å…¥ç»Ÿè®¡æ•°æ®åˆ°æ–‡ä»¶
    with open(filepath, 'w', encoding='utf-8') as f:
        # å…ˆè®°å½•æœ€æ–°è¯„è®ºä¸æœ€æ—§è¯„è®ºçš„æ—¶é—´
        f.write(f"æœ€æ–°è¯„è®ºæ—¶é—´ï¼š{datetime.fromtimestamp(max_timestamp).strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"æœ€æ—§è¯„è®ºæ—¶é—´ï¼š{datetime.fromtimestamp(min_timestamp).strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        # ç»Ÿè®¡æ±‡æ€»
        f.write("=== ç»Ÿè®¡æ±‡æ€» ===\n")
        f.write(f"- ç»Ÿè®¡ç²’åº¦: {granularity_name}\n")
        f.write(f"- ç»Ÿè®¡å¯¹è±¡ï¼š{video_title or 'æœªçŸ¥è§†é¢‘'}\n")
        f.write(f"- ç»Ÿè®¡å¯¹è±¡BVå·ï¼š{bvid}\n")
        f.write(f"- ç´¯è®¡ç»Ÿè®¡æ—¶é—´æ®µæ•°é‡: {total_periods}\n")
        f.write(f"- æ€»è¯„è®ºæ•°: {total_comments}\n")
        f.write(f"- å¹³å‡æ¯{granularity_name}: {avg_comments:.2f} æ¡è¯„è®º\n")
        f.write(f"- æœ€é«˜å³°: {max_count} æ¡è¯„è®ºï¼Œå‡ºç°åœ¨ {max_period_desc}\n")
        f.write(f"- æœ€ä½è°·: {min_count} æ¡è¯„è®ºï¼Œå‡ºç°åœ¨ {min_period_desc}\n\n")
        
        # è¯¦ç»†ç»Ÿè®¡æ•°æ®
        f.write("=== è¯¦ç»†ç»Ÿè®¡ ===\n")
        # æŒ‰æ—¶é—´é¡ºåºæ’åºè¾“å‡º
        sorted_stats = sorted(stats.items())
        for key, count in sorted_stats:
            f.write(f"{key}ï¼š{count}\n")
    
    if logger:
        logger.info(f"æ—¶é—´ç»Ÿè®¡æ–‡ä»¶å·²ä¿å­˜: {filepath}")
    
    return filepath
    


def generate_time_trend_chart(time_points, counts, granularity_name, output_folder, 
                           bvid, video_title, publish_datetime, max_timestamp):
    """
    ä½¿ç”¨matplotlibç”Ÿæˆè¯„è®ºæ•°é‡å˜åŒ–è¶‹åŠ¿çš„æŠ˜çº¿å›¾
    """
    try:
        import matplotlib.pyplot as plt
        import matplotlib.dates as mdates
        from matplotlib.font_manager import FontProperties
        
        # åˆ›å»ºå›¾è¡¨
        plt.figure(figsize=(12, 6))
        
        # ç»˜åˆ¶æŠ˜çº¿å›¾
        plt.plot(time_points, counts, marker='o', linewidth=2, markersize=4)
        
        # è®¾ç½®æ ‡é¢˜å’Œæ ‡ç­¾
        try:
            font_prop = FontProperties(fname='C:\\Windows\\Fonts\\simhei.ttf')  # å‡è®¾ç³»ç»Ÿæœ‰SimHeiå­—ä½“ï¼Œéœ€ç¡®è®¤è·¯å¾„
            plt.rcParams['font.sans-serif'] = [font_prop.get_name()]
            plt.rcParams['axes.unicode_minus'] = False
        except Exception as font_error:
            print(f"âš ï¸ å­—ä½“è®¾ç½®å¤±è´¥: {font_error}. ä½¿ç”¨é»˜è®¤å­—ä½“ã€‚")
            plt.rcParams['axes.unicode_minus'] = False
        
        # è®¡ç®—æœ€æ–°è¯„è®ºæ—¶é—´
        max_comment_datetime = datetime.fromtimestamp(max_timestamp)
        
        plt.title(f'{video_title or "æœªçŸ¥è§†é¢‘"} - {bvid} - è¯„è®ºæ•°é‡å˜åŒ–è¶‹åŠ¿\nè§†é¢‘å‘å¸ƒæ—¶é—´ï¼š{publish_datetime.strftime("%Y-%m-%d %H:%M:%S")} - æœ€æ–°è¯„è®ºæ—¶é—´ï¼š{max_comment_datetime.strftime("%Y-%m-%d %H:%M:%S")} - æŒ‰{granularity_name}ç»Ÿè®¡', 
                 fontsize=14, fontweight='bold')
        plt.xlabel(f'æ—¶é—´ï¼ˆ{granularity_name}ï¼‰', fontsize=12)
        plt.ylabel('è¯„è®ºæ•°é‡', fontsize=12)
        
        # è®¾ç½®ç½‘æ ¼
        plt.grid(True, alpha=0.3)
        
        # æ—‹è½¬xè½´æ ‡ç­¾
        plt.xticks(rotation=45)
        
        # æ ¹æ®ç»Ÿè®¡ç²’åº¦è®¾ç½®xè½´æ ¼å¼
        if granularity_name == 'åˆ†é’Ÿ':
            plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))
        elif granularity_name == 'å°æ—¶':
            plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%m-%d %H:00'))
        elif granularity_name == 'æ—¥':
            plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))
        elif granularity_name == 'æœˆ':
            plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))
        else:  # å¹´
            plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        safe_title = "".join(c for c in (video_title or "") if c.isalnum() or c in (' ', '-', '_')).strip()
        safe_title = safe_title[:30]  # é™åˆ¶é•¿åº¦
        timestamp_str = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        chart_filename = f"è¯„è®ºçˆ¬å–ç»Ÿè®¡ç»“æœ_æŒ‰{granularity_name}ç»Ÿè®¡_{bvid}_{safe_title}_{timestamp_str}_è¶‹åŠ¿å›¾.png"
        chart_filepath = os.path.join(output_folder, "æŒ‰æ—¶é—´ç»Ÿè®¡", chart_filename)
        
        plt.savefig(chart_filepath, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“ˆ è¯„è®ºè¶‹åŠ¿å›¾å·²ç”Ÿæˆ: {chart_filepath}")
        return chart_filepath
        
    except ImportError:
        print("âš ï¸  matplotlibæœªå®‰è£…ï¼Œæ— æ³•ç”Ÿæˆè¶‹åŠ¿å›¾")
        return None
    except Exception as e:
        print(f"âš ï¸  ç”Ÿæˆè¶‹åŠ¿å›¾æ—¶å‡ºé”™: {e}")
        return None

# ä¿æŒåŸæœ‰çš„æ—¶é—´ç»Ÿè®¡å‡½æ•°ä½œä¸ºå¤‡é€‰
def generate_smart_time_statistics(all_comments, output_folder, oid, logger=None, video_title=None):
    """
    æ™ºèƒ½é€‰æ‹©æ—¶é—´ç»Ÿè®¡æ–¹å¼å¹¶ç”Ÿæˆå¯¹åº”çš„ç»Ÿè®¡æ–‡ä»¶
    
    Args:
        all_comments (list): æ‰€æœ‰è¯„è®ºæ•°æ®
        output_folder (str): è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„
        oid (str): è§†é¢‘oid
        logger: æ—¥å¿—è®°å½•å™¨
        video_title (str): è§†é¢‘æ ‡é¢˜
    
    Returns:
        list: ç”Ÿæˆçš„ç»Ÿè®¡æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """

def generate_statistics(all_comments, stats_filename, logger=None, oid=None, video_title=None, bv_id=None, video_info=None):
    """
    ç”Ÿæˆç”¨æˆ·ä¿¡æ¯ç»Ÿè®¡
    
    Args:
        all_comments (list): æ‰€æœ‰è¯„è®ºæ•°æ®
        stats_filename (str): ç»Ÿè®¡æ–‡ä»¶è·¯å¾„
        logger: æ—¥å¿—è®°å½•å™¨
        oid (str): è§†é¢‘OID
        video_title (str): è§†é¢‘æ ‡é¢˜
        bv_id (str): è§†é¢‘BVå·
        video_info (dict): è§†é¢‘ä¿¡æ¯ï¼ŒåŒ…å«å‘å¸ƒæ—¶é—´ç­‰
    """
    if not all_comments:
        return
    
    # ç»Ÿè®¡å„é¡¹ä¿¡æ¯
    genders = [comment['æ€§åˆ«'] for comment in all_comments if comment['æ€§åˆ«']]
    locations = [comment['IPåœ°åŒº'] for comment in all_comments if comment['IPåœ°åŒº']]
    levels = [comment['ç”¨æˆ·ç­‰çº§'] for comment in all_comments if comment['ç”¨æˆ·ç­‰çº§']]
    comment_types = [comment['è¯„è®ºç±»å‹'] for comment in all_comments if comment['è¯„è®ºç±»å‹']]
    
    # è®¡ç®—ç»Ÿè®¡æ•°æ®
    gender_stats = Counter(genders)
    location_stats = Counter(locations)
    level_stats = Counter(levels)
    type_stats = Counter(comment_types)
    
    # è®¡ç®—æ€»æ•°å’Œå…¶ä»–ç»Ÿè®¡ä¿¡æ¯
    total_comments = len(all_comments)
    total_likes = sum(comment['ç‚¹èµæ•°'] for comment in all_comments if isinstance(comment['ç‚¹èµæ•°'], int))
    total_replies = sum(comment['å›å¤æ•°'] for comment in all_comments if isinstance(comment['å›å¤æ•°'], int))
    
    # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
    with open(stats_filename, 'w', encoding='utf-8') as f:
        f.write("=== Bç«™è¯„è®ºæ•°æ®ç»Ÿè®¡æŠ¥å‘Š ===\n\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        
        # æ’å…¥è§†é¢‘å‘å¸ƒæ—¶é—´
        if video_info and video_info.get('pubdate', 0) > 0:
            publish_time = datetime.fromtimestamp(video_info['pubdate']).strftime('%Y-%m-%d %H:%M:%S')
            f.write(f"è§†é¢‘å‘å¸ƒæ—¶é—´: {publish_time}\n")
        
        if bv_id:
            f.write(f"è§†é¢‘BVå·: {bv_id}\n")
        if oid:
            f.write(f"è§†é¢‘OID: {oid}\n")
        if video_title:
            f.write(f"è§†é¢‘æ ‡é¢˜: {video_title}\n")
        f.write(f"æ€»è¯„è®ºæ•°: {total_comments} æ¡\n")
        f.write(f"æ€»ç‚¹èµæ•°: {total_likes} ä¸ª\n")
        f.write(f"æ€»å›å¤æ•°: {total_replies} ä¸ª\n\n")
        
        # æ€§åˆ«ç»Ÿè®¡
        f.write("=== ç”¨æˆ·æ€§åˆ«åˆ†å¸ƒ ===\n")
        for gender, count in gender_stats.most_common():
            percentage = (count / len(genders)) * 100 if genders else 0
            f.write(f"{gender}: {count} åç”¨æˆ· ({percentage:.1f}%)\n")
        f.write("\n")
        
        # IPåœ°åŒºç»Ÿè®¡
        f.write("=== ç”¨æˆ·IPåœ°åŒºåˆ†å¸ƒ ===\n")
        for location, count in location_stats.most_common():
            percentage = (count / len(locations)) * 100 if locations else 0
            f.write(f"{location}: {count} åç”¨æˆ· ({percentage:.1f}%)\n")
        f.write("\n")
        
        # ç”¨æˆ·ç­‰çº§ç»Ÿè®¡
        f.write("=== ç”¨æˆ·ç­‰çº§åˆ†å¸ƒ ===\n")
        for level, count in sorted(level_stats.items()):
            percentage = (count / len(levels)) * 100 if levels else 0
            f.write(f"LV{level}: {count} åç”¨æˆ· ({percentage:.1f}%)\n")
        f.write("\n")
        
        # è¯„è®ºç±»å‹ç»Ÿè®¡
        f.write("=== è¯„è®ºç±»å‹åˆ†å¸ƒ ===\n")
        for comment_type, count in type_stats.most_common():
            percentage = (count / total_comments) * 100
            f.write(f"{comment_type}: {count} æ¡ ({percentage:.1f}%)\n")
        f.write("\n")
        
        # çƒ­é—¨è¯„è®ºç»Ÿè®¡ï¼ˆç‚¹èµæ•°å‰10ï¼‰
        f.write("=== çƒ­é—¨è¯„è®ºTOP10 ===\n")
        sorted_comments = sorted(all_comments, key=lambda x: x['ç‚¹èµæ•°'] if isinstance(x['ç‚¹èµæ•°'], int) else 0, reverse=True)
        for i, comment in enumerate(sorted_comments[:10], 1):
            f.write(f"{i}. ç”¨æˆ·: {comment['ç”¨æˆ·åç§°']} | ç‚¹èµ: {comment['ç‚¹èµæ•°']} | å†…å®¹: {comment['è¯„è®ºå†…å®¹'][:50]}...\n")
    
    if logger:
        logger.info(f"ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜: {stats_filename}")
    
    print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯å·²ç”Ÿæˆ: {stats_filename}")
    print(f"   - æ€»è¯„è®ºæ•°: {total_comments} æ¡")
    print(f"   - æ€§åˆ«åˆ†å¸ƒ: {len(gender_stats)} ç§")
    print(f"   - åœ°åŒºåˆ†å¸ƒ: {len(location_stats)} ä¸ªåœ°åŒº")
    print(f"   - ç­‰çº§åˆ†å¸ƒ: LV{min(levels) if levels else 0}-LV{max(levels) if levels else 0}")

def merge_and_deduplicate_comments(popularity_comments, time_comments, logger=None):
    """
    åˆå¹¶ä¸¤ä¸ªè¯„è®ºåˆ—è¡¨å¹¶å»é™¤é‡å¤è¯„è®ºï¼ˆåŸºäºrpidå’Œæ—¶é—´æˆ³ï¼‰
    
    Args:
        popularity_comments (list): çƒ­åº¦æ’åºçš„è¯„è®ºåˆ—è¡¨
        time_comments (list): æ—¶é—´æ’åºçš„è¯„è®ºåˆ—è¡¨
        logger: æ—¥å¿—è®°å½•å™¨
    
    Returns:
        tuple: (åˆå¹¶å»é‡åçš„è¯„è®ºåˆ—è¡¨, é‡å¤è¯„è®ºåˆ—è¡¨)
    """
    if logger:
        logger.info("å¼€å§‹åˆå¹¶å’Œå»é‡è¯„è®ºï¼ˆåŸºäºrpidå’Œæ—¶é—´æˆ³ï¼‰")
    
    # ä½¿ç”¨åŸºäºçˆ¬å–æ—¶é—´çš„å»é‡é€»è¾‘
    def deduplicate_by_rpid(comments, comment_type):
        rpid_to_comment = {}
        duplicates = []
        
        def extract_crawl_time_from_filename(comment):
            """ä»è¯„è®ºæ•°æ®çš„æ¥æºæ–‡ä»¶åä¸­æå–çˆ¬å–æ—¶é—´"""
            # å°è¯•ä»è¯„è®ºæ•°æ®ä¸­è·å–æ–‡ä»¶æ¥æºä¿¡æ¯
            # å¦‚æœæ²¡æœ‰ç›´æ¥çš„æ–‡ä»¶åä¿¡æ¯ï¼Œä½¿ç”¨æ—¶é—´æˆ³ä½œä¸ºå¤‡é€‰
            crawl_time_str = comment.get('çˆ¬å–æ—¶é—´', '')
            if crawl_time_str:
                try:
                    # è§£ææ—¶é—´æ ¼å¼ï¼šYYYYå¹´MMæœˆDDæ—¥_HHæ—¶MMåˆ†SSç§’
                    from datetime import datetime
                    dt = datetime.strptime(crawl_time_str, '%Yå¹´%mæœˆ%dæ—¥_%Hæ—¶%Måˆ†%Sç§’')
                    return int(dt.timestamp())
                except:
                    pass
            
            # å¤‡é€‰æ–¹æ¡ˆï¼šä½¿ç”¨è¯„è®ºçš„æ—¶é—´æˆ³
            return comment.get('æ—¶é—´æˆ³', 0)
        
        for comment in comments:
            rpid = comment.get('rpid', '')
            if rpid:
                # å¦‚æœå·²å­˜åœ¨è¯¥rpidï¼Œæ¯”è¾ƒçˆ¬å–æ—¶é—´ï¼Œä¿ç•™çˆ¬å–æ—¶é—´æ›´æ™šçš„
                if rpid in rpid_to_comment:
                    existing_crawl_time = extract_crawl_time_from_filename(rpid_to_comment[rpid])
                    current_crawl_time = extract_crawl_time_from_filename(comment)
                    
                    if current_crawl_time > existing_crawl_time:
                        # å°†æ—§è¯„è®ºæ ‡è®°ä¸ºé‡å¤
                        old_comment = rpid_to_comment[rpid].copy()
                        old_comment['é‡å¤æ¥æº'] = comment_type
                        old_comment['åŸå§‹è¯„è®ºæ¥æº'] = comment_type
                        duplicates.append(old_comment)
                        # æ›´æ–°ä¸ºæ–°è¯„è®ºï¼ˆä¿ç•™åŠ¨æ€å±æ€§ï¼šç‚¹èµæ•°ã€å›å¤æ•°ã€æ—¶é—´æˆ³ç­‰ï¼‰
                        rpid_to_comment[rpid] = comment
                    else:
                        # å½“å‰è¯„è®ºæ˜¯é‡å¤çš„ï¼ˆçˆ¬å–æ—¶é—´æ›´æ—©æˆ–ç›¸ç­‰ï¼‰
                        duplicate_comment = comment.copy()
                        duplicate_comment['é‡å¤æ¥æº'] = comment_type
                        duplicate_comment['åŸå§‹è¯„è®ºæ¥æº'] = comment_type
                        duplicates.append(duplicate_comment)
                else:
                    rpid_to_comment[rpid] = comment
        
        return list(rpid_to_comment.values()), duplicates
    
    # ç¬¬ä¸€æ­¥ï¼šå¯¹çƒ­åº¦å’Œæ—¶é—´çˆ¬å–åˆ†åˆ«å»é‡
    deduped_popularity, pop_duplicates = deduplicate_by_rpid(popularity_comments, "çƒ­åº¦æ’åº")
    deduped_time, time_duplicates = deduplicate_by_rpid(time_comments, "æ—¶é—´æ’åº")
    
    # ç¬¬äºŒæ­¥ï¼šåˆå¹¶ä¸¤ç§çˆ¬å–ç»“æœå¹¶å»é‡
    all_comments = deduped_popularity + deduped_time
    final_comments, merge_duplicates = deduplicate_by_rpid(all_comments, "åˆå¹¶å»é‡")
    
    # åˆå¹¶æ‰€æœ‰é‡å¤è¯„è®º
    all_duplicates = pop_duplicates + time_duplicates + merge_duplicates
    
    if logger:
        logger.info(f"åˆå¹¶å®Œæˆï¼šå”¯ä¸€è¯„è®º {len(final_comments)} æ¡ï¼Œé‡å¤è¯„è®º {len(all_duplicates)} æ¡")
    
    return final_comments, all_duplicates

def crawl_comprehensive_mode_comments(oid, ps=20, delay_ms=1000, test_mode=False, logger=None, output_folder=None):
    """
    ç»¼åˆæ¨¡å¼è¯„è®ºçˆ¬å–ï¼šä¼˜å…ˆæŒ‰çƒ­åº¦çˆ¬å–ï¼Œå¦‚æœä¸æ˜¯å› ä¸ºreplyä¸ºç©ºè€Œç»“æŸï¼Œåˆ™ç»§ç»­æŒ‰æ—¶é—´çˆ¬å–ï¼Œæœ€ååˆå¹¶å»é‡
    
    Args:
        oid (str): è§†é¢‘oid
        ps (int): æ¯é¡µè¯„è®ºæ•°é‡
        delay_ms (int): è¯·æ±‚å»¶æ—¶ï¼ˆæ¯«ç§’ï¼‰
        test_mode (bool): æµ‹è¯•æ¨¡å¼ï¼Œåªçˆ¬å–ä¸€é¡µ
        logger: æ—¥å¿—è®°å½•å™¨
        output_folder (str): è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„
    
    Returns:
        tuple: (çƒ­åº¦çˆ¬å–ç»“æœ, æ—¶é—´çˆ¬å–ç»“æœ, åˆå¹¶å»é‡ç»“æœ, é‡å¤è¯„è®ºåˆ—è¡¨, çƒ­åº¦çˆ¬å–ç»“æŸåŸå› )
    """
    if logger:
        logger.info("å¼€å§‹ç»¼åˆæ¨¡å¼çˆ¬å–")
        logger.info("ç¬¬ä¸€é˜¶æ®µï¼šæŒ‰çƒ­åº¦æ’åºçˆ¬å–")
    
    print("\n=== ç»¼åˆæ¨¡å¼çˆ¬å– ===")
    print("ğŸ“‹ çˆ¬å–ç­–ç•¥ï¼š")
    print("   1ï¸âƒ£ ä¼˜å…ˆæŒ‰çƒ­åº¦æ’åºçˆ¬å–æ‰€æœ‰è¯„è®º")
    print("   2ï¸âƒ£ å¦‚æœçƒ­åº¦çˆ¬å–æœªå› è¯„è®ºè€—å°½è€Œç»“æŸï¼Œåˆ™è¡¥å……æŒ‰æ—¶é—´æ’åºçˆ¬å–")
    print("   3ï¸âƒ£ åˆå¹¶æ•°æ®å¹¶å»é™¤é‡å¤è¯„è®º")
    print("   4ï¸âƒ£ æŒ‰çƒ­åº¦æ•´ç†æœ€ç»ˆç»“æœå¹¶ç”Ÿæˆç»Ÿè®¡")
    print()
    
    # ç¬¬ä¸€é˜¶æ®µï¼šæŒ‰çƒ­åº¦æ’åºçˆ¬å–
    print("ğŸ”¥ ç¬¬ä¸€é˜¶æ®µï¼šæŒ‰çƒ­åº¦æ’åºçˆ¬å–")
    popularity_comments, popularity_end_reason = crawl_all_comments_with_reason(
        oid=oid, 
        mode=1,  # çƒ­åº¦æ’åºï¼ˆæŒ‰ç‚¹èµæ•°æ’åºï¼‰
        ps=ps, 
        delay_ms=delay_ms, 
        test_mode=test_mode,
        logger=logger,
        output_folder=output_folder
    )
    
    if logger:
        logger.info(f"çƒ­åº¦æ’åºçˆ¬å–å®Œæˆï¼Œè·å¾— {len(popularity_comments)} æ¡è¯„è®ºï¼Œç»“æŸåŸå› ï¼š{popularity_end_reason}")
    print(f"âœ… çƒ­åº¦æ’åºçˆ¬å–å®Œæˆï¼Œå…±è·å¾— {len(popularity_comments)} æ¡è¯„è®º")
    print(f"ğŸ“‹ ç»“æŸåŸå› ï¼š{popularity_end_reason}")
    
    # åˆ¤æ–­æ˜¯å¦éœ€è¦è¿›è¡Œæ—¶é—´æ’åºçˆ¬å–
    time_comments = []
    need_time_crawl = popularity_end_reason != "è¯„è®ºå·²å…¨éƒ¨çˆ¬å–å®Œæ¯•"
    
    if need_time_crawl:
        print("\nâ° ç¬¬äºŒé˜¶æ®µï¼šè¡¥å……æŒ‰æ—¶é—´æ’åºçˆ¬å–")
        print(f"ğŸ’¡ ç”±äºçƒ­åº¦çˆ¬å–ç»“æŸåŸå› ä¸º'{popularity_end_reason}'ï¼Œéœ€è¦è¡¥å……æ—¶é—´çˆ¬å–ä»¥è·å–å®Œæ•´æ•°æ®")
        time_comments, time_end_reason = crawl_all_comments_with_reason(
            oid=oid, 
            mode=0,  # æ—¶é—´æ’åºï¼ˆæŒ‰æ—¶é—´æ’åºï¼‰
            ps=ps, 
            delay_ms=delay_ms, 
            test_mode=test_mode,
            logger=logger,
            output_folder=output_folder
        )
        
        if logger:
            logger.info(f"æ—¶é—´æ’åºçˆ¬å–å®Œæˆï¼Œè·å¾— {len(time_comments)} æ¡è¯„è®ºï¼Œç»“æŸåŸå› ï¼š{time_end_reason}")
        print(f"âœ… æ—¶é—´æ’åºçˆ¬å–å®Œæˆï¼Œå…±è·å¾— {len(time_comments)} æ¡è¯„è®º")
    else:
        print("\nâ­ï¸ è·³è¿‡æ—¶é—´æ’åºçˆ¬å–")
        print("ğŸ’¡ çƒ­åº¦çˆ¬å–å·²è·å–æ‰€æœ‰è¯„è®ºï¼Œæ— éœ€è¡¥å……æ—¶é—´çˆ¬å–")
        if logger:
            logger.info("è·³è¿‡æ—¶é—´æ’åºçˆ¬å–ï¼Œçƒ­åº¦çˆ¬å–å·²å®Œæ•´")
    
    # ç¬¬ä¸‰é˜¶æ®µï¼šåˆå¹¶å’Œå»é‡
    if time_comments:
        print("\nğŸ”„ ç¬¬ä¸‰é˜¶æ®µï¼šåˆå¹¶æ•°æ®å¹¶å»é‡")
        merged_comments, duplicate_comments = merge_and_deduplicate_comments(
            popularity_comments, time_comments, logger
        )
        
        if logger:
            logger.info(f"åˆå¹¶å»é‡å®Œæˆï¼Œæœ€ç»ˆè·å¾— {len(merged_comments)} æ¡å”¯ä¸€è¯„è®ºï¼Œå‘ç° {len(duplicate_comments)} æ¡é‡å¤è¯„è®º")
        
        print(f"âœ… åˆå¹¶å»é‡å®Œæˆï¼š")
        print(f"   ğŸ“Š æœ€ç»ˆå”¯ä¸€è¯„è®ºï¼š{len(merged_comments)} æ¡")
        print(f"   ğŸ”„ é‡å¤è¯„è®ºï¼š{len(duplicate_comments)} æ¡")
        print(f"   ğŸ“ˆ å»é‡ç‡ï¼š{len(duplicate_comments)/(len(popularity_comments)+len(time_comments))*100:.1f}%")
        print(f"   âœ… æ•°æ®éªŒè¯ï¼š{len(merged_comments)} + {len(duplicate_comments)} = {len(merged_comments) + len(duplicate_comments)} (æ€»çˆ¬å–è¯„è®ºæ•°)")
        print(f"   ğŸ’¡ è¯´æ˜ï¼šé‡å¤è¯„è®ºæ•°é«˜æ˜¯å› ä¸ºä¸¤ç§æ’åºæ¨¡å¼è¿”å›äº†å¤§é‡ç›¸åŒè¯„è®ºï¼Œè¿™æ˜¯Bç«™APIçš„ç‰¹æ€§")
    else:
        # åªæœ‰çƒ­åº¦è¯„è®ºï¼Œæ— éœ€å»é‡
        merged_comments = popularity_comments
        duplicate_comments = []
        print("\nâœ… ä»…ä½¿ç”¨çƒ­åº¦æ’åºç»“æœï¼Œæ— éœ€å»é‡")
        if logger:
            logger.info(f"ä»…ä½¿ç”¨çƒ­åº¦æ’åºç»“æœï¼Œå…± {len(merged_comments)} æ¡è¯„è®º")
    
    return popularity_comments, time_comments, merged_comments, duplicate_comments, popularity_end_reason

def crawl_test_mode_comments(oid, sort_mode, ps=20, delay_ms=1000, max_pages=5, logger=None, output_folder=None):
    """
    æµ‹è¯•æ¨¡å¼çˆ¬å–è¯„è®º
    
    Args:
        oid (str): è§†é¢‘oid
        sort_mode (int): æ’åºæ¨¡å¼ (0=æ—¶é—´æ’åº, 1=çƒ­åº¦æ’åº)
        ps (int): æ¯é¡µè¯„è®ºæ•°é‡
        delay_ms (int): è¯·æ±‚å»¶æ—¶ï¼ˆæ¯«ç§’ï¼‰
        max_pages (int): æœ€å¤§çˆ¬å–é¡µæ•°
        logger: æ—¥å¿—è®°å½•å™¨
        output_folder (str): è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„
    
    Returns:
        tuple: (è¯„è®ºåˆ—è¡¨, ç»“æŸåŸå› )
    """
    if logger:
        logger.info(f"å¼€å§‹æµ‹è¯•æ¨¡å¼çˆ¬å–ï¼Œoid={oid}, sort_mode={sort_mode}, max_pages={max_pages}")
    
    sort_name = "çƒ­åº¦æ’åº" if sort_mode == 1 else "æ—¶é—´æ’åº"
    print(f"ğŸ§ª æµ‹è¯•æ¨¡å¼çˆ¬å–è®¾ç½®ï¼š")
    print(f"   ğŸ“Š æ’åºæ–¹å¼ï¼š{sort_name}")
    print(f"   ğŸ“„ æœ€å¤§é¡µæ•°ï¼š{max_pages}")
    print(f"   ğŸ“ æ¯é¡µæ•°é‡ï¼š{ps}")
    print(f"   â±ï¸  è¯·æ±‚å»¶æ—¶ï¼š{delay_ms}ms")
    
    comments = []
    current_page = 1
    next_offset = ''
    
    while current_page <= max_pages:
        print(f"\nğŸ“„ æ­£åœ¨çˆ¬å–ç¬¬ {current_page}/{max_pages} é¡µ...")
        
        # è·å–è¯„è®ºæ•°æ®ï¼ˆé¡µé¢æ—¥å¿—è®°å½•å™¨å°†åœ¨get_bilibili_commentså‡½æ•°å†…éƒ¨åˆ›å»ºï¼‰
        comments_data = get_bilibili_comments(oid, sort_mode, ps, next_offset, current_page == 1, current_page, logger, output_folder)
        
        if comments_data:
            data = comments_data.get('data', {})
            page_comments = data.get('replies', [])
            cursor = data.get('cursor', {})
            next_offset = cursor.get('next', '')
            has_more = cursor.get('is_end', False) == False
        else:
            page_comments = []
            next_offset = ''
            has_more = False
        
        if not page_comments:
            end_reason = "å½“å‰é¡µæ— è¯„è®ºæ•°æ®"
            print(f"âš ï¸  ç¬¬ {current_page} é¡µæ— è¯„è®ºæ•°æ®ï¼Œåœæ­¢çˆ¬å–")
            if logger:
                logger.info(f"ç¬¬ {current_page} é¡µæ— è¯„è®ºæ•°æ®ï¼Œçˆ¬å–ç»“æŸ")
            break
        
        # å¤„ç†è¯„è®ºæ•°æ®ï¼Œè½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼ï¼ˆä¸éœ€è¦é¡µé¢æ—¥å¿—è®°å½•å™¨ï¼Œå› ä¸ºå·²åœ¨get_bilibili_commentsä¸­è®°å½•ï¼‰
        processed_comments = process_comments_page(page_comments, start_index=len(comments)+1, logger=logger, oid=oid)
        comments.extend(processed_comments)
        print(f"âœ… ç¬¬ {current_page} é¡µå®Œæˆï¼Œè·å¾— {len(page_comments)} æ¡è¯„è®º")
        
        if logger:
            logger.info(f"ç¬¬ {current_page} é¡µçˆ¬å–å®Œæˆï¼Œè·å¾— {len(page_comments)} æ¡è¯„è®º")
        
        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ›´å¤šé¡µé¢
        if not has_more:
            end_reason = "è¯„è®ºå·²å…¨éƒ¨çˆ¬å–å®Œæ¯•"
            print(f"âœ… æ‰€æœ‰è¯„è®ºå·²çˆ¬å–å®Œæ¯•ï¼ˆå…± {current_page} é¡µï¼‰")
            if logger:
                logger.info(f"æ‰€æœ‰è¯„è®ºå·²çˆ¬å–å®Œæ¯•ï¼Œå…±çˆ¬å– {current_page} é¡µ")
            break
        
        current_page += 1
        
        # è¯·æ±‚å»¶æ—¶
        if delay_ms > 0:
            time.sleep(delay_ms / 1000.0)
    
    # å¦‚æœè¾¾åˆ°æœ€å¤§é¡µæ•°é™åˆ¶
    if current_page > max_pages:
        end_reason = "å·²è¾¾åˆ°æŒ‡å®šé¡µæ•°é™åˆ¶"
        if logger:
            logger.info(f"å·²è¾¾åˆ°æŒ‡å®šé¡µæ•°é™åˆ¶ {max_pages}ï¼Œçˆ¬å–ç»“æŸ")
    
    print(f"\nğŸ¯ æµ‹è¯•æ¨¡å¼çˆ¬å–å®Œæˆï¼š")
    print(f"   ğŸ“Š æ’åºæ–¹å¼ï¼š{sort_name}")
    print(f"   ğŸ“„ å®é™…çˆ¬å–ï¼š{current_page-1} é¡µ")
    print(f"   ğŸ’¬ æ€»è¯„è®ºæ•°ï¼š{len(comments)} æ¡")
    print(f"   â¹ï¸  ç»“æŸåŸå› ï¼š{end_reason}")
    
    if logger:
        logger.info(f"æµ‹è¯•æ¨¡å¼çˆ¬å–å®Œæˆï¼Œå…±è·å¾— {len(comments)} æ¡è¯„è®ºï¼Œç»“æŸåŸå› ï¼š{end_reason}")
    
    return comments, end_reason

def process_comprehensive_mode_data(oid, popularity_comments, time_comments, merged_comments, duplicate_comments, output_folder, logger=None, video_title=None):
    """
    å¤„ç†ç»¼åˆæ¨¡å¼æ•°æ®ï¼Œç”Ÿæˆ4ä¸ªæ–‡æ¡£
    
    Args:
        oid (str): è§†é¢‘oid
        popularity_comments (list): çƒ­åº¦æ’åºè¯„è®º
        time_comments (list): æ—¶é—´æ’åºè¯„è®º
        merged_comments (list): åˆå¹¶å»é‡è¯„è®º
        duplicate_comments (list): é‡å¤è¯„è®º
        output_folder (str): è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„
        logger: æ—¥å¿—è®°å½•å™¨
        video_title (str): è§†é¢‘æ ‡é¢˜
    
    Returns:
        tuple: (åŸå§‹æ•°æ®æ–‡ä»¶å¤¹è·¯å¾„, 4ä¸ªæ–‡æ¡£è·¯å¾„åˆ—è¡¨)
    """
    # åˆ›å»ºåŸå§‹æ•°æ®æ–‡ä»¶å¤¹
    raw_data_folder = os.path.join(output_folder, 'åŸå§‹æ•°æ®')
    if not os.path.exists(raw_data_folder):
        os.makedirs(raw_data_folder)
    
    if logger:
        logger.info(f"åˆ›å»ºåŸå§‹æ•°æ®æ–‡ä»¶å¤¹: {raw_data_folder}")
    
    print(f"\nåˆ›å»ºåŸå§‹æ•°æ®æ–‡ä»¶å¤¹: {raw_data_folder}")
    
    # ç”Ÿæˆ4ä¸ªæ–‡æ¡£
    doc_paths = []
    
    # æ–‡æ¡£1ï¼šçƒ­åº¦çˆ¬å–ç»“æœ
    doc1_filename = generate_safe_filename(video_title, oid, "çƒ­åº¦æ’åºçˆ¬å–ç»“æœ", "original")
    doc1_path = os.path.join(raw_data_folder, f'{doc1_filename}.csv')
    save_comments_to_csv(popularity_comments, doc1_path, 'çƒ­åº¦æ’åºçˆ¬å–ç»“æœ')
    doc_paths.append(doc1_path)
    
    # æ–‡æ¡£2ï¼šæ—¶é—´çˆ¬å–ç»“æœ
    doc2_filename = generate_safe_filename(video_title, oid, "æ—¶é—´æ’åºçˆ¬å–ç»“æœ", "original")
    doc2_path = os.path.join(raw_data_folder, f'{doc2_filename}.csv')
    save_comments_to_csv(time_comments, doc2_path, 'æ—¶é—´æ’åºçˆ¬å–ç»“æœ')
    doc_paths.append(doc2_path)
    
    # æ–‡æ¡£3ï¼šåˆå¹¶å»é‡ç»“æœ
    doc3_filename = generate_safe_filename(video_title, oid, "åˆå¹¶å»é‡ç»“æœ", "final")
    doc3_path = os.path.join(raw_data_folder, f'{doc3_filename}.csv')
    save_comments_to_csv(merged_comments, doc3_path, 'åˆå¹¶å»é‡ç»“æœ')
    doc_paths.append(doc3_path)
    
    # æ–‡æ¡£4ï¼šé‡å¤è¯„è®ºåˆ—è¡¨
    doc4_filename = generate_safe_filename(video_title, oid, "é‡å¤è¯„è®ºåˆ—è¡¨", "final")
    doc4_path = os.path.join(raw_data_folder, f'{doc4_filename}.csv')
    save_comments_to_csv(duplicate_comments, doc4_path, 'é‡å¤è¯„è®ºåˆ—è¡¨')
    doc_paths.append(doc4_path)
    
    if logger:
        logger.info(f"ç”Ÿæˆ4ä¸ªåŸå§‹æ•°æ®æ–‡æ¡£å®Œæˆ")
        for i, path in enumerate(doc_paths, 1):
            logger.info(f"æ–‡æ¡£{i}: {path}")
    
    print("\nç”ŸæˆåŸå§‹æ•°æ®æ–‡æ¡£ï¼š")
    print(f"  1. çƒ­åº¦æ’åºçˆ¬å–ç»“æœ: {len(popularity_comments)} æ¡è¯„è®º")
    print(f"  2. æ—¶é—´æ’åºçˆ¬å–ç»“æœ: {len(time_comments)} æ¡è¯„è®º")
    print(f"  3. åˆå¹¶å»é‡ç»“æœ: {len(merged_comments)} æ¡è¯„è®º")
    print(f"  4. é‡å¤è¯„è®ºåˆ—è¡¨: {len(duplicate_comments)} æ¡è¯„è®º")
    
    return raw_data_folder, doc_paths

def save_comments_to_csv(comments, file_path, data_type):
    """
    ä¿å­˜è¯„è®ºæ•°æ®åˆ°CSVæ–‡ä»¶
    
    Args:
        comments (list): è¯„è®ºæ•°æ®åˆ—è¡¨
        file_path (str): æ–‡ä»¶è·¯å¾„
        data_type (str): æ•°æ®ç±»å‹æ ‡è¯†
    """
    if not comments:
        # åˆ›å»ºç©ºæ–‡ä»¶
        with open(file_path, 'w', newline='', encoding='utf-8-sig') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow(['æ•°æ®ç±»å‹', 'ä¸»æ¥¼åºå·', 'æ¥¼ä¸­æ¥¼åºå·', 'ç”¨æˆ·åç§°', 'è¯„è®ºå†…å®¹', 'å›å¤å¯¹è±¡', 'ç‚¹èµæ•°', 'å›å¤æ•°', 'rpid', 'parent', 'å‘å¸ƒæ—¶é—´', 'æ—¶é—´æˆ³', 'ç”¨æˆ·ç­‰çº§', 'IPåœ°åŒº', 'æ€§åˆ«', 'è¯„è®ºç±»å‹', 'çˆ¬å–æ—¶é—´'])
        return
    
    # ä¸ºè¯„è®ºæ·»åŠ æ•°æ®ç±»å‹æ ‡è¯†
    comments_with_type = []
    for comment in comments:
        comment_with_type = comment.copy()
        comment_with_type['æ•°æ®ç±»å‹'] = data_type
        comments_with_type.append(comment_with_type)
    
    # ä¿å­˜åˆ°CSV
    fieldnames = ['æ•°æ®ç±»å‹', 'ä¸»æ¥¼åºå·', 'æ¥¼ä¸­æ¥¼åºå·', 'ç”¨æˆ·åç§°', 'è¯„è®ºå†…å®¹', 'å›å¤å¯¹è±¡', 'ç‚¹èµæ•°', 'å›å¤æ•°', 'rpid', 'parent', 'å‘å¸ƒæ—¶é—´', 'æ—¶é—´æˆ³', 'ç”¨æˆ·ç­‰çº§', 'IPåœ°åŒº', 'æ€§åˆ«', 'è¯„è®ºç±»å‹', 'çˆ¬å–æ—¶é—´']
    
    # å¦‚æœæ˜¯é‡å¤è¯„è®ºï¼Œæ·»åŠ é¢å¤–å­—æ®µ
    if comments_with_type and 'é‡å¤æ¥æº' in comments_with_type[0]:
        fieldnames.extend(['é‡å¤æ¥æº', 'åŸå§‹è¯„è®ºæ¥æº'])
    
    with open(file_path, 'w', newline='', encoding='utf-8-sig') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(comments_with_type)

def crawl_all_comments_with_reason(oid, mode=1, ps=20, delay_ms=1000, test_mode=False, logger=None, output_folder=None):
    """
    çˆ¬å–æ‰€æœ‰è¯„è®ºå¹¶è¿”å›ç»“æŸåŸå› 
    
    Args:
        oid: è§†é¢‘çš„oidï¼ˆç¨¿ä»¶avidï¼‰
        mode (int): æ’åºæ¨¡å¼ï¼Œæ ¹æ®Bç«™APIæ–‡æ¡£ï¼š0=æŒ‰æ—¶é—´æ’åºï¼Œ1=æŒ‰ç‚¹èµæ•°æ’åºï¼ˆçƒ­åº¦ï¼‰ï¼Œ2=æŒ‰å›å¤æ•°æ’åº
        ps (int): æ¯é¡µè¯„è®ºæ•°é‡
        delay_ms (int): è¯·æ±‚å»¶æ—¶ï¼ˆæ¯«ç§’ï¼‰
        test_mode (bool): æ˜¯å¦ä¸ºæµ‹è¯•æ¨¡å¼ï¼Œæµ‹è¯•æ¨¡å¼åªçˆ¬å–ä¸€é¡µ
    
    Returns:
        tuple: (è¯„è®ºåˆ—è¡¨, ç»“æŸåŸå› )
    """
    all_comments = []
    next_offset = ''
    page_count = 1
    total_comments = 0
    end_reason = "æœªçŸ¥åŸå› "
    
    # å®šä¹‰æ’åºæ¨¡å¼åç§°æ˜ å°„
    mode_names = {0: 'æ—¶é—´æ’åº', 1: 'çƒ­åº¦æ’åº', 2: 'æŒ‰å›å¤æ•°æ’åº'}
    mode_name = mode_names.get(mode, f'æœªçŸ¥æ¨¡å¼({mode})')
    
    print(f"\nğŸš€ å¼€å§‹çˆ¬å–è¯„è®º (oid: {oid})")
    print(f"ğŸ“Š æ’åºæ¨¡å¼: {mode_name}")
    print(f"ğŸ“„ æ¯é¡µæ•°é‡: {ps}æ¡")
    print(f"â±ï¸  å»¶æ—¶è®¾ç½®: {delay_ms}ms")
    print(f"ğŸ§ª æµ‹è¯•æ¨¡å¼: {'æ˜¯' if test_mode else 'å¦'}")
    
    while True:
        print(f"\nğŸ“„ æ­£åœ¨çˆ¬å–ç¬¬ {page_count} é¡µ...")
        
        # è·å–è¯„è®ºæ•°æ®
        is_first_page = (page_count == 1)
        result = get_bilibili_comments(oid, mode, ps, next_offset, is_first_page, page_count, logger, output_folder)
        
        if not result:
            end_reason = "APIè¯·æ±‚å¤±è´¥"
            print(f"âŒ {end_reason}ï¼Œåœæ­¢çˆ¬å–")
            break
        
        # æ£€æŸ¥å“åº”çŠ¶æ€
        if result.get('code') != 0:
            end_reason = f"APIè¿”å›é”™è¯¯: {result.get('message', 'æœªçŸ¥é”™è¯¯')}"
            print(f"âŒ {end_reason}")
            break
        
        # è·å–è¯„è®ºæ•°æ®
        data = result.get('data', {})
        replies = data.get('replies', [])
        
        if not replies:
            end_reason = "è¯„è®ºå·²å…¨éƒ¨çˆ¬å–å®Œæ¯•"
            print(f"â„¹ï¸  {end_reason}")
            break
        
        print(f"âœ… æœ¬é¡µè·å–åˆ° {len(replies)} æ¡è¯„è®º")
        
        # å¤„ç†è¯„è®ºæ•°æ®
        start_index = total_comments + 1
        page_comments = process_comments_page(replies, start_index, oid=oid)
        all_comments.extend(page_comments)
        total_comments += len(page_comments)
        
        print(f"ğŸ“ˆ ç´¯è®¡å¤„ç† {total_comments} æ¡è¯„è®º")
        
        # æ£€æŸ¥åˆ†é¡µä¿¡æ¯
        cursor = data.get('cursor', {})
        next_offset = cursor.get('next', '')
        
        # æµ‹è¯•æ¨¡å¼åªçˆ¬å–ä¸€é¡µ
        if test_mode:
            end_reason = "æµ‹è¯•æ¨¡å¼é™åˆ¶"
            print(f"ğŸ§ª {end_reason}ï¼Œåœæ­¢çˆ¬å–")
            break
        
        page_count += 1
        
        # æ·»åŠ å»¶æ—¶
        if delay_ms > 0:
            print(f"â³ ç­‰å¾… {delay_ms}ms...")
            time.sleep(delay_ms / 1000)
    
    print(f"\nğŸ‰ è¯„è®ºçˆ¬å–å®Œæˆï¼")
    print(f"ğŸ“Š æ€»å…±çˆ¬å–äº† {page_count} é¡µï¼Œ{total_comments} æ¡è¯„è®º")
    print(f"ğŸ ç»“æŸåŸå› ï¼š{end_reason}")
    
    return all_comments, end_reason

def crawl_all_comments(oid, mode=3, ps=20, delay_ms=1000, test_mode=False, video_title=None, video_info=None):
    """
    çˆ¬å–æ‰€æœ‰è¯„è®ºï¼ˆå…¼å®¹æ€§å‡½æ•°ï¼‰
    
    Args:
        oid: è§†é¢‘çš„oidï¼ˆç¨¿ä»¶avidï¼‰
        mode (int): æ’åºæ¨¡å¼ï¼Œ3ä¸ºçƒ­åº¦æ’åºï¼Œ2ä¸ºæ—¶é—´æ’åº
        ps (int): æ¯é¡µè¯„è®ºæ•°é‡
        delay_ms (int): è¯·æ±‚å»¶æ—¶ï¼ˆæ¯«ç§’ï¼‰
        test_mode (bool): æ˜¯å¦ä¸ºæµ‹è¯•æ¨¡å¼ï¼Œæµ‹è¯•æ¨¡å¼åªçˆ¬å–ä¸€é¡µ
    
    Returns:
        bool: æ˜¯å¦æˆåŠŸ
    """
    # ç¡®å®šæ¨¡å¼ç±»å‹
    if test_mode:
        mode_type = "test_time" if mode == 2 else "test_popularity"
    else:
        mode_type = None
    
    # åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹
    output_folder = create_output_folder(oid, video_title, mode_type)
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶å¤¹å·²åˆ›å»º: {output_folder}")
    
    # è®¾ç½®æ—¥å¿—
    logger, main_log_file = setup_logging(oid, output_folder)
    print(f"ğŸ“„ ä¸»æ—¥å¿—æ–‡ä»¶: {os.path.basename(main_log_file)}")
    
    all_comments = []
    next_offset = ''
    page_count = 1
    total_comments = 0
    
    print(f"\nğŸš€ å¼€å§‹çˆ¬å–è¯„è®º (oid: {oid})")
    print(f"ğŸ“Š æ’åºæ¨¡å¼: {'çƒ­åº¦æ’åº' if mode == 3 else 'æ—¶é—´æ’åº'}")
    print(f"ğŸ“„ æ¯é¡µæ•°é‡: {ps}æ¡")
    print(f"â±ï¸  å»¶æ—¶è®¾ç½®: {delay_ms}ms")
    print(f"ğŸ§ª æµ‹è¯•æ¨¡å¼: {'æ˜¯' if test_mode else 'å¦'}")
    print(f"ğŸ“„ åœæ­¢æ¡ä»¶: å½“è¿”å›æ•°æ®ä¸ºç©ºæ—¶è‡ªåŠ¨åœæ­¢")
    
    logger.info(f"å¼€å§‹çˆ¬å–è¯„è®ºï¼Œoid: {oid}, æ’åºæ¨¡å¼: {mode}, å»¶æ—¶: {delay_ms}ms, æµ‹è¯•æ¨¡å¼: {test_mode}")
    
    while True:
        print(f"\nğŸ“„ æ­£åœ¨çˆ¬å–ç¬¬ {page_count} é¡µ...")
        logger.info(f"å¼€å§‹çˆ¬å–ç¬¬ {page_count} é¡µ")
        
        # è·å–è¯„è®ºæ•°æ®
        is_first_page = (page_count == 1)
        result = get_bilibili_comments(oid, mode, ps, next_offset, is_first_page, page_count, logger, output_folder)
        
        if not result:
            error_msg = "è·å–è¯„è®ºå¤±è´¥ï¼Œåœæ­¢çˆ¬å–"
            print(f"âŒ {error_msg}")
            logger.error(error_msg)
            break
        
        # æ£€æŸ¥å“åº”çŠ¶æ€
        if result.get('code') != 0:
            error_msg = f"APIè¿”å›é”™è¯¯: {result.get('message', 'æœªçŸ¥é”™è¯¯')}"
            print(f"âŒ {error_msg}")
            logger.error(error_msg)
            break
        
        # è·å–è¯„è®ºæ•°æ®
        data = result.get('data', {})
        replies = data.get('replies', [])
        
        if not replies:
            info_msg = "æ²¡æœ‰æ›´å¤šè¯„è®ºäº†"
            print(f"â„¹ï¸  {info_msg}")
            logger.info(info_msg)
            break
        
        print(f"âœ… æœ¬é¡µè·å–åˆ° {len(replies)} æ¡è¯„è®º")
        logger.info(f"ç¬¬ {page_count} é¡µè·å–åˆ° {len(replies)} æ¡è¯„è®º")
        
        # å¤„ç†è¯„è®ºæ•°æ®
        start_index = total_comments + 1
        page_comments = process_comments_page(replies, start_index, logger, oid=oid)
        all_comments.extend(page_comments)
        total_comments += len(page_comments)
        
        print(f"ğŸ“ˆ ç´¯è®¡å¤„ç† {total_comments} æ¡è¯„è®º")
        logger.info(f"ç¬¬ {page_count} é¡µå¤„ç†å®Œæˆï¼Œç´¯è®¡ {total_comments} æ¡è¯„è®º")
        
        # æ£€æŸ¥åˆ†é¡µä¿¡æ¯
        cursor = data.get('cursor', {})
        logger.debug(f"åˆ†é¡µä¿¡æ¯: {cursor}")
        
        # è·å–ä¸‹ä¸€é¡µçš„åç§»é‡
        next_offset = cursor.get('next', '')
        is_end = cursor.get('is_end', False)
        has_next = cursor.get('has_next', False)
        
        logger.debug(f"next_offset: {next_offset}, is_end: {is_end}, has_next: {has_next}")
        
        # åˆ¤æ–­æ˜¯å¦ç»§ç»­ - åªæœ‰å½“repliesä¸ºç©ºæ—¶æ‰åœæ­¢
        if not replies:
            continue_reason = "è¿”å›æ•°æ®ä¸ºç©ºï¼Œåœæ­¢çˆ¬å–"
            print(f"ğŸ {continue_reason}")
            logger.info(f"åˆ†é¡µåˆ¤æ–­: {continue_reason}")
            break
        
        # å¦‚æœæœ‰æ•°æ®ï¼Œç»§ç»­çˆ¬å–ä¸‹ä¸€é¡µ
        logger.info(f"åˆ†é¡µåˆ¤æ–­: æ£€æµ‹åˆ°è¯„è®ºæ•°æ®ï¼Œç»§ç»­çˆ¬å–ä¸‹ä¸€é¡µ")
        
        # æµ‹è¯•æ¨¡å¼åªçˆ¬å–ä¸€é¡µ
        if test_mode:
            print(f"ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼Œåªçˆ¬å–ä¸€é¡µ")
            logger.info("æµ‹è¯•æ¨¡å¼ï¼Œåœæ­¢çˆ¬å–")
            break
        
        page_count += 1
        
        # æ·»åŠ å»¶æ—¶
        if delay_ms > 0:
            print(f"â³ ç­‰å¾… {delay_ms}ms...")
            logger.debug(f"å»¶æ—¶ {delay_ms}ms")
            time.sleep(delay_ms / 1000)
    
    print(f"\nğŸ‰ è¯„è®ºçˆ¬å–å®Œæˆï¼")
    print(f"ğŸ“Š æ€»å…±çˆ¬å–äº† {page_count} é¡µï¼Œ{total_comments} æ¡è¯„è®º")
    logger.info(f"çˆ¬å–å®Œæˆï¼Œæ€»å…± {page_count} é¡µï¼Œ{total_comments} æ¡è¯„è®º")
    
    # æ•´ç†å’Œä¿å­˜æ•°æ®
    if all_comments:
        try:
            print(f"\nğŸ“Š å¼€å§‹æ•´ç†å’Œç»Ÿè®¡æ•°æ®...")
            logger.info("å¼€å§‹æ•°æ®æ•´ç†å’Œç»Ÿè®¡")
            
            # è°ƒç”¨æ•°æ®æ•´ç†å’Œç»Ÿè®¡å‡½æ•°ï¼ˆæŒ‰çƒ­åº¦æ’åºï¼‰
            _, processed_file, stats_file = process_and_organize_data(
                all_comments, output_folder, oid, logger, video_title, sort_by_popularity=True, video_info=video_info
            )
            
            # å¦‚æœæ˜¯æ—¶é—´æ’åºæ¨¡å¼ï¼Œç”ŸæˆæŒ‰æ—¶é—´ç»Ÿè®¡çš„æ–‡ä»¶
            time_stats_files = []
            if mode == 2:  # æ—¶é—´æ’åºæ¨¡å¼
                print(f"\nâ° æ£€æµ‹åˆ°æ—¶é—´æ’åºæ¨¡å¼ï¼Œå¼€å§‹ç”Ÿæˆæ—¶é—´ç»Ÿè®¡æ–‡ä»¶...")
                logger.info("å¼€å§‹ç”ŸæˆæŒ‰æ—¶é—´ç»Ÿè®¡çš„æ–‡ä»¶")
                bv_id = aid_to_bvid(int(oid))
                time_stats_files = generate_restructured_time_statistics(all_comments, output_folder, bv_id, logger, video_title, video_info)
                
                if time_stats_files:
                    print(f"âœ… æ—¶é—´ç»Ÿè®¡å®Œæˆï¼Œç”Ÿæˆäº† {len(time_stats_files)} ä¸ªç»Ÿè®¡æ–‡ä»¶")
                    for file_path in time_stats_files:
                        print(f"ğŸ“„ æ—¶é—´ç»Ÿè®¡æ–‡ä»¶: {os.path.basename(file_path)}")
                    logger.info(f"æ—¶é—´ç»Ÿè®¡æ–‡ä»¶ç”Ÿæˆå®Œæˆï¼Œå…± {len(time_stats_files)} ä¸ªæ–‡ä»¶")
                else:
                    print(f"âš ï¸  æ—¶é—´ç»Ÿè®¡æ–‡ä»¶ç”Ÿæˆå¤±è´¥")
                    logger.warning("æ—¶é—´ç»Ÿè®¡æ–‡ä»¶ç”Ÿæˆå¤±è´¥")
            
            print(f"\nâœ… æ•°æ®å¤„ç†å®Œæˆï¼")
            print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶å¤¹: {output_folder}")
            if processed_file:
                print(f"ğŸ“„ æ•´ç†æ•°æ®ï¼ˆçƒ­åº¦æ’åºï¼‰: {os.path.basename(processed_file)}")
            print(f"ğŸ“„ ç»Ÿè®¡æŠ¥å‘Š: {os.path.basename(stats_file)}")
            
            # ç”Ÿæˆæ–‡ä»¶å¤¹ç»“æ„æ–‡æ¡£
            print("\nğŸ“‹ ç”Ÿæˆæ–‡ä»¶å¤¹ç»“æ„æ–‡æ¡£...")
            # ç”ŸæˆBVå·
            try:
                bv_id = aid_to_bvid(int(oid))
            except:
                bv_id = None
            structure_md_path = generate_folder_structure_md(output_folder, oid, video_title, logger, bv_id)
            if structure_md_path:
                print(f"ğŸ“„ æ–‡ä»¶å¤¹ç»“æ„æ–‡æ¡£: {os.path.basename(structure_md_path)}")
            
            logger.info("æ•°æ®å¤„ç†å’Œç»Ÿè®¡å®Œæˆ")
            return True
        except Exception as e:
            error_msg = f"æ•°æ®å¤„ç†å¤±è´¥: {e}"
            print(f"âŒ {error_msg}")
            logger.error(error_msg)
            return False
    else:
        error_msg = "æ²¡æœ‰è·å–åˆ°ä»»ä½•è¯„è®ºæ•°æ®"
        print(f"âŒ {error_msg}")
        logger.error(error_msg)
        return False

def crawl_iteration_mode_comments(oid, ps, delay_ms, iteration_config, logger, output_folder, video_title=None, video_info=None):
    """
    è¿­ä»£æ¨¡å¼çˆ¬å–è¯„è®º
    
    Args:
        oid: è§†é¢‘oid
        ps: æ¯é¡µè¯„è®ºæ•°
        delay_ms: è¯·æ±‚å»¶æ—¶
        iteration_config: è¿­ä»£é…ç½®
        logger: æ—¥å¿—è®°å½•å™¨
        output_folder: è¾“å‡ºæ–‡ä»¶å¤¹
    
    Returns:
        bool: æ˜¯å¦æˆåŠŸ
    """
    try:
        iteration_type = iteration_config['type']
        
        if iteration_type == 'time':
            # æ—¶é—´è¿­ä»£æ¨¡å¼
            iteration_hours = iteration_config['hours']
            print(f"ğŸ• æ—¶é—´è¿­ä»£æ¨¡å¼: {iteration_hours} å°æ—¶")
            logger.info(f"å¼€å§‹æ—¶é—´è¿­ä»£æ¨¡å¼ï¼Œè¿­ä»£æ—¶é—´: {iteration_hours} å°æ—¶")
            
            return crawl_time_iteration(oid, ps, delay_ms, iteration_hours, logger, output_folder, video_title, video_info)
            
        elif iteration_type == 'duplicate_rate':
            # é‡å¤ç‡è¿­ä»£æ¨¡å¼
            popularity_threshold = iteration_config['hot_rate_threshold']
            time_threshold = iteration_config['time_rate_threshold']
            print(f"ğŸ“Š é‡å¤ç‡è¿­ä»£æ¨¡å¼: çƒ­åº¦é˜ˆå€¼={popularity_threshold}%, æ—¶é—´é˜ˆå€¼={time_threshold}%")
            logger.info(f"å¼€å§‹é‡å¤ç‡è¿­ä»£æ¨¡å¼ï¼Œçƒ­åº¦é˜ˆå€¼: {popularity_threshold}%, æ—¶é—´é˜ˆå€¼: {time_threshold}%")
            
            return crawl_duplicate_rate_iteration(oid, ps, delay_ms, popularity_threshold, time_threshold, logger, output_folder, video_title, video_info)
            
        else:
            logger.error(f"æœªçŸ¥çš„è¿­ä»£ç±»å‹: {iteration_type}")
            return False
            
    except Exception as e:
        logger.error(f"è¿­ä»£æ¨¡å¼çˆ¬å–å¤±è´¥: {e}")
        return False

def crawl_time_iteration(oid, ps, delay_ms, iteration_hours, logger, output_folder, video_title=None, video_info=None):
    """
    æ—¶é—´è¿­ä»£æ¨¡å¼ï¼šçƒ­åº¦-æ—¶é—´äº¤æ›¿çˆ¬å–ç›´åˆ°è¾¾åˆ°æ—¶é—´é™åˆ¶
    
    Args:
        oid: è§†é¢‘oid
        ps: æ¯é¡µè¯„è®ºæ•°
        delay_ms: è¯·æ±‚å»¶æ—¶
        iteration_hours: è¿­ä»£æ—¶é—´ï¼ˆå°æ—¶ï¼‰
        logger: æ—¥å¿—è®°å½•å™¨
        output_folder: è¾“å‡ºæ–‡ä»¶å¤¹
    
    Returns:
        bool: æ˜¯å¦æˆåŠŸ
    """
    import time
    from datetime import datetime, timedelta
    
    start_time = datetime.now()
    end_time = start_time + timedelta(hours=iteration_hours)
    
    # åˆ›å»ºè¿­ä»£æ•°æ®å­˜å‚¨æ–‡ä»¶å¤¹
    iteration_folder = os.path.join(output_folder, 'åŸå§‹æ•°æ®')
    popularity_folder = os.path.join(iteration_folder, 'çƒ­åº¦çˆ¬å–åŸå§‹æ•°æ®')
    time_folder = os.path.join(iteration_folder, 'æ—¶é—´çˆ¬å–åŸå§‹æ•°æ®')
    
    for folder in [iteration_folder, popularity_folder, time_folder]:
        if not os.path.exists(folder):
            os.makedirs(folder)
    
    if logger:
        logger.info(f"åˆ›å»ºè¿­ä»£æ•°æ®æ–‡ä»¶å¤¹: {iteration_folder}")
        logger.info(f"åˆ›å»ºçƒ­åº¦çˆ¬å–æ–‡ä»¶å¤¹: {popularity_folder}")
        logger.info(f"åˆ›å»ºæ—¶é—´çˆ¬å–æ–‡ä»¶å¤¹: {time_folder}")
    
    print(f"\nğŸ“ åˆ›å»ºè¿­ä»£æ•°æ®æ–‡ä»¶å¤¹: {iteration_folder}")
    print(f"ğŸ“ çƒ­åº¦çˆ¬å–åŸå§‹æ•°æ®: {popularity_folder}")
    print(f"ğŸ“ æ—¶é—´çˆ¬å–åŸå§‹æ•°æ®: {time_folder}")
    
    iteration_count = 0
    all_popularity_comments = []
    all_time_comments = []
    
    logger.info(f"æ—¶é—´è¿­ä»£å¼€å§‹ï¼Œé¢„è®¡ç»“æŸæ—¶é—´: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    while datetime.now() < end_time:
        iteration_count += 1
        current_time = datetime.now()
        remaining_time = end_time - current_time
        
        print(f"\nğŸ”„ ç¬¬ {iteration_count} è½®è¿­ä»£ (å‰©ä½™æ—¶é—´: {str(remaining_time).split('.')[0]})")
        logger.info(f"å¼€å§‹ç¬¬ {iteration_count} è½®è¿­ä»£")
        
        # çƒ­åº¦æ’åºçˆ¬å–
        print(f"ğŸ”¥ çƒ­åº¦æ’åºçˆ¬å–...")
        popularity_comments, popularity_reason = crawl_all_comments_with_reason(
            oid=oid, mode=1, ps=ps, delay_ms=delay_ms, test_mode=False, logger=logger, output_folder=output_folder
        )
        
        if popularity_comments:
            # ä¿å­˜çƒ­åº¦çˆ¬å–åŸå§‹æ•°æ®
            popularity_filename = generate_safe_filename(video_title, oid, f"ç¬¬{iteration_count}æ¬¡çƒ­åº¦æ’åºçˆ¬å–ç»“æœ", "original")
            popularity_file = os.path.join(popularity_folder, f'{popularity_filename}.csv')
            save_comments_to_csv(popularity_comments, popularity_file, f"ç¬¬{iteration_count}æ¬¡çƒ­åº¦æ’åºçˆ¬å–ç»“æœ")
            all_popularity_comments.extend(popularity_comments)
            
            print(f"   âœ… çƒ­åº¦çˆ¬å–å®Œæˆ: {len(popularity_comments)} æ¡è¯„è®º")
            print(f"   ğŸ’¾ å·²ä¿å­˜: {os.path.basename(popularity_file)}")
            logger.info(f"ç¬¬ {iteration_count} è½®çƒ­åº¦çˆ¬å–å®Œæˆ: {len(popularity_comments)} æ¡è¯„è®º")
            logger.info(f"çƒ­åº¦çˆ¬å–åŸå§‹æ•°æ®å·²ä¿å­˜: {popularity_file}")
        
        # æ£€æŸ¥å‰©ä½™æ—¶é—´
        if datetime.now() >= end_time:
            print("â° æ—¶é—´å·²åˆ°ï¼Œåœæ­¢è¿­ä»£")
            break
        
        # æ—¶é—´æ’åºçˆ¬å–
        print(f"â° æ—¶é—´æ’åºçˆ¬å–...")
        time_comments, time_reason = crawl_all_comments_with_reason(
            oid=oid, mode=0, ps=ps, delay_ms=delay_ms, test_mode=False, logger=logger, output_folder=output_folder
        )
        
        if time_comments:
            # ä¿å­˜æ—¶é—´çˆ¬å–åŸå§‹æ•°æ®
            time_filename = generate_safe_filename(video_title, oid, f"ç¬¬{iteration_count}æ¬¡æ—¶é—´æ’åºçˆ¬å–ç»“æœ", "original")
            time_file = os.path.join(time_folder, f'{time_filename}.csv')
            save_comments_to_csv(time_comments, time_file, f"ç¬¬{iteration_count}æ¬¡æ—¶é—´æ’åºçˆ¬å–ç»“æœ")
            all_time_comments.extend(time_comments)
            
            print(f"   âœ… æ—¶é—´çˆ¬å–å®Œæˆ: {len(time_comments)} æ¡è¯„è®º")
            print(f"   ğŸ’¾ å·²ä¿å­˜: {os.path.basename(time_file)}")
            logger.info(f"ç¬¬ {iteration_count} è½®æ—¶é—´çˆ¬å–å®Œæˆ: {len(time_comments)} æ¡è¯„è®º")
            logger.info(f"æ—¶é—´çˆ¬å–åŸå§‹æ•°æ®å·²ä¿å­˜: {time_file}")
        
        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ—¶é—´è¿›è¡Œä¸‹ä¸€è½®
        if datetime.now() >= end_time:
            print("â° æ—¶é—´å·²åˆ°ï¼Œåœæ­¢è¿­ä»£")
            break
        
        # è½®æ¬¡é—´éš”
        time.sleep(2)
    
    # æ‰§è¡Œè¿­ä»£å»é‡
    print(f"\nğŸ”„ å¼€å§‹è¿­ä»£å»é‡å¤„ç†...")
    deduped_popularity, deduped_time, merged_comments, duplicate_comments = perform_iteration_deduplication(
        all_popularity_comments, all_time_comments, logger
    )
    
    # ä¿å­˜ä¸‰ä»½å»é‡ç»“æœåˆ°åŸå§‹æ•°æ®æ–‡ä»¶å¤¹
    popularity_filename = generate_safe_filename(video_title, oid, "æŒ‰çƒ­åº¦è¿­ä»£å»é‡ç»“æœ", "final")
    time_filename = generate_safe_filename(video_title, oid, "æŒ‰æ—¶é—´è¿­ä»£å»é‡ç»“æœ", "final")
    final_filename = generate_safe_filename(video_title, oid, "åˆå¹¶å»é‡ç»“æœ", "final")
    
    popularity_file = os.path.join(iteration_folder, f'{popularity_filename}.csv')
    time_file = os.path.join(iteration_folder, f'{time_filename}.csv')
    final_file = os.path.join(iteration_folder, f'{final_filename}.csv')
    
    save_comments_to_csv(deduped_popularity, popularity_file, "æŒ‰çƒ­åº¦è¿­ä»£å»é‡ç»“æœ")
    save_comments_to_csv(deduped_time, time_file, "æŒ‰æ—¶é—´è¿­ä»£å»é‡ç»“æœ")
    save_comments_to_csv(merged_comments, final_file, "åˆå¹¶å»é‡ç»“æœ")
    
    print(f"\nğŸ’¾ æŒ‰çƒ­åº¦å»é‡ç»“æœå·²ä¿å­˜: {os.path.basename(popularity_file)}")
    print(f"ğŸ’¾ æŒ‰æ—¶é—´å»é‡ç»“æœå·²ä¿å­˜: {os.path.basename(time_file)}")
    print(f"ğŸ’¾ åˆå¹¶å»é‡ç»“æœå·²ä¿å­˜: {os.path.basename(final_file)}")
    logger.info(f"æŒ‰çƒ­åº¦å»é‡ç»“æœå·²ä¿å­˜: {popularity_file}")
    logger.info(f"æŒ‰æ—¶é—´å»é‡ç»“æœå·²ä¿å­˜: {time_file}")
    logger.info(f"åˆå¹¶å»é‡ç»“æœå·²ä¿å­˜: {final_file}")
    
    # ä¼˜åŒ–ï¼šä¸å†ç”Ÿæˆçƒ­åº¦æ’åºå’Œæ—¶é—´æ’åºçš„åŸå§‹æ•°æ®æ–‡ä»¶
    # åªä¿å­˜åˆå¹¶å»é‡ç»“æœå’Œé‡å¤è¯„è®ºåˆ—è¡¨
    print("\n=== å¼€å§‹ç”Ÿæˆå¿…è¦çš„åŸå§‹æ•°æ®æ–‡æ¡£ ===")
    
    # åˆ›å»ºåŸå§‹æ•°æ®æ–‡ä»¶å¤¹
    raw_data_folder = os.path.join(output_folder, 'åŸå§‹æ•°æ®')
    if not os.path.exists(raw_data_folder):
        os.makedirs(raw_data_folder)
    
    # è®¡ç®—é‡å¤è¯„è®ºåˆ—è¡¨
    duplicate_comments = []
    all_rpids = set()
    for comment in all_popularity_comments + all_time_comments:
        rpid = comment.get('rpid', '')
        if rpid in all_rpids:
            duplicate_comments.append(comment)
        else:
            all_rpids.add(rpid)
    
    # åªä¿å­˜é‡å¤è¯„è®ºåˆ—è¡¨ï¼ˆåˆå¹¶å»é‡ç»“æœå·²åœ¨ä¸Šé¢ä¿å­˜ï¼‰
    duplicate_filename = generate_safe_filename(video_title, oid, "é‡å¤è¯„è®ºåˆ—è¡¨", "final")
    duplicate_file = os.path.join(raw_data_folder, f'{duplicate_filename}.csv')
    save_comments_to_csv(duplicate_comments, duplicate_file, 'é‡å¤è¯„è®ºåˆ—è¡¨')
    
    print(f"ğŸ’¾ é‡å¤è¯„è®ºåˆ—è¡¨å·²ä¿å­˜: {os.path.basename(duplicate_file)}")
    logger.info(f"é‡å¤è¯„è®ºåˆ—è¡¨å·²ä¿å­˜: {duplicate_file}")
    
    print(f"âœ… ä¼˜åŒ–å®Œæˆï¼šè·³è¿‡ç”Ÿæˆçƒ­åº¦æ’åºå’Œæ—¶é—´æ’åºåŸå§‹æ•°æ®æ–‡ä»¶")
    print(f"   - åˆå¹¶å»é‡ç»“æœ: {len(merged_comments)} æ¡è¯„è®º")
    print(f"   - é‡å¤è¯„è®ºåˆ—è¡¨: {len(duplicate_comments)} æ¡è¯„è®º")
    
    # å¯¹åˆå¹¶ç»“æœè¿›è¡ŒåŒé‡æ•´ç†ï¼ˆä¸ç»¼åˆæ¨¡å¼ç›¸åŒï¼‰
    print("\n=== å¼€å§‹åŒé‡æ•´ç† ===")
    print("1. æŒ‰çƒ­åº¦æ’åºæ•´ç†...")
    
    # æŒ‰çƒ­åº¦æ’åºæ•´ç†ï¼ˆä½¿ç”¨åˆå¹¶åçš„æ•°æ®ï¼‰
    _, popularity_organized_file, popularity_stats_file = process_and_organize_data(
        merged_comments, output_folder, oid, logger, video_title, sort_by_popularity=True, video_info=video_info, mode="iteration"
    )
    
    print("2. æŒ‰æ—¶é—´ç»Ÿè®¡æ•´ç†...")
    
    # æŒ‰æ—¶é—´ç»Ÿè®¡æ•´ç†ï¼ˆä½¿ç”¨åˆå¹¶åçš„æ•°æ®ï¼‰- ä¸ç”Ÿæˆæ•´ç†æ–‡ä»¶
    _, _, time_stats_file = process_and_organize_data(
        merged_comments, output_folder, oid, logger, video_title, sort_by_popularity=False, video_info=video_info, mode="iteration"
    )
    
    # ç”Ÿæˆæ™ºèƒ½æ—¶é—´ç»Ÿè®¡æ–‡ä»¶
    print("3. ç”Ÿæˆæ—¶é—´ç»Ÿè®¡åˆ†æ...")
    bv_id = aid_to_bvid(int(oid))
    time_analysis_files = generate_restructured_time_statistics(
        merged_comments, output_folder, bv_id, logger, video_title, video_info
    )
    
    if time_analysis_files:
        print(f"   âœ… å·²ç”Ÿæˆ {len(time_analysis_files)} ä¸ªæ—¶é—´ç»Ÿè®¡æ–‡ä»¶")
        for file_path in time_analysis_files:
            print(f"      - {os.path.basename(file_path)}")
    else:
        print("   âš ï¸  æœªç”Ÿæˆæ—¶é—´ç»Ÿè®¡æ–‡ä»¶ï¼ˆå¯èƒ½å› ä¸ºæ•°æ®ä¸è¶³ï¼‰")
    
    # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
    generate_iteration_statistics(
        all_popularity_comments, all_time_comments, merged_comments, 
        iteration_count, iteration_hours, output_folder, oid, logger,
        deduped_popularity=deduped_popularity, deduped_time=deduped_time,
        bv_id=bv_id, video_title=video_title
    )
    
    print(f"\nâœ… æ—¶é—´è¿­ä»£å®Œæˆ: {iteration_count} è½®è¿­ä»£ï¼Œæœ€ç»ˆè·å¾— {len(merged_comments)} æ¡å”¯ä¸€è¯„è®º")
    logger.info(f"æ—¶é—´è¿­ä»£å®Œæˆ: {iteration_count} è½®è¿­ä»£ï¼Œæœ€ç»ˆè·å¾— {len(merged_comments)} æ¡å”¯ä¸€è¯„è®º")
    
    # ç”Ÿæˆæ–‡ä»¶å¤¹ç»“æ„æ–‡æ¡£
    try:
        # ç”ŸæˆBVå·
        try:
            bv_id = aid_to_bvid(int(oid))
        except:
            bv_id = None
        structure_md_path = generate_folder_structure_md(output_folder, oid, video_title, logger, bv_id)
        print(f"ğŸ“„ æ–‡ä»¶å¤¹ç»“æ„æ–‡æ¡£: {os.path.basename(structure_md_path)}")
    except Exception as e:
        logger.error(f"ç”Ÿæˆæ–‡ä»¶å¤¹ç»“æ„æ–‡æ¡£å¤±è´¥: {e}")
    
    return True

def crawl_duplicate_rate_iteration(oid, ps, delay_ms, popularity_threshold, time_threshold, logger, output_folder, video_title=None, video_info=None):
    """
    é‡å¤ç‡è¿­ä»£æ¨¡å¼ï¼šæ ¹æ®é‡å¤ç‡é˜ˆå€¼å†³å®šåœæ­¢æ¡ä»¶
    
    Args:
        oid: è§†é¢‘oid
        ps: æ¯é¡µè¯„è®ºæ•°
        delay_ms: è¯·æ±‚å»¶æ—¶
        popularity_threshold: çƒ­åº¦çˆ¬å–é‡å¤ç‡é˜ˆå€¼
        time_threshold: æ—¶é—´çˆ¬å–é‡å¤ç‡é˜ˆå€¼
        logger: æ—¥å¿—è®°å½•å™¨
        output_folder: è¾“å‡ºæ–‡ä»¶å¤¹
    
    Returns:
        bool: æ˜¯å¦æˆåŠŸ
    """
    import time
    from datetime import datetime
    
    # åˆ›å»ºè¿­ä»£æ•°æ®å­˜å‚¨æ–‡ä»¶å¤¹
    iteration_folder = os.path.join(output_folder, 'åŸå§‹æ•°æ®')
    popularity_folder = os.path.join(iteration_folder, 'çƒ­åº¦çˆ¬å–åŸå§‹æ•°æ®')
    time_folder = os.path.join(iteration_folder, 'æ—¶é—´çˆ¬å–åŸå§‹æ•°æ®')
    
    for folder in [iteration_folder, popularity_folder, time_folder]:
        if not os.path.exists(folder):
            os.makedirs(folder)
    
    if logger:
        logger.info(f"åˆ›å»ºè¿­ä»£æ•°æ®æ–‡ä»¶å¤¹: {iteration_folder}")
        logger.info(f"åˆ›å»ºçƒ­åº¦çˆ¬å–æ–‡ä»¶å¤¹: {popularity_folder}")
        logger.info(f"åˆ›å»ºæ—¶é—´çˆ¬å–æ–‡ä»¶å¤¹: {time_folder}")
    
    print(f"\nğŸ“ åˆ›å»ºè¿­ä»£æ•°æ®æ–‡ä»¶å¤¹: {iteration_folder}")
    print(f"ğŸ“ çƒ­åº¦çˆ¬å–åŸå§‹æ•°æ®: {popularity_folder}")
    print(f"ğŸ“ æ—¶é—´çˆ¬å–åŸå§‹æ•°æ®: {time_folder}")
    
    iteration_count = 0
    all_popularity_comments = []
    all_time_comments = []
    
    # å­˜å‚¨æ¯è½®çš„rpidé›†åˆç”¨äºè®¡ç®—é‡å¤ç‡
    popularity_rpid_history = []
    time_rpid_history = []
    
    # å­˜å‚¨æ¯è½®é‡å¤ç‡æ•°æ®
    popularity_duplicate_rates = []
    time_duplicate_rates = []
    
    # æ§åˆ¶çˆ¬å–æ–¹å¼çš„ç»§ç»­çŠ¶æ€
    popularity_continue = True
    time_continue = True
    
    logger.info(f"é‡å¤ç‡è¿­ä»£å¼€å§‹ï¼Œçƒ­åº¦é˜ˆå€¼: {popularity_threshold}%, æ—¶é—´é˜ˆå€¼: {time_threshold}%")
    
    while popularity_continue or time_continue:
        iteration_count += 1
        print(f"\nğŸ”„ ç¬¬ {iteration_count} è½®è¿­ä»£")
        logger.info(f"å¼€å§‹ç¬¬ {iteration_count} è½®è¿­ä»£")
        
        # çƒ­åº¦æ’åºçˆ¬å–ï¼ˆä»…åœ¨æœªè¾¾åˆ°é˜ˆå€¼æ—¶æ‰§è¡Œï¼‰
        if popularity_continue:
            print(f"ğŸ”¥ çƒ­åº¦æ’åºçˆ¬å–...")
            popularity_comments, popularity_reason = crawl_all_comments_with_reason(
                oid=oid, mode=1, ps=ps, delay_ms=delay_ms, test_mode=False, logger=logger, output_folder=output_folder
            )
            
            if popularity_comments:
                # è®¡ç®—é‡å¤ç‡
                current_rpids = set(comment.get('rpid', '') for comment in popularity_comments if comment.get('rpid'))
                popularity_rpid_history.append(current_rpids)
                
                if len(popularity_rpid_history) >= 2:
                    duplicate_rate = calculate_duplicate_rate(
                        popularity_rpid_history[-2], popularity_rpid_history[-1]
                    )
                    popularity_duplicate_rates.append(duplicate_rate)
                    print(f"   ğŸ“Š çƒ­åº¦çˆ¬å–é‡å¤ç‡: {duplicate_rate:.1f}%")
                    logger.info(f"ç¬¬ {iteration_count} è½®çƒ­åº¦çˆ¬å–é‡å¤ç‡: {duplicate_rate:.1f}%")
                    
                    if duplicate_rate >= popularity_threshold:
                        print(f"   ğŸ›‘ çƒ­åº¦çˆ¬å–é‡å¤ç‡è¾¾åˆ°é˜ˆå€¼ ({popularity_threshold}%)ï¼Œåç»­è½®æ¬¡å°†è·³è¿‡çƒ­åº¦çˆ¬å–")
                        popularity_continue = False
                
                # ä¿å­˜çƒ­åº¦çˆ¬å–åŸå§‹æ•°æ®
                popularity_filename = generate_safe_filename(video_title, oid, f"ç¬¬{iteration_count}æ¬¡çƒ­åº¦æ’åºçˆ¬å–ç»“æœ", "original")
                popularity_file = os.path.join(popularity_folder, f'{popularity_filename}.csv')
                save_comments_to_csv(popularity_comments, popularity_file, f"ç¬¬{iteration_count}æ¬¡çƒ­åº¦æ’åºçˆ¬å–ç»“æœ")
                all_popularity_comments.extend(popularity_comments)
                print(f"   âœ… çƒ­åº¦çˆ¬å–å®Œæˆ: {len(popularity_comments)} æ¡è¯„è®º")
                print(f"   ğŸ’¾ å·²ä¿å­˜: {os.path.basename(popularity_file)}")
                logger.info(f"çƒ­åº¦çˆ¬å–åŸå§‹æ•°æ®å·²ä¿å­˜: {popularity_file}")
            else:
                print(f"   âš ï¸  çƒ­åº¦çˆ¬å–æœªè·å–åˆ°è¯„è®ºï¼Œåœæ­¢çƒ­åº¦çˆ¬å–")
                popularity_continue = False
        else:
            print(f"ğŸ”¥ çƒ­åº¦æ’åºçˆ¬å–å·²è·³è¿‡ï¼ˆé‡å¤ç‡å·²è¾¾é˜ˆå€¼ï¼‰")
        
        # æ—¶é—´æ’åºçˆ¬å–ï¼ˆä»…åœ¨æœªè¾¾åˆ°é˜ˆå€¼æ—¶æ‰§è¡Œï¼‰
        if time_continue:
            print(f"â° æ—¶é—´æ’åºçˆ¬å–...")
            time_comments, time_reason = crawl_all_comments_with_reason(
                oid=oid, mode=0, ps=ps, delay_ms=delay_ms, test_mode=False, logger=logger, output_folder=output_folder
            )
            
            if time_comments:
                # è®¡ç®—é‡å¤ç‡
                current_rpids = set(comment.get('rpid', '') for comment in time_comments if comment.get('rpid'))
                time_rpid_history.append(current_rpids)
                
                if len(time_rpid_history) >= 2:
                    duplicate_rate = calculate_duplicate_rate(
                        time_rpid_history[-2], time_rpid_history[-1]
                    )
                    time_duplicate_rates.append(duplicate_rate)
                    print(f"   ğŸ“Š æ—¶é—´çˆ¬å–é‡å¤ç‡: {duplicate_rate:.1f}%")
                    logger.info(f"ç¬¬ {iteration_count} è½®æ—¶é—´çˆ¬å–é‡å¤ç‡: {duplicate_rate:.1f}%")
                    
                    if duplicate_rate >= time_threshold:
                        print(f"   ğŸ›‘ æ—¶é—´çˆ¬å–é‡å¤ç‡è¾¾åˆ°é˜ˆå€¼ ({time_threshold}%)ï¼Œåç»­è½®æ¬¡å°†è·³è¿‡æ—¶é—´çˆ¬å–")
                        time_continue = False
                
                # ä¿å­˜æ—¶é—´çˆ¬å–åŸå§‹æ•°æ®
                time_filename = generate_safe_filename(video_title, oid, f"ç¬¬{iteration_count}æ¬¡æ—¶é—´æ’åºçˆ¬å–ç»“æœ", "original")
                time_file = os.path.join(time_folder, f'{time_filename}.csv')
                save_comments_to_csv(time_comments, time_file, f"ç¬¬{iteration_count}æ¬¡æ—¶é—´æ’åºçˆ¬å–ç»“æœ")
                all_time_comments.extend(time_comments)
                print(f"   âœ… æ—¶é—´çˆ¬å–å®Œæˆ: {len(time_comments)} æ¡è¯„è®º")
                print(f"   ğŸ’¾ å·²ä¿å­˜: {os.path.basename(time_file)}")
                logger.info(f"æ—¶é—´çˆ¬å–åŸå§‹æ•°æ®å·²ä¿å­˜: {time_file}")
            else:
                print(f"   âš ï¸  æ—¶é—´çˆ¬å–æœªè·å–åˆ°è¯„è®ºï¼Œåœæ­¢æ—¶é—´çˆ¬å–")
                time_continue = False
        else:
            print(f"â° æ—¶é—´æ’åºçˆ¬å–å·²è·³è¿‡ï¼ˆé‡å¤ç‡å·²è¾¾é˜ˆå€¼ï¼‰")
        
        # æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢è¿­ä»£
        if not popularity_continue and not time_continue:
            print(f"\nğŸ›‘ ä¸¤ç§çˆ¬å–æ–¹å¼çš„é‡å¤ç‡éƒ½è¾¾åˆ°é˜ˆå€¼ï¼Œåœæ­¢è¿­ä»£")
            logger.info("é‡å¤ç‡è¿­ä»£ç»“æŸï¼šä¸¤ç§çˆ¬å–æ–¹å¼çš„é‡å¤ç‡éƒ½è¾¾åˆ°é˜ˆå€¼")
            break
        
        # è½®æ¬¡é—´éš”
        time.sleep(2)
    
    # æ‰§è¡Œè¿­ä»£å»é‡
    print(f"\nğŸ”„ å¼€å§‹è¿­ä»£å»é‡å¤„ç†...")
    deduped_popularity, deduped_time, merged_comments, duplicate_comments = perform_iteration_deduplication(
        all_popularity_comments, all_time_comments, logger
    )
    
    # ä¿å­˜ä¸‰ä»½å»é‡ç»“æœåˆ°åŸå§‹æ•°æ®æ–‡ä»¶å¤¹
    popularity_filename = generate_safe_filename(video_title, oid, "æŒ‰çƒ­åº¦è¿­ä»£å»é‡ç»“æœ", "final")
    time_filename = generate_safe_filename(video_title, oid, "æŒ‰æ—¶é—´è¿­ä»£å»é‡ç»“æœ", "final")
    final_filename = generate_safe_filename(video_title, oid, "åˆå¹¶å»é‡ç»“æœ", "final")
    
    popularity_file = os.path.join(iteration_folder, f'{popularity_filename}.csv')
    time_file = os.path.join(iteration_folder, f'{time_filename}.csv')
    final_file = os.path.join(iteration_folder, f'{final_filename}.csv')
    
    save_comments_to_csv(deduped_popularity, popularity_file, "æŒ‰çƒ­åº¦è¿­ä»£å»é‡ç»“æœ")
    save_comments_to_csv(deduped_time, time_file, "æŒ‰æ—¶é—´è¿­ä»£å»é‡ç»“æœ")
    save_comments_to_csv(merged_comments, final_file, "åˆå¹¶å»é‡ç»“æœ")
    
    print(f"\nğŸ’¾ æŒ‰çƒ­åº¦å»é‡ç»“æœå·²ä¿å­˜: {os.path.basename(popularity_file)}")
    print(f"ğŸ’¾ æŒ‰æ—¶é—´å»é‡ç»“æœå·²ä¿å­˜: {os.path.basename(time_file)}")
    print(f"ğŸ’¾ åˆå¹¶å»é‡ç»“æœå·²ä¿å­˜: {os.path.basename(final_file)}")
    logger.info(f"æŒ‰çƒ­åº¦å»é‡ç»“æœå·²ä¿å­˜: {popularity_file}")
    logger.info(f"æŒ‰æ—¶é—´å»é‡ç»“æœå·²ä¿å­˜: {time_file}")
    logger.info(f"åˆå¹¶å»é‡ç»“æœå·²ä¿å­˜: {final_file}")
    
    # å¤„ç†è¿­ä»£æ¨¡å¼æ•°æ®ï¼Œä»…ç”Ÿæˆé‡å¤è¯„è®ºåˆ—è¡¨
    print("\n=== å¼€å§‹ç”Ÿæˆé‡å¤è¯„è®ºåˆ—è¡¨ ===")
    # ç”Ÿæˆé‡å¤è¯„è®ºåˆ—è¡¨
    duplicate_comments = []
    # ä»åŸå§‹æ•°æ®ä¸­æ‰¾å‡ºé‡å¤çš„è¯„è®º
    all_rpids = set()
    for comment in all_popularity_comments + all_time_comments:
        rpid = comment.get('rpid', '')
        if rpid in all_rpids:
            duplicate_comments.append(comment)
        else:
            all_rpids.add(rpid)
    
    # åˆ›å»ºåŸå§‹æ•°æ®æ–‡ä»¶å¤¹
    raw_data_folder = os.path.join(output_folder, "åŸå§‹æ•°æ®")
    os.makedirs(raw_data_folder, exist_ok=True)
    
    # ä»…ä¿å­˜é‡å¤è¯„è®ºåˆ—è¡¨
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    duplicate_file = os.path.join(raw_data_folder, f"è¯„è®ºçˆ¬å–_é‡å¤è¯„è®ºåˆ—è¡¨_{video_title}_{oid}_{timestamp}.csv")
    save_comments_to_csv(duplicate_comments, duplicate_file, "é‡å¤è¯„è®ºåˆ—è¡¨")
    print(f"ğŸ’¾ é‡å¤è¯„è®ºåˆ—è¡¨å·²ä¿å­˜: {os.path.basename(duplicate_file)}")
    logger.info(f"é‡å¤è¯„è®ºåˆ—è¡¨å·²ä¿å­˜: {duplicate_file}")
    
    # å¯¹åˆå¹¶ç»“æœè¿›è¡ŒåŒé‡æ•´ç†ï¼ˆä¸ç»¼åˆæ¨¡å¼ç›¸åŒï¼‰
    print("\n=== å¼€å§‹åŒé‡æ•´ç† ===")
    print("1. æŒ‰çƒ­åº¦æ’åºæ•´ç†...")
    
    # æŒ‰çƒ­åº¦æ’åºæ•´ç†ï¼ˆä½¿ç”¨åˆå¹¶åçš„æ•°æ®ï¼‰
    _, popularity_organized_file, popularity_stats_file = process_and_organize_data(
        merged_comments, output_folder, oid, logger, video_title, sort_by_popularity=True, video_info=video_info
    )
    
    print("2. æŒ‰æ—¶é—´ç»Ÿè®¡æ•´ç†...")
    
    # æŒ‰æ—¶é—´ç»Ÿè®¡æ•´ç†ï¼ˆä½¿ç”¨åˆå¹¶åçš„æ•°æ®ï¼‰- ä¸ç”Ÿæˆæ•´ç†æ–‡ä»¶
    _, _, time_stats_file = process_and_organize_data(
        merged_comments, output_folder, oid, logger, video_title, sort_by_popularity=False, video_info=video_info
    )
    
    # ç”Ÿæˆæ™ºèƒ½æ—¶é—´ç»Ÿè®¡æ–‡ä»¶
    print("3. ç”Ÿæˆæ—¶é—´ç»Ÿè®¡åˆ†æ...")
    bv_id = aid_to_bvid(int(oid))
    time_analysis_files = generate_restructured_time_statistics(
        merged_comments, output_folder, bv_id, logger, video_title, video_info
    )
    
    if time_analysis_files:
        print(f"   âœ… å·²ç”Ÿæˆ {len(time_analysis_files)} ä¸ªæ—¶é—´ç»Ÿè®¡æ–‡ä»¶")
        for file_path in time_analysis_files:
            print(f"      - {os.path.basename(file_path)}")
    else:
        print("   âš ï¸  æœªç”Ÿæˆæ—¶é—´ç»Ÿè®¡æ–‡ä»¶ï¼ˆå¯èƒ½å› ä¸ºæ•°æ®ä¸è¶³ï¼‰")
    
    # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
    generate_iteration_statistics(
        all_popularity_comments, all_time_comments, merged_comments, 
        iteration_count, None, output_folder, oid, logger, 
        popularity_threshold=popularity_threshold, time_threshold=time_threshold,
        deduped_popularity=deduped_popularity, deduped_time=deduped_time,
        popularity_duplicate_rates=popularity_duplicate_rates, time_duplicate_rates=time_duplicate_rates,
        bv_id=bv_id, video_title=video_title
    )
    
    print(f"\nâœ… é‡å¤ç‡è¿­ä»£å®Œæˆ: {iteration_count} è½®è¿­ä»£ï¼Œæœ€ç»ˆè·å¾— {len(merged_comments)} æ¡å”¯ä¸€è¯„è®º")
    logger.info(f"é‡å¤ç‡è¿­ä»£å®Œæˆ: {iteration_count} è½®è¿­ä»£ï¼Œæœ€ç»ˆè·å¾— {len(merged_comments)} æ¡å”¯ä¸€è¯„è®º")
    
    # ç”Ÿæˆæ–‡ä»¶å¤¹ç»“æ„æ–‡æ¡£
    try:
        # ç”ŸæˆBVå·
        try:
            bv_id = aid_to_bvid(int(oid))
        except:
            bv_id = None
        structure_md_path = generate_folder_structure_md(output_folder, oid, video_title, logger, bv_id)
        print(f"ğŸ“„ æ–‡ä»¶å¤¹ç»“æ„æ–‡æ¡£: {os.path.basename(structure_md_path)}")
    except Exception as e:
        logger.error(f"ç”Ÿæˆæ–‡ä»¶å¤¹ç»“æ„æ–‡æ¡£å¤±è´¥: {e}")
    
    return True

def calculate_duplicate_rate(prev_rpids, current_rpids):
    """
    è®¡ç®—ä¸¤æ¬¡çˆ¬å–çš„é‡å¤ç‡
    
    Args:
        prev_rpids: ä¸Šä¸€æ¬¡çˆ¬å–çš„rpidé›†åˆ
        current_rpids: å½“å‰çˆ¬å–çš„rpidé›†åˆ
    
    Returns:
        float: é‡å¤ç‡ç™¾åˆ†æ¯”
    """
    if not current_rpids:
        return 0.0
    
    intersection = prev_rpids.intersection(current_rpids)
    duplicate_rate = (len(intersection) / len(current_rpids)) * 100
    
    return duplicate_rate

def perform_iteration_deduplication(popularity_comments, time_comments, logger):
    """
    æ‰§è¡Œè¿­ä»£å»é‡ç®—æ³•ï¼šæŒ‰æ—¶é—´é¡ºåºä¿ç•™æœ€æ–°æ•°æ®ï¼Œå…ˆå¯¹åŒç±»å‹çˆ¬å–å»é‡ï¼Œå†åˆå¹¶å»é‡
    
    Args:
        popularity_comments: æ‰€æœ‰çƒ­åº¦çˆ¬å–çš„è¯„è®º
        time_comments: æ‰€æœ‰æ—¶é—´çˆ¬å–çš„è¯„è®º
        logger: æ—¥å¿—è®°å½•å™¨
    
    Returns:
        tuple: (çƒ­åº¦å»é‡ç»“æœ, æ—¶é—´å»é‡ç»“æœ, åˆå¹¶å»é‡ç»“æœ)
    """
    logger.info("å¼€å§‹è¿­ä»£å»é‡å¤„ç†")
    
    # ç¬¬ä¸€æ­¥ï¼šå¯¹åŒç±»å‹çˆ¬å–è¿›è¡Œå»é‡ï¼ˆä¿ç•™æœ€æ–°çš„ï¼‰
    def deduplicate_by_rpid(comments, comment_type):
        """ç»Ÿä¸€çš„å»é‡å‡½æ•°ï¼ŒåŸºäºrpidå’Œçˆ¬å–æ—¶é—´å»é‡ï¼Œä¸ç»¼åˆæ¨¡å¼ä¿æŒä¸€è‡´"""
        rpid_to_comment = {}
        duplicates = []
        
        def extract_crawl_time_from_filename(comment):
            """ä»è¯„è®ºæ•°æ®çš„æ¥æºæ–‡ä»¶åä¸­æå–çˆ¬å–æ—¶é—´"""
            # å°è¯•ä»è¯„è®ºæ•°æ®ä¸­è·å–æ–‡ä»¶æ¥æºä¿¡æ¯
            # å¦‚æœæ²¡æœ‰ç›´æ¥çš„æ–‡ä»¶åä¿¡æ¯ï¼Œä½¿ç”¨æ—¶é—´æˆ³ä½œä¸ºå¤‡é€‰
            crawl_time_str = comment.get('çˆ¬å–æ—¶é—´', '')
            if crawl_time_str:
                try:
                    # è§£ææ—¶é—´æ ¼å¼ï¼šYYYYå¹´MMæœˆDDæ—¥_HHæ—¶MMåˆ†SSç§’
                    from datetime import datetime
                    dt = datetime.strptime(crawl_time_str, '%Yå¹´%mæœˆ%dæ—¥_%Hæ—¶%Måˆ†%Sç§’')
                    return int(dt.timestamp())
                except:
                    pass
            
            # å¤‡é€‰æ–¹æ¡ˆï¼šä½¿ç”¨è¯„è®ºçš„æ—¶é—´æˆ³
            return comment.get('æ—¶é—´æˆ³', 0)
        
        for comment in comments:
            rpid = comment.get('rpid', '')
            if rpid:
                # å¦‚æœå·²å­˜åœ¨è¯¥rpidï¼Œæ¯”è¾ƒçˆ¬å–æ—¶é—´ï¼Œä¿ç•™çˆ¬å–æ—¶é—´æ›´æ™šçš„
                if rpid in rpid_to_comment:
                    existing_crawl_time = extract_crawl_time_from_filename(rpid_to_comment[rpid])
                    current_crawl_time = extract_crawl_time_from_filename(comment)
                    
                    if current_crawl_time > existing_crawl_time:
                        # å°†æ—§è¯„è®ºæ ‡è®°ä¸ºé‡å¤
                        old_comment = rpid_to_comment[rpid].copy()
                        old_comment['é‡å¤æ¥æº'] = comment_type
                        old_comment['åŸå§‹è¯„è®ºæ¥æº'] = comment_type
                        duplicates.append(old_comment)
                        # æ›´æ–°ä¸ºæ–°è¯„è®ºï¼ˆä¿ç•™åŠ¨æ€å±æ€§ï¼šç‚¹èµæ•°ã€å›å¤æ•°ã€æ—¶é—´æˆ³ç­‰ï¼‰
                        rpid_to_comment[rpid] = comment
                    else:
                        # å½“å‰è¯„è®ºæ˜¯é‡å¤çš„ï¼ˆçˆ¬å–æ—¶é—´æ›´æ—©æˆ–ç›¸ç­‰ï¼‰
                        duplicate_comment = comment.copy()
                        duplicate_comment['é‡å¤æ¥æº'] = comment_type
                        duplicate_comment['åŸå§‹è¯„è®ºæ¥æº'] = comment_type
                        duplicates.append(duplicate_comment)
                else:
                    rpid_to_comment[rpid] = comment
        
        deduped_comments = list(rpid_to_comment.values())
        logger.info(f"{comment_type}å»é‡: {len(comments)} -> {len(deduped_comments)} æ¡è¯„è®ºï¼Œé‡å¤ {len(duplicates)} æ¡")
        return deduped_comments, duplicates
    
    # å¯¹çƒ­åº¦å’Œæ—¶é—´çˆ¬å–åˆ†åˆ«å»é‡
    deduped_popularity, pop_duplicates = deduplicate_by_rpid(popularity_comments, "çƒ­åº¦çˆ¬å–")
    deduped_time, time_duplicates = deduplicate_by_rpid(time_comments, "æ—¶é—´çˆ¬å–")
    
    # ç¬¬äºŒæ­¥ï¼šåˆå¹¶ä¸¤ç§çˆ¬å–ç»“æœå¹¶å»é‡
    all_comments = deduped_popularity + deduped_time
    final_comments, merge_duplicates = deduplicate_by_rpid(all_comments, "æœ€ç»ˆåˆå¹¶")
    
    # åˆå¹¶æ‰€æœ‰é‡å¤è¯„è®º
    all_duplicates = pop_duplicates + time_duplicates + merge_duplicates
    
    logger.info(f"è¿­ä»£å»é‡å®Œæˆ: çƒ­åº¦{len(deduped_popularity)}æ¡ï¼Œæ—¶é—´{len(deduped_time)}æ¡ï¼Œåˆå¹¶{len(final_comments)}æ¡ï¼Œæ€»é‡å¤{len(all_duplicates)}æ¡")
    return deduped_popularity, deduped_time, final_comments, all_duplicates

def generate_iteration_statistics(popularity_comments, time_comments, merged_comments, 
                                iteration_count, iteration_hours, output_folder, oid, logger,
                                deduped_popularity=None, deduped_time=None,
                                popularity_threshold=None, time_threshold=None,
                                popularity_duplicate_rates=None, time_duplicate_rates=None,
                                bv_id=None, video_title=None):
    """
    ç”Ÿæˆåˆå¹¶çš„è¿­ä»£ç»Ÿè®¡æŠ¥å‘Š
    
    Args:
        popularity_comments: çƒ­åº¦çˆ¬å–è¯„è®º
        time_comments: æ—¶é—´çˆ¬å–è¯„è®º
        merged_comments: åˆå¹¶åè¯„è®º
        iteration_count: è¿­ä»£è½®æ•°
        iteration_hours: è¿­ä»£æ—¶é—´ï¼ˆæ—¶é—´è¿­ä»£æ¨¡å¼ï¼‰
        output_folder: è¾“å‡ºæ–‡ä»¶å¤¹
        oid: è§†é¢‘oid
        logger: æ—¥å¿—è®°å½•å™¨
        deduped_popularity: å»é‡åçš„çƒ­åº¦è¯„è®º
        deduped_time: å»é‡åçš„æ—¶é—´è¯„è®º
        popularity_threshold: çƒ­åº¦é‡å¤ç‡é˜ˆå€¼ï¼ˆé‡å¤ç‡è¿­ä»£æ¨¡å¼ï¼‰
        time_threshold: æ—¶é—´é‡å¤ç‡é˜ˆå€¼ï¼ˆé‡å¤ç‡è¿­ä»£æ¨¡å¼ï¼‰
        popularity_duplicate_rates: çƒ­åº¦çˆ¬å–æ¯è½®é‡å¤ç‡åˆ—è¡¨ï¼ˆé‡å¤ç‡è¿­ä»£æ¨¡å¼ï¼‰
        time_duplicate_rates: æ—¶é—´çˆ¬å–æ¯è½®é‡å¤ç‡åˆ—è¡¨ï¼ˆé‡å¤ç‡è¿­ä»£æ¨¡å¼ï¼‰
        bv_id: è§†é¢‘BVå·
        video_title: è§†é¢‘æ ‡é¢˜
    """
    from datetime import datetime
    
    # å°†ç»Ÿè®¡æŠ¥å‘Šä¿å­˜åˆ°åŸå§‹æ•°æ®æ–‡ä»¶å¤¹
    iteration_folder = os.path.join(output_folder, 'åŸå§‹æ•°æ®')
    
    # ç”Ÿæˆåˆå¹¶çš„è¿­ä»£ç»Ÿè®¡æŠ¥å‘Š
    filename_suffix = f"{video_title}_{bv_id}"
    merged_stats_file = os.path.join(iteration_folder, f'è¿­ä»£ç»Ÿè®¡æŠ¥å‘Š_{filename_suffix}.txt')
    
    with open(merged_stats_file, 'w', encoding='utf-8') as f:
        f.write("=" * 60 + "\n")
        f.write("Bç«™è¯„è®ºçˆ¬è™« - è¿­ä»£ç»Ÿè®¡æŠ¥å‘Š\n")
        f.write("=" * 60 + "\n\n")
        
        # åŸºæœ¬ä¿¡æ¯
        f.write("=== åŸºæœ¬ä¿¡æ¯ ===\n")
        f.write(f"è§†é¢‘BVå·: {bv_id}\n")
        f.write(f"è§†é¢‘æ ‡é¢˜: {video_title}\n")
        f.write(f"ç»Ÿè®¡æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        
        if iteration_hours:
            f.write(f"è¿­ä»£æ¨¡å¼: æ—¶é—´è¿­ä»£ ({iteration_hours} å°æ—¶)\n")
        else:
            f.write(f"è¿­ä»£æ¨¡å¼: é‡å¤ç‡è¿­ä»£\n")
            if popularity_threshold:
                f.write(f"çƒ­åº¦é‡å¤ç‡é˜ˆå€¼: {popularity_threshold}%\n")
            if time_threshold:
                f.write(f"æ—¶é—´é‡å¤ç‡é˜ˆå€¼: {time_threshold}%\n")
        
        f.write(f"è¿­ä»£è½®æ•°: {iteration_count}\n\n")
        
        # æ€»ä½“ç»Ÿè®¡
        f.write("=== æ€»ä½“ç»Ÿè®¡ ===\n")
        all_raw_comments = popularity_comments + time_comments
        f.write(f"æ€»åŸå§‹çˆ¬å–: {len(all_raw_comments)} æ¡è¯„è®º\n")
        f.write(f"æœ€ç»ˆå»é‡å: {len(merged_comments)} æ¡è¯„è®º\n")
        total_duplicate_count = len(all_raw_comments) - len(merged_comments)
        total_duplicate_rate = (total_duplicate_count / len(all_raw_comments) * 100) if len(all_raw_comments) > 0 else 0
        f.write(f"æ€»é‡å¤è¯„è®º: {total_duplicate_count} æ¡\n")
        
        # åˆ†ç±»ç»Ÿè®¡
        f.write("=== åˆ†ç±»ç»Ÿè®¡ ===\n")
        
        # çƒ­åº¦çˆ¬å–ç»Ÿè®¡
        if deduped_popularity is not None:
            f.write("ã€çƒ­åº¦çˆ¬å–ã€‘\n")
            f.write(f"  åŸå§‹çˆ¬å–: {len(popularity_comments)} æ¡è¯„è®º\n")
            f.write(f"  å»é‡å: {len(deduped_popularity)} æ¡è¯„è®º\n")
            pop_duplicate_count = len(popularity_comments) - len(deduped_popularity)
            pop_duplicate_rate = (pop_duplicate_count / len(popularity_comments) * 100) if len(popularity_comments) > 0 else 0
            f.write(f"  é‡å¤è¯„è®º: {pop_duplicate_count} æ¡\n")

        # æ—¶é—´çˆ¬å–ç»Ÿè®¡
        if deduped_time is not None:
            f.write("ã€æ—¶é—´çˆ¬å–ã€‘\n")
            f.write(f"  åŸå§‹çˆ¬å–: {len(time_comments)} æ¡è¯„è®º\n")
            f.write(f"  å»é‡å: {len(deduped_time)} æ¡è¯„è®º\n")
            time_duplicate_count = len(time_comments) - len(deduped_time)
            time_duplicate_rate = (time_duplicate_count / len(time_comments) * 100) if len(time_comments) > 0 else 0
            f.write(f"  é‡å¤è¯„è®º: {time_duplicate_count} æ¡\n")
        
        # é‡å¤ç‡è¿­ä»£è¯¦æƒ…ï¼ˆä»…åœ¨é‡å¤ç‡è¿­ä»£æ¨¡å¼ä¸‹æ˜¾ç¤ºï¼‰
        if not iteration_hours and (popularity_duplicate_rates or time_duplicate_rates):
            f.write("=== é‡å¤ç‡è¿­ä»£è¯¦æƒ… ===\n")
            
            if popularity_duplicate_rates:
                f.write("ã€çƒ­åº¦çˆ¬å–æ¯è½®é‡å¤ç‡ã€‘\n")
                for i, rate in enumerate(popularity_duplicate_rates, 1):
                    f.write(f"  ç¬¬{i+1}è½®ä¸ç¬¬{i}è½®é‡å¤ç‡: {rate:.1f}%\n")
                f.write("\n")
            
            if time_duplicate_rates:
                f.write("ã€æ—¶é—´çˆ¬å–æ¯è½®é‡å¤ç‡ã€‘\n")
                for i, rate in enumerate(time_duplicate_rates, 1):
                    f.write(f"  ç¬¬{i+1}è½®ä¸ç¬¬{i}è½®é‡å¤ç‡: {rate:.1f}%\n")
                f.write("\n")

    print(f"ğŸ“Š è¿­ä»£ç»Ÿè®¡æŠ¥å‘Š: {os.path.basename(merged_stats_file)}")
    logger.info(f"è¿­ä»£ç»Ÿè®¡æŠ¥å‘Šå·²ç”Ÿæˆ: {merged_stats_file}")

def generate_folder_structure_md(output_folder, oid, video_title=None, logger=None, bv_id=None):
    """
    ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å¤¹ç»“æ„çš„markdownæ–‡æ¡£
    
    Args:
        output_folder (str): è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„
        oid (str): è§†é¢‘oid
        video_title (str, optional): è§†é¢‘æ ‡é¢˜
        logger: æ—¥å¿—è®°å½•å™¨
        bv_id (str, optional): è§†é¢‘BVå·
    
    Returns:
        str: ç”Ÿæˆçš„markdownæ–‡ä»¶è·¯å¾„
    """
    import os
    from datetime import datetime
    
    def get_folder_tree(folder_path, prefix="", is_last=True, max_depth=5, current_depth=0):
        """
        é€’å½’ç”Ÿæˆæ–‡ä»¶å¤¹æ ‘ç»“æ„
        """
        if current_depth >= max_depth:
            return []
        
        items = []
        if not os.path.exists(folder_path):
            return items
        
        try:
            # è·å–æ–‡ä»¶å¤¹å†…å®¹å¹¶æ’åº
            entries = os.listdir(folder_path)
            entries.sort()
            
            # åˆ†ç¦»æ–‡ä»¶å¤¹å’Œæ–‡ä»¶
            folders = []
            files = []
            
            for entry in entries:
                entry_path = os.path.join(folder_path, entry)
                if os.path.isdir(entry_path):
                    folders.append(entry)
                else:
                    files.append(entry)
            
            # å…ˆå¤„ç†æ–‡ä»¶å¤¹
            all_entries = folders + files
            
            for i, entry in enumerate(all_entries):
                is_last_entry = (i == len(all_entries) - 1)
                entry_path = os.path.join(folder_path, entry)
                
                # ç¡®å®šå‰ç¼€ç¬¦å·
                if is_last_entry:
                    current_prefix = prefix + "â””â”€â”€ "
                    next_prefix = prefix + "    "
                else:
                    current_prefix = prefix + "â”œâ”€â”€ "
                    next_prefix = prefix + "â”‚   "
                
                # æ·»åŠ å½“å‰é¡¹
                if os.path.isdir(entry_path):
                    items.append(f"{current_prefix}{entry}/")
                    # ç‰¹æ®Šå¤„ç†logsæ–‡ä»¶å¤¹ï¼Œä¸é€’å½’æ‰«æ
                    if entry.lower() == "logs":
                        items.append(f"{next_prefix}â””â”€â”€ ï¼ˆè¿è¡Œæ—¥å¿—ï¼Œçœç•¥ï¼‰")
                    else:
                        # é€’å½’å¤„ç†å­æ–‡ä»¶å¤¹
                        sub_items = get_folder_tree(entry_path, next_prefix, is_last_entry, max_depth, current_depth + 1)
                        items.extend(sub_items)
                else:
                    # æ–‡ä»¶å¤§å°ä¿¡æ¯
                    try:
                        file_size = os.path.getsize(entry_path)
                        if file_size > 1024 * 1024:  # MB
                            size_str = f" ({file_size / (1024 * 1024):.1f} MB)"
                        elif file_size > 1024:  # KB
                            size_str = f" ({file_size / 1024:.1f} KB)"
                        else:
                            size_str = f" ({file_size} B)"
                    except:
                        size_str = ""
                    
                    items.append(f"{current_prefix}{entry}{size_str}")
            
        except PermissionError:
            items.append(f"{prefix}â””â”€â”€ [æƒé™ä¸è¶³ï¼Œæ— æ³•è®¿é—®]")
        except Exception as e:
            items.append(f"{prefix}â””â”€â”€ [é”™è¯¯: {str(e)}]")
        
        return items
    
    # ç”Ÿæˆmarkdownå†…å®¹
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    folder_name = os.path.basename(output_folder)
    
    md_content = f"""# Bç«™è¯„è®ºçˆ¬è™«è¾“å‡ºæ–‡ä»¶å¤¹ç»“æ„

## åŸºæœ¬ä¿¡æ¯

- **ç”Ÿæˆæ—¶é—´**: {timestamp}
- **è§†é¢‘OID**: {oid}
- **è§†é¢‘BVå·**: {bv_id or 'æœªè·å–'}
- **è§†é¢‘æ ‡é¢˜**: {video_title or 'æœªè·å–'}
- **è¾“å‡ºæ–‡ä»¶å¤¹**: {folder_name}
- **å®Œæ•´è·¯å¾„**: {output_folder}

## æ–‡ä»¶å¤¹ç»“æ„æ ‘

```
{folder_name}/
"""
    
    # ç”Ÿæˆæ–‡ä»¶å¤¹æ ‘
    tree_items = get_folder_tree(output_folder)
    for item in tree_items:
        md_content += item + "\n"
    
    md_content += "```\n\n## æ–‡ä»¶è¯´æ˜\n\n### ä¸»è¦æ–‡ä»¶ç±»å‹\n\n"
    
    # æ·»åŠ æ–‡ä»¶è¯´æ˜
    md_content += """- **CSVæ–‡ä»¶**: è¯„è®ºæ•°æ®æ–‡ä»¶
  - `*_æŒ‰çƒ­åº¦æ’åº.csv`: æŒ‰çƒ­åº¦æ’åºçš„è¯„è®ºæ•°æ®
  - `*_ä¸»æ¥¼è¯„è®º.csv`: ä»…åŒ…å«ä¸»æ¥¼è¯„è®ºçš„æ•°æ®
  - `*_åˆå¹¶å»é‡ç»“æœ.csv`: åˆå¹¶å»é‡åçš„å®Œæ•´æ•°æ®
  - `*_é‡å¤è¯„è®ºåˆ—è¡¨.csv`: æ£€æµ‹åˆ°çš„é‡å¤è¯„è®º

- **TXTæ–‡ä»¶**: ç»Ÿè®¡æŠ¥å‘Šæ–‡ä»¶
  - `*_statistics.txt`: åŸºç¡€ç»Ÿè®¡æŠ¥å‘Š
  - `æŒ‰*ç»Ÿè®¡_*.txt`: å„ç§æ—¶é—´ç²’åº¦çš„ç»Ÿè®¡æ–‡ä»¶

- **LOGæ–‡ä»¶**: æ—¥å¿—è®°å½•æ–‡ä»¶
  - `*_main.log`: ä¸»ç¨‹åºè¿è¡Œæ—¥å¿—
  - `*_page_*.log`: åˆ†é¡µçˆ¬å–è¯¦ç»†æ—¥å¿—

### æ–‡ä»¶å¤¹è¯´æ˜

- **logs/**: å­˜æ”¾æ‰€æœ‰æ—¥å¿—æ–‡ä»¶
- **åŸå§‹æ•°æ®/**: å­˜æ”¾åŸå§‹çˆ¬å–æ•°æ®å’Œä¸­é—´å¤„ç†ç»“æœ
- **æŒ‰æ—¶é—´ç»Ÿè®¡/**: å­˜æ”¾å„ç§æ—¶é—´ç²’åº¦çš„ç»Ÿè®¡æ–‡ä»¶

### æ–‡ä»¶å‘½åè§„åˆ™

æ–‡ä»¶åæ ¼å¼é€šå¸¸ä¸º: `[ç±»å‹]_[è§†é¢‘æ ‡é¢˜]_[BVå·]_[æ—¶é—´æˆ³].[æ‰©å±•å]`

- ç±»å‹: å¦‚\"è¯„è®ºçˆ¬å–\"ã€\"æŒ‰æ—¥ç»Ÿè®¡\"ç­‰
- è§†é¢‘æ ‡é¢˜: æ¸…ç†åçš„å®‰å…¨æ–‡ä»¶å
- BVå·: è§†é¢‘çš„BVå·æ ‡è¯†
- æ—¶é—´æˆ³: ç”Ÿæˆæ—¶é—´(HHMMSS_YYYYMMDDæ ¼å¼)

## ä½¿ç”¨è¯´æ˜

1. **æŸ¥çœ‹è¯„è®ºæ•°æ®**: æ‰“å¼€CSVæ–‡ä»¶ï¼Œæ¨èä½¿ç”¨Excelæˆ–å…¶ä»–è¡¨æ ¼è½¯ä»¶
2. **æŸ¥çœ‹ç»Ÿè®¡æŠ¥å‘Š**: æ‰“å¼€TXTæ–‡ä»¶ï¼ŒåŒ…å«è¯¦ç»†çš„æ•°æ®åˆ†æ
3. **æ£€æŸ¥è¿è¡Œæ—¥å¿—**: æŸ¥çœ‹logsæ–‡ä»¶å¤¹ä¸­çš„æ—¥å¿—æ–‡ä»¶ï¼Œäº†è§£çˆ¬å–è¿‡ç¨‹
4. **æ—¶é—´ç»Ÿè®¡åˆ†æ**: æŸ¥çœ‹\"æŒ‰æ—¶é—´ç»Ÿè®¡\"æ–‡ä»¶å¤¹ä¸­çš„å„ç§æ—¶é—´ç»´åº¦ç»Ÿè®¡

---

*æ­¤æ–‡æ¡£ç”±Bç«™è¯„è®ºçˆ¬è™«è‡ªåŠ¨ç”Ÿæˆ*
"""
    
    # ä¿å­˜markdownæ–‡ä»¶
    md_filename = generate_safe_filename(video_title, oid, "æ–‡ä»¶å¤¹ç»“æ„", "other")
    md_file_path = os.path.join(output_folder, f"{md_filename}.md")
    
    try:
        with open(md_file_path, 'w', encoding='utf-8') as f:
            f.write(md_content)
        
        if logger:
            logger.info(f"æ–‡ä»¶å¤¹ç»“æ„æ–‡æ¡£å·²ç”Ÿæˆ: {md_file_path}")
        
        return md_file_path
        
    except Exception as e:
        error_msg = f"ç”Ÿæˆæ–‡ä»¶å¤¹ç»“æ„æ–‡æ¡£å¤±è´¥: {e}"
        if logger:
            logger.error(error_msg)
        print(f"âŒ {error_msg}")
        return None


# ä¸»ç¨‹åº
if __name__ == "__main__":
    try:
        # åŠ è½½é…ç½®æ–‡ä»¶
        print("ğŸ”§ æ­£åœ¨åŠ è½½é…ç½®...")
        config = load_config()
        print("âœ… é…ç½®åŠ è½½å®Œæˆ")
        
        # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
        if len(sys.argv) > 1:
            # æµ‹è¯•æ¨¡å¼ï¼šä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°
            if len(sys.argv) >= 6:
                default_oid = sys.argv[1]
                default_mode = int(sys.argv[2])
                default_ps = int(sys.argv[3])
                default_delay_ms = int(sys.argv[4])
                test_mode_flag = sys.argv[5].lower() == 'true'
                
                print(f"ğŸ§ª æµ‹è¯•æ¨¡å¼å¯åŠ¨")
                print(f"å‚æ•°: oid={default_oid}, mode={default_mode}, ps={default_ps}, delay={default_delay_ms}ms, test_mode={test_mode_flag}")
                
                # è·å–è§†é¢‘ä¿¡æ¯
                print("\nğŸ” æ­£åœ¨è·å–è§†é¢‘ä¿¡æ¯...")
                video_info = None
                video_title = None
                try:
                    bv_id = aid_to_bvid(int(default_oid))
                    if bv_id:
                        video_info = get_video_info(bv_id)
                        if video_info and 'title' in video_info:
                            video_title = video_info['title']
                            print(f"ğŸ“º è§†é¢‘æ ‡é¢˜: {video_title}")
                        else:
                            # å›é€€åˆ°å¿«é€Ÿè·å–æ ‡é¢˜
                            video_title = get_video_title_quick(bv_id)
                            if video_title:
                                print(f"ğŸ“º è§†é¢‘æ ‡é¢˜: {video_title}")
                            else:
                                print("âŒ æ— æ³•è·å–è§†é¢‘æ ‡é¢˜ï¼Œç¨‹åºç»ˆæ­¢")
                                print("ç¨‹åºé€€å‡º")
                                sys.exit(1)
                    else:
                        print("âŒ æ— æ³•è·å–BVå·ï¼Œç¨‹åºç»ˆæ­¢")
                        print("ç¨‹åºé€€å‡º")
                        sys.exit(1)
                except Exception as e:
                    print(f"âŒ è·å–è§†é¢‘ä¿¡æ¯å¤±è´¥: {e}")
                    print("ç¨‹åºé€€å‡º")
                    sys.exit(1)
                
                # å¼€å§‹çˆ¬å–
                crawl_all_comments(default_oid, default_mode, default_ps, default_delay_ms, test_mode_flag, video_title, video_info)
            else:
                print("âŒ æµ‹è¯•æ¨¡å¼å‚æ•°ä¸è¶³ï¼")
                print("ç”¨æ³•: python script.py <oid> <mode> <ps> <delay_ms> <test_mode>")
                sys.exit(1)
        else:
            # æ­£å¸¸æ¨¡å¼ï¼šè·å–ç”¨æˆ·è¾“å…¥
            user_input = get_user_input()
            
            if user_input[0] is None:
                print("ç¨‹åºé€€å‡º")
                sys.exit(1)
            
            # è§£åŒ…ç”¨æˆ·è¾“å…¥
            oid, mode, ps, delay_ms, max_pages, test_sort_mode, iteration_config, video_title, video_info = user_input
            
            # æ ¹æ®æ¨¡å¼ç¡®å®šmode_type
            if mode == 'iteration':
                if iteration_config and iteration_config.get('type') == 'time':
                    mode_type = "iteration_time"
                elif iteration_config and iteration_config.get('type') == 'duplicate_rate':
                    mode_type = "iteration_rate"
                else:
                    mode_type = "iteration_time"  # é»˜è®¤ä¸ºæ—¶é—´é™å®š
            elif mode == 'comprehensive':
                mode_type = "comprehensive"
            elif mode == 'test':
                # æµ‹è¯•æ¨¡å¼æ ¹æ®æ’åºæ–¹å¼ç¡®å®šmode_type
                if test_sort_mode == 0:  # æ—¶é—´æ’åº
                    mode_type = "test_time"
                else:  # çƒ­åº¦æ’åº
                    mode_type = "test_popularity"
            else:
                mode_type = None
            
            # åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹å’Œæ—¥å¿—
            output_folder = create_output_folder(oid, video_title, mode_type)
            logger, main_log_file = setup_logging(oid, output_folder)
            
            # æ ¹æ®æ¨¡å¼é€‰æ‹©ä¸åŒçš„çˆ¬å–æ–¹å¼
            if mode == 'iteration':
                # è¿­ä»£æ¨¡å¼
                print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶å¤¹: {output_folder}")
                print(f"ğŸ“ ä¸»æ—¥å¿—æ–‡ä»¶: {main_log_file}")
                print(f"\nğŸ”„ å¼€å§‹è¿­ä»£æ¨¡å¼çˆ¬å–...")
                
                # æ‰§è¡Œè¿­ä»£æ¨¡å¼çˆ¬å–
                result = crawl_iteration_mode_comments(
                    oid=oid,
                    ps=ps,
                    delay_ms=delay_ms,
                    iteration_config=iteration_config,
                    logger=logger,
                    output_folder=output_folder,
                    video_title=video_title,
                    video_info=video_info
                )
                
                if result:
                    print("âœ… è¿­ä»£æ¨¡å¼çˆ¬å–å®Œæˆ")
                    logger.info("è¿­ä»£æ¨¡å¼çˆ¬å–å®Œæˆ")
                else:
                    print("âŒ è¿­ä»£æ¨¡å¼çˆ¬å–å¤±è´¥")
                    logger.error("è¿­ä»£æ¨¡å¼çˆ¬å–å¤±è´¥")
                    sys.exit(1)
                    
            elif mode == 'comprehensive':
        # ç»¼åˆæ¨¡å¼
                print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶å¤¹: {output_folder}")
                print(f"ğŸ“ ä¸»æ—¥å¿—æ–‡ä»¶: {main_log_file}")
                
                # æ‰§è¡Œç»¼åˆæ¨¡å¼çˆ¬å–
                result = crawl_comprehensive_mode_comments(
                    oid, ps, delay_ms, test_mode=False, logger=logger, output_folder=output_folder
                )
                
                if result:
                    popularity_comments, time_comments, merged_comments, duplicate_comments, popularity_end_reason = result
                    print(f"\nğŸ“Š çƒ­åº¦çˆ¬å–ç»“æŸåŸå› : {popularity_end_reason}")
                else:
                    print("âŒ ç»¼åˆæ¨¡å¼çˆ¬å–å¤±è´¥")
                    logger.error("ç»¼åˆæ¨¡å¼çˆ¬å–å¤±è´¥")
                    sys.exit(1)
                
                # å¤„ç†ç»¼åˆæ¨¡å¼æ•°æ®ï¼Œç”Ÿæˆ4ä¸ªåŸå§‹æ•°æ®æ–‡æ¡£
                raw_data_folder, doc_paths = process_comprehensive_mode_data(
                    oid, popularity_comments, time_comments, merged_comments, duplicate_comments, output_folder, logger, video_title
                )
                
                # å¯¹åˆå¹¶ç»“æœè¿›è¡ŒåŒé‡æ•´ç†
                print("\n=== å¼€å§‹åŒé‡æ•´ç† ===")
                print("1. æŒ‰çƒ­åº¦æ’åºæ•´ç†...")
                
                # æŒ‰çƒ­åº¦æ’åºæ•´ç†ï¼ˆä½¿ç”¨åˆå¹¶åçš„æ•°æ®ï¼‰
                _, popularity_organized_file, popularity_stats_file = process_and_organize_data(
                    merged_comments, output_folder, oid, logger, video_title, sort_by_popularity=True, video_info=video_info, mode="comprehensive"
                )
                
                print("2. æŒ‰æ—¶é—´ç»Ÿè®¡æ•´ç†...")
                
                # æŒ‰æ—¶é—´ç»Ÿè®¡æ•´ç†ï¼ˆä½¿ç”¨åˆå¹¶åçš„æ•°æ®ï¼‰- ä¸ç”Ÿæˆæ•´ç†æ–‡ä»¶
                _, _, time_stats_file = process_and_organize_data(
                    merged_comments, output_folder, oid, logger, video_title, sort_by_popularity=False, video_info=video_info, mode="comprehensive"
                )
                
                # ç”Ÿæˆæ™ºèƒ½æ—¶é—´ç»Ÿè®¡æ–‡ä»¶
                print("3. ç”Ÿæˆæ—¶é—´ç»Ÿè®¡åˆ†æ...")
                bv_id = aid_to_bvid(int(oid))
                time_analysis_files = generate_restructured_time_statistics(
                    merged_comments, output_folder, bv_id, logger, video_title, video_info
                )
                
                if time_analysis_files:
                    print(f"   âœ… å·²ç”Ÿæˆ {len(time_analysis_files)} ä¸ªæ—¶é—´ç»Ÿè®¡æ–‡ä»¶")
                    for file_path in time_analysis_files:
                        print(f"      - {os.path.basename(file_path)}")
                else:
                    print("   âš ï¸  æœªç”Ÿæˆæ—¶é—´ç»Ÿè®¡æ–‡ä»¶ï¼ˆå¯èƒ½å› ä¸ºæ•°æ®ä¸è¶³ï¼‰")
                
                # è¾“å‡ºæœ€ç»ˆç»“æœ
                print("\n=== ç»¼åˆæ¨¡å¼çˆ¬å–å®Œæˆ ===")
                print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶å¤¹: {output_folder}")
                print(f"ğŸ“ åŸå§‹æ•°æ®æ–‡ä»¶å¤¹: {raw_data_folder}")
                print("\nğŸ“„ åŸå§‹æ•°æ®æ–‡æ¡£:")
                for i, doc_path in enumerate(doc_paths, 1):
                    print(f"  {i}. {os.path.basename(doc_path)}")
                print("\nğŸ“„ æ•´ç†åæ–‡æ¡£:")
                if popularity_organized_file:
                    print(f"  - çƒ­åº¦æ’åºæ•´ç†: {os.path.basename(popularity_organized_file)}")
                if popularity_stats_file:
                    print(f"  - çƒ­åº¦æ’åºç»Ÿè®¡: {os.path.basename(popularity_stats_file)}")
                if time_stats_file:
                    print(f"  - æ—¶é—´æ’åºç»Ÿè®¡: {os.path.basename(time_stats_file)}")
                
                if time_analysis_files:
                    print("\nğŸ“Š æ—¶é—´ç»Ÿè®¡åˆ†ææ–‡ä»¶:")
                    for file_path in time_analysis_files:
                        print(f"  - {os.path.basename(file_path)}")
                
                logger.info("ç»¼åˆæ¨¡å¼çˆ¬å–å’Œæ•´ç†å®Œæˆ")
                
                # ç”Ÿæˆæ–‡ä»¶å¤¹ç»“æ„æ–‡æ¡£
                print("\nğŸ“‹ ç”Ÿæˆæ–‡ä»¶å¤¹ç»“æ„æ–‡æ¡£...")
                # ç”ŸæˆBVå·
                try:
                    bv_id = aid_to_bvid(int(oid))
                except:
                    bv_id = None
                structure_md_path = generate_folder_structure_md(output_folder, oid, video_title, logger, bv_id)
                if structure_md_path:
                    print(f"ğŸ“„ æ–‡ä»¶å¤¹ç»“æ„æ–‡æ¡£: {os.path.basename(structure_md_path)}")
                
                # è¾“å‡ºçˆ¬å–ç»“æœæ€»ç»“
                print("\n" + "="*50)
                print("ğŸ¯ çˆ¬å–ç»“æœæ€»ç»“")
                print("="*50)
                
                # çƒ­åº¦çˆ¬å–ç»“æœ
                popularity_success = len(popularity_comments) > 0
                print(f"ğŸ”¥ çƒ­åº¦æ’åºçˆ¬å–: {'âœ… æˆåŠŸ' if popularity_success else 'âŒ å¤±è´¥'} ({len(popularity_comments)} æ¡è¯„è®º)")
                print(f"   ç»“æŸåŸå› : {popularity_end_reason}")
                
                # æ—¶é—´çˆ¬å–ç»“æœ
                time_crawl_performed = len(time_comments) > 0
                if time_crawl_performed:
                    time_success = len(time_comments) > 0
                    print(f"â° æ—¶é—´æ’åºçˆ¬å–: {'âœ… æˆåŠŸ' if time_success else 'âŒ å¤±è´¥'} ({len(time_comments)} æ¡è¯„è®º)")
                else:
                    print(f"â° æ—¶é—´æ’åºçˆ¬å–: â­ï¸ è·³è¿‡ (çƒ­åº¦çˆ¬å–å·²è·å–å®Œæ•´æ•°æ®)")
                
                # åˆå¹¶ç»“æœ
                print(f"ğŸ”— åˆå¹¶å»é‡ç»“æœ: âœ… å®Œæˆ ({len(merged_comments)} æ¡å”¯ä¸€è¯„è®º, {len(duplicate_comments)} æ¡é‡å¤è¯„è®º)")
                
                # å®Œæ•´æ€§è¯„ä¼°
                if popularity_end_reason == "è¯„è®ºå·²å…¨éƒ¨çˆ¬å–å®Œæ¯•":
                    print(f"ğŸ“‹ æ•°æ®å®Œæ•´æ€§: âœ… å®Œå…¨çˆ¬å– (å·²è·å–è¯¥è§†é¢‘çš„æ‰€æœ‰è¯„è®º)")
                elif time_crawl_performed and len(time_comments) > 0:
                    print(f"ğŸ“‹ æ•°æ®å®Œæ•´æ€§: âœ… åŒé‡çˆ¬å–å®Œæˆ (çƒ­åº¦+æ—¶é—´æ’åºè¡¥å……)")
                else:
                    print(f"ğŸ“‹ æ•°æ®å®Œæ•´æ€§: âš ï¸ éƒ¨åˆ†çˆ¬å– (å¯èƒ½å­˜åœ¨æœªè·å–çš„è¯„è®º)")
                
                print("="*50)
            elif mode == 'test':
                # æµ‹è¯•æ¨¡å¼ - è·å–è§†é¢‘æ ‡é¢˜
                print("\nğŸ” æ­£åœ¨è·å–è§†é¢‘ä¿¡æ¯...")
                try:
                    # ä»oid(aid)è½¬æ¢ä¸ºBVå·æ¥è·å–è§†é¢‘æ ‡é¢˜
                    bv_id = aid_to_bvid(int(oid))
                    video_title = get_video_title_quick(bv_id) if bv_id else None
                    if not video_title:
                        print("âŒ æ— æ³•è·å–è§†é¢‘æ ‡é¢˜ï¼Œç¨‹åºç»ˆæ­¢")
                        print("ç¨‹åºé€€å‡º")
                        sys.exit(1)
                    else:
                        print(f"ğŸ“º è§†é¢‘æ ‡é¢˜: {video_title}")
                except Exception as e:
                    print(f"âŒ è·å–è§†é¢‘æ ‡é¢˜å¤±è´¥: {e}")
                    print("ç¨‹åºé€€å‡º")
                    sys.exit(1)
                
                print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶å¤¹: {output_folder}")
                print(f"ğŸ“ ä¸»æ—¥å¿—æ–‡ä»¶: {main_log_file}")
                print(f"\nğŸ§ª å¼€å§‹æµ‹è¯•æ¨¡å¼çˆ¬å–...")
                
                # æ‰§è¡Œæµ‹è¯•æ¨¡å¼çˆ¬å–
                comments, end_reason = crawl_test_mode_comments(
                    oid=oid,
                    sort_mode=test_sort_mode,
                    ps=ps,
                    delay_ms=delay_ms,
                    max_pages=max_pages,
                    logger=logger,
                    output_folder=output_folder
                )
                
                # å¤„ç†å’Œæ•´ç†æ•°æ®
                if comments:
                    print(f"\nğŸ“Š å¼€å§‹æ•´ç† {len(comments)} æ¡è¯„è®º...")
                    
                    # ç”ŸæˆåŸå§‹æ•°æ®CSVæ–‡æ¡£ï¼ˆä¸æ•´ç†åæ–‡ä»¶åŒçº§ï¼‰
                    sort_name = "çƒ­åº¦æ’åº" if test_sort_mode == 1 else "æ—¶é—´æ’åº"
                    raw_filename = generate_safe_filename(video_title, oid, f"æµ‹è¯•æ¨¡å¼_{sort_name}", "original")
                    raw_csv_file = os.path.join(output_folder, f"{raw_filename}.csv")
                    
                    # è°ƒç”¨ç»¼åˆæ¨¡å¼ä½¿ç”¨çš„å‡½æ•°ç”ŸæˆåŸå§‹æ•°æ®CSV
                    save_comments_to_csv(comments, raw_csv_file, f"æµ‹è¯•æ¨¡å¼_{sort_name}")
                    print(f"âœ… åŸå§‹æ•°æ®å·²ä¿å­˜: {os.path.basename(raw_csv_file)}")
                    
                    # æ•´ç†æ•°æ®
                    sort_by_popularity = (test_sort_mode == 1)
                    mode_param = "test_popularity" if test_sort_mode == 1 else "test_time"
                    _, organized_file, stats_file = process_and_organize_data(
                        comments, output_folder, oid, logger, video_title, sort_by_popularity=sort_by_popularity, video_info=video_info, mode=mode_param
                    )
                    
                    print(f"âœ… æ•´ç†å®Œæˆ")
                    if organized_file:
                        print(f"   - æ•´ç†æ–‡ä»¶: {os.path.basename(organized_file)}")
                    if stats_file:
                        print(f"   - ç»Ÿè®¡æ–‡ä»¶: {os.path.basename(stats_file)}")
                    
                    # ç”Ÿæˆæ–‡ä»¶å¤¹ç»“æ„æ–‡æ¡£
                    try:
                        # ç”ŸæˆBVå·
                        try:
                            bv_id = aid_to_bvid(int(oid))
                        except:
                            bv_id = None
                        structure_md_path = generate_folder_structure_md(output_folder, oid, video_title, logger, bv_id)
                        if structure_md_path:
                            print(f"   - æ–‡ä»¶å¤¹ç»“æ„æ–‡æ¡£: {os.path.basename(structure_md_path)}")
                    except Exception as e:
                        logger.error(f"ç”Ÿæˆæ–‡ä»¶å¤¹ç»“æ„æ–‡æ¡£å¤±è´¥: {e}")
                
                # è¾“å‡ºçˆ¬å–ç»“æœæ€»ç»“
                print("\n" + "="*50)
                print("ğŸ¯ æµ‹è¯•æ¨¡å¼çˆ¬å–ç»“æœ")
                print("="*50)
                
                sort_name = "çƒ­åº¦æ’åº" if test_sort_mode == 1 else "æ—¶é—´æ’åº"
                success = len(comments) > 0
                print(f"ğŸ§ª æµ‹è¯•æ¨¡å¼ - {sort_name}: {'âœ… æˆåŠŸ' if success else 'âŒ å¤±è´¥'} ({len(comments)} æ¡è¯„è®º)")
                print(f"ğŸ“„ çˆ¬å–é¡µæ•°: {max_pages} é¡µ")
                print(f"â±ï¸  ç»“æŸåŸå› : {end_reason}")
                
                # å®Œæ•´æ€§è¯„ä¼°
                if end_reason == "å·²è¾¾åˆ°æŒ‡å®šé¡µæ•°é™åˆ¶":
                    print(f"ğŸ“‹ æ•°æ®å®Œæ•´æ€§: âœ… æŒ‰è®¾å®šå®Œæˆ (å·²çˆ¬å–æŒ‡å®šçš„ {max_pages} é¡µ)")
                elif end_reason == "è¯„è®ºå·²å…¨éƒ¨çˆ¬å–å®Œæ¯•":
                    print(f"ğŸ“‹ æ•°æ®å®Œæ•´æ€§: âœ… å®Œå…¨çˆ¬å– (å·²è·å–è¯¥è§†é¢‘çš„æ‰€æœ‰è¯„è®º)")
                else:
                    print(f"ğŸ“‹ æ•°æ®å®Œæ•´æ€§: âš ï¸ éƒ¨åˆ†çˆ¬å– (å¯èƒ½å­˜åœ¨æœªè·å–çš„è¯„è®º)")
                
                print("="*50)
            else:
                print("âŒ ä¸æ”¯æŒçš„æ¨¡å¼é€‰æ‹©")
                print("ç³»ç»Ÿä»…æ”¯æŒä»¥ä¸‹ä¸‰ç§æ¨¡å¼ï¼š")
                print("- ç»¼åˆæ¨¡å¼ï¼šæ™ºèƒ½ç»„åˆçƒ­åº¦å’Œæ—¶é—´çˆ¬å–")
                print("- æµ‹è¯•æ¨¡å¼ï¼šå•ç‹¬æµ‹è¯•åŸºç¡€çˆ¬å–åŠŸèƒ½")
                print("- è¿­ä»£æ¨¡å¼ï¼šäº¤æ›¿æ‰§è¡Œçƒ­åº¦å’Œæ—¶é—´çˆ¬å–")
                sys.exit(1)
        
    except CookieBannedException:
        # Cookieè¢«å°ç¦çš„ç‰¹æ®Šå¤„ç†
        try:
            # å°è¯•è·å–è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„å’Œæ—¥å¿—è®°å½•å™¨
            if 'output_folder' in locals():
                output_folder_path = output_folder
            else:
                # å¦‚æœè¿˜æ²¡æœ‰åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹ï¼Œå°è¯•æ„å»ºè·¯å¾„
                if 'oid' in locals() and 'video_title' in locals() and 'mode_type' in locals():
                    output_folder_path = create_output_folder(oid, video_title, mode_type)
                else:
                    output_folder_path = None
            
            logger_instance = logger if 'logger' in locals() else None
            
            # è°ƒç”¨Cookieå°ç¦å¤„ç†å‡½æ•°
            handle_cookie_banned_error(output_folder_path, logger_instance)
        except Exception as cleanup_error:
            print(f"\nâŒ å¤„ç†Cookieå°ç¦é”™è¯¯æ—¶å‡ºç°é—®é¢˜: {cleanup_error}")
            print("ğŸš« Cookieå·²è¢«æš‚æ—¶å°ç¦ï¼Œè¯·æ›´æ¢Cookieæˆ–ç­‰å¾…ä¸€æ®µæ—¶é—´åé‡è¯•")
        
        sys.exit(1)
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­ç¨‹åº")
        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        sys.exit(1)


